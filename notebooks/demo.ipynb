{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1moHJ1TdhO"
      },
      "source": [
        "# Mixtral in Colab\n",
        "\n",
        "Welcome! In this notebook you can run [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) with decent generation speed **right in Google Colab or on a consumer-grade GPU**. This was made possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy.\n",
        "\n",
        "To learn more, read our [tech report](https://arxiv.org/abs/2312.17238) or check out the [repo](https://github.com/dvmazur/mixtral-offloading) on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-dvAX_hKZT4"
      },
      "source": [
        "One will need approximately 16 GB of VRAM and 11 GB of RAM to run this notebook and generate somewhat long texts.\n",
        "\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>How to balance between RAM and GPU VRAM usage</summary>\n",
        "\n",
        "You can balance between RAM and GPU VRAM usage by changing <code>offload_per_layer</code> variable in the <a href=\"#scrollTo=_mIpePTMFyRY&line=10&uniqifier=1\">Initialize model</a> section. Increasing <code>offload_per_layer</code> will decrease GPU VRAM usage, increase RAM usage and decrease generation speed. Decreasing <code>offload_per_layer</code> will have the opposite effect.\n",
        "\n",
        "Note that this notebook should run normally in Google Colab with <code>offload_per_layer = 4</code>, but may crush with other values. However, if you run this somewhere else, you're free to play with this variable.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MhvkC7TKEL"
      },
      "source": [
        "## Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f7qY7ebqX7T7",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# fix numpy in colab\n",
        "import numpy\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# fix triton in colab\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export HF_HOME=/scratch/bcjw/yyuan6/.cache\n",
        "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "#!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "# !ldconfig /usr/lib64-nvidia\n",
        "\n",
        "#!git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n",
        "#!cd mixtral-offloading && pip install -q -r requirements.txt\n",
        "#!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
        "\n",
        "# clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "03638a96888649dd9dc625a623eb6c34",
            "ffa7a23d200843068267e4e21553e98d",
            "ad60a3884afe47a9ab9b4a70a461d49b",
            "2248396d844b4f159fd895fa100f5875",
            "073cc65432b44fe29eae1c91471ff970",
            "137e376507144fccbabb12bf1268f699",
            "1a6a03868cdc4fb286b4ec580da9a391",
            "108098be0bdc49e791fb784d487fe175",
            "77eb47b1a453410b860b91a01357e98f",
            "ddc0e14f4448489499a534c5cf2f5945",
            "6e8a37d65bf64bbaac84230613164936"
          ]
        },
        "id": "GgpjnV7fV49W",
        "outputId": "1c10ea85-61e5-4572-aac9-0f02c4d2cb30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36mhqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/bcjw/yyuan6/miniconda3/envs/cuda122/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading\")\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from hqq.core.quantize import BaseQuantizeConfig\n",
        "# from huggingface_hub import snapshot_download\n",
        "# from IPython.display import clear_output\n",
        "# from tqdm.auto import trange\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "# from transformers.utils import logging as hf_logging\n",
        "from src.build_model import OffloadConfig, QuantConfig, build_model, build_model_without_quant\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkSYibHcTQsH"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial GPU Memory Usage: 0.0 GB\n",
            "number experts: 8\n",
            "OffloadConfig(main_size=32, offload_size=224, buffer_size=4, offload_per_layer=7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/bcjw/yyuan6/miniconda3/envs/cuda122/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 8.459648132324219 GB\n",
            "Peak GPU Memory Usage: 13.100425720214844 GB\n",
            "Peak GPU Memory Usage: 13.256828308105469 GB\n",
            "Peak GPU Memory Usage: 13.413230895996094 GB\n",
            "Peak GPU Memory Usage: 13.413230895996094 GB\n",
            "Peak GPU Memory Usage: 13.413230895996094 GB\n",
            "Peak GPU Memory Usage: 14.04840087890625 GB\n",
            "Peak GPU Memory Usage: 14.120803833007812 GB\n",
            "Peak GPU Memory Usage: 14.199005126953125 GB\n",
            "Peak GPU Memory Usage: 14.605331420898438 GB\n",
            "Peak GPU Memory Usage: 14.761734008789062 GB\n",
            "Peak GPU Memory Usage: 14.918136596679688 GB\n",
            "Peak GPU Memory Usage: 15.074539184570312 GB\n",
            "Peak GPU Memory Usage: 15.152740478515625 GB\n",
            "Peak GPU Memory Usage: 15.230941772460938 GB\n",
            "Peak GPU Memory Usage: 15.387344360351562 GB\n",
            "Peak GPU Memory Usage: 15.543746948242188 GB\n",
            "Peak GPU Memory Usage: 15.700149536132812 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MixtralBLockSparseTop2MLP is deprecated by MixtralBlockSparseTop2MLP and will be removed in v4.40.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "dict_keys(['model.layers.10.block_sparse_moe.gate.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.block_sparse_moe.gate.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.block_sparse_moe.gate.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.20.block_sparse_moe.gate.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.block_sparse_moe.gate.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.block_sparse_moe.gate.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.block_sparse_moe.gate.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.block_sparse_moe.gate.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'lm_head.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.block_sparse_moe.gate.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.norm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.block_sparse_moe.gate.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.block_sparse_moe.gate.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.block_sparse_moe.gate.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.block_sparse_moe.gate.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.embed_tokens.weight', 'model.layers.0.block_sparse_moe.gate.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.block_sparse_moe.gate.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.block_sparse_moe.gate.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.block_sparse_moe.gate.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.block_sparse_moe.gate.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.block_sparse_moe.gate.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.block_sparse_moe.gate.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.block_sparse_moe.gate.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.block_sparse_moe.gate.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.block_sparse_moe.gate.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.block_sparse_moe.gate.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.block_sparse_moe.gate.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.block_sparse_moe.gate.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.block_sparse_moe.gate.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.block_sparse_moe.gate.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.block_sparse_moe.gate.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.block_sparse_moe.gate.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.block_sparse_moe.gate.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.block_sparse_moe.gate.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight'])\n",
            "Finish loading trunk states!\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.778350830078125 GB\n",
            "Peak GPU Memory Usage: 15.887725830078125 GB\n",
            "Peak GPU Memory Usage: 16.215850830078125 GB\n",
            "Peak GPU Memory Usage: 16.543975830078125 GB\n",
            "Peak GPU Memory Usage: 16.872100830078125 GB\n",
            "Peak GPU Memory Usage: 17.200225830078125 GB\n",
            "Peak GPU Memory Usage: 17.528350830078125 GB\n",
            "Peak GPU Memory Usage: 17.856475830078125 GB\n",
            "Peak GPU Memory Usage: 18.184600830078125 GB\n",
            "Peak GPU Memory Usage: 18.512725830078125 GB\n",
            "Peak GPU Memory Usage: 18.840850830078125 GB\n",
            "Peak GPU Memory Usage: 19.168975830078125 GB\n",
            "Peak GPU Memory Usage: 19.497100830078125 GB\n",
            "Peak GPU Memory Usage: 19.825225830078125 GB\n",
            "Peak GPU Memory Usage: 20.153350830078125 GB\n",
            "Peak GPU Memory Usage: 20.481475830078125 GB\n",
            "Peak GPU Memory Usage: 20.809600830078125 GB\n",
            "Peak GPU Memory Usage: 21.137725830078125 GB\n",
            "Peak GPU Memory Usage: 21.465850830078125 GB\n",
            "Peak GPU Memory Usage: 21.793975830078125 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 22.122100830078125 GB\n",
            "Peak GPU Memory Usage: 22.450225830078125 GB\n",
            "Peak GPU Memory Usage: 22.778350830078125 GB\n",
            "Peak GPU Memory Usage: 23.106475830078125 GB\n",
            "Peak GPU Memory Usage: 23.215850830078125 GB\n",
            "Created expert cache! ... Cleaning cache\n",
            "Cleaned.\n",
            "Peak GPU Memory Usage: 23.215850830078125 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 27.335128784179688 GB\n",
            "Peak GPU Memory Usage: 27.663253784179688 GB\n",
            "Peak GPU Memory Usage: 27.663253784179688 GB\n",
            "Peak GPU Memory Usage: 27.991378784179688 GB\n",
            "Peak GPU Memory Usage: 28.319503784179688 GB\n",
            "Peak GPU Memory Usage: 28.647628784179688 GB\n",
            "Peak GPU Memory Usage: 28.647628784179688 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   3%|▎         | 1/32 [00:21<11:02, 21.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 28.647628784179688 GB\n",
            "Peak GPU Memory Usage: 28.647628784179688 GB\n",
            "Peak GPU Memory Usage: 28.647628784179688 GB\n",
            "Peak GPU Memory Usage: 28.897628784179688 GB\n",
            "Peak GPU Memory Usage: 28.897628784179688 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   6%|▋         | 2/32 [00:44<11:10, 22.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   9%|▉         | 3/32 [01:06<10:40, 22.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 32.63215637207031 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  12%|█▎        | 4/32 [01:31<10:51, 23.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  16%|█▌        | 5/32 [01:53<10:13, 22.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  19%|█▉        | 6/32 [02:14<09:40, 22.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.21601867675781 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  22%|██▏       | 7/32 [02:38<09:27, 22.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  25%|██▌       | 8/32 [02:59<08:57, 22.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  28%|██▊       | 9/32 [03:26<09:04, 23.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  31%|███▏      | 10/32 [03:48<08:33, 23.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  34%|███▍      | 11/32 [04:10<08:00, 22.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.62232971191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  38%|███▊      | 12/32 [04:34<07:42, 23.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  41%|████      | 13/32 [04:56<07:13, 22.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 33.95045471191406 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  44%|████▍     | 14/32 [05:23<07:13, 24.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  47%|████▋     | 15/32 [05:45<06:39, 23.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  50%|█████     | 16/32 [06:10<06:21, 23.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  53%|█████▎    | 17/32 [06:36<06:10, 24.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  56%|█████▋    | 18/32 [06:59<05:36, 24.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  59%|█████▉    | 19/32 [07:26<05:23, 24.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  62%|██████▎   | 20/32 [07:48<04:48, 24.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  66%|██████▌   | 21/32 [08:12<04:25, 24.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.341156005859375 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  69%|██████▉   | 22/32 [08:35<03:55, 23.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.341156005859375 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  72%|███████▏  | 23/32 [09:00<03:37, 24.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 35.59107971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  75%|███████▌  | 24/32 [09:24<03:11, 23.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  78%|███████▊  | 25/32 [09:46<02:43, 23.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  81%|████████▏ | 26/32 [10:10<02:21, 23.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  84%|████████▍ | 27/32 [10:32<01:55, 23.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  88%|████████▊ | 28/32 [10:58<01:35, 23.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  91%|█████████ | 29/32 [11:24<01:14, 24.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  94%|█████████▍| 30/32 [11:47<00:48, 24.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  97%|█████████▋| 31/32 [12:10<00:23, 23.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts: 100%|██████████| 32/32 [12:30<00:00, 23.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except NameError:\n",
        "    pass\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/Mixtral-8x7B-Instruct-v0.1\"\n",
        "# state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "config = AutoConfig.from_pretrained(model_name, torch_dtype=torch.float16,)\n",
        "device = torch.device(\"cuda:0\")\n",
        "config.num_experts_per_tok = 1\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "print(f\"initial GPU Memory Usage: {peak_memory_usage} GB\")  \n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "offload_per_layer = 7\n",
        "\n",
        "num_experts = config.num_local_experts\n",
        "print(\"number experts:\", num_experts)\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "print(offload_config)\n",
        "\n",
        "model = build_model_without_quant(\n",
        "    device=device,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")\n",
        "peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "print(f\"Peak GPU Memory Usage: {peak_memory_usage} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227,
          "referenced_widgets": [
            "67dc7ff4d0fe4af796308b2c8355d150",
            "b41dad78195344a6a77b570b1783f7a0",
            "494bb8b09d1f4f2abbc111eb5e6da130",
            "243efd24b8994278a406c597039d0fc4",
            "c2ccdf34885d41a0ab3f9b40c775b25a",
            "39efadb9ccb048deab07969016e7bd38",
            "d36446b0a9534f41a9f21b865b0a08f4",
            "6a458bdef43543d0b7b7edc2883e2dc1",
            "7f85bc3a52a540558999a1ff25a813b3",
            "325f9b174633405483cf60cff12d652b",
            "e60bfa41b7454322a908e01bb9caed65",
            "7b7b50c298de414b85cc622ef5660dfb",
            "d1e82a41293d41d6aa8807d9d575e86b",
            "a19471c4ecce428bbebab2a90f535861",
            "8a2c815f41b8483692a79165ebeaf3f6",
            "9782a56e37e24892a78b282295ae5ac0",
            "9be1146cefb9416cb875741640dca611",
            "15bfcbb901574f0583b8af3733f58b7a",
            "c0dba9bd6ee94ca1a0084eabeac0e1ca",
            "83bcda80b79a4e6eaf5b0f27bcbc0ee0",
            "b792acc1fff049fca658f836557c8e6f",
            "dcf2b97ed0b7415fa02f38c8bb0e3009"
          ]
        },
        "id": "_mIpePTMFyRY",
        "outputId": "8a113aee-bcaa-4434-bdfc-2e65c0c58b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number experts: 8\n",
            "dict_keys(['lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.block_sparse_moe.gate.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.W_q', 'model.layers.0.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.k_proj.meta.scale_q', 'model.layers.0.self_attn.k_proj.meta.zero_q', 'model.layers.0.self_attn.o_proj.W_q', 'model.layers.0.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.o_proj.meta.scale_q', 'model.layers.0.self_attn.o_proj.meta.zero_q', 'model.layers.0.self_attn.q_proj.W_q', 'model.layers.0.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.q_proj.meta.scale_q', 'model.layers.0.self_attn.q_proj.meta.zero_q', 'model.layers.0.self_attn.v_proj.W_q', 'model.layers.0.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.v_proj.meta.scale_q', 'model.layers.0.self_attn.v_proj.meta.zero_q', 'model.layers.1.block_sparse_moe.gate.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.W_q', 'model.layers.1.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.k_proj.meta.scale_q', 'model.layers.1.self_attn.k_proj.meta.zero_q', 'model.layers.1.self_attn.o_proj.W_q', 'model.layers.1.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.o_proj.meta.scale_q', 'model.layers.1.self_attn.o_proj.meta.zero_q', 'model.layers.1.self_attn.q_proj.W_q', 'model.layers.1.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.q_proj.meta.scale_q', 'model.layers.1.self_attn.q_proj.meta.zero_q', 'model.layers.1.self_attn.v_proj.W_q', 'model.layers.1.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.v_proj.meta.scale_q', 'model.layers.1.self_attn.v_proj.meta.zero_q', 'model.layers.10.block_sparse_moe.gate.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.W_q', 'model.layers.10.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.k_proj.meta.scale_q', 'model.layers.10.self_attn.k_proj.meta.zero_q', 'model.layers.10.self_attn.o_proj.W_q', 'model.layers.10.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.o_proj.meta.scale_q', 'model.layers.10.self_attn.o_proj.meta.zero_q', 'model.layers.10.self_attn.q_proj.W_q', 'model.layers.10.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.q_proj.meta.scale_q', 'model.layers.10.self_attn.q_proj.meta.zero_q', 'model.layers.10.self_attn.v_proj.W_q', 'model.layers.10.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.v_proj.meta.scale_q', 'model.layers.10.self_attn.v_proj.meta.zero_q', 'model.layers.11.block_sparse_moe.gate.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.W_q', 'model.layers.11.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.k_proj.meta.scale_q', 'model.layers.11.self_attn.k_proj.meta.zero_q', 'model.layers.11.self_attn.o_proj.W_q', 'model.layers.11.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.o_proj.meta.scale_q', 'model.layers.11.self_attn.o_proj.meta.zero_q', 'model.layers.11.self_attn.q_proj.W_q', 'model.layers.11.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.q_proj.meta.scale_q', 'model.layers.11.self_attn.q_proj.meta.zero_q', 'model.layers.11.self_attn.v_proj.W_q', 'model.layers.11.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.v_proj.meta.scale_q', 'model.layers.11.self_attn.v_proj.meta.zero_q', 'model.layers.12.block_sparse_moe.gate.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.W_q', 'model.layers.12.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.k_proj.meta.scale_q', 'model.layers.12.self_attn.k_proj.meta.zero_q', 'model.layers.12.self_attn.o_proj.W_q', 'model.layers.12.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.o_proj.meta.scale_q', 'model.layers.12.self_attn.o_proj.meta.zero_q', 'model.layers.12.self_attn.q_proj.W_q', 'model.layers.12.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.q_proj.meta.scale_q', 'model.layers.12.self_attn.q_proj.meta.zero_q', 'model.layers.12.self_attn.v_proj.W_q', 'model.layers.12.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.v_proj.meta.scale_q', 'model.layers.12.self_attn.v_proj.meta.zero_q', 'model.layers.13.block_sparse_moe.gate.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.W_q', 'model.layers.13.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.k_proj.meta.scale_q', 'model.layers.13.self_attn.k_proj.meta.zero_q', 'model.layers.13.self_attn.o_proj.W_q', 'model.layers.13.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.o_proj.meta.scale_q', 'model.layers.13.self_attn.o_proj.meta.zero_q', 'model.layers.13.self_attn.q_proj.W_q', 'model.layers.13.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.q_proj.meta.scale_q', 'model.layers.13.self_attn.q_proj.meta.zero_q', 'model.layers.13.self_attn.v_proj.W_q', 'model.layers.13.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.v_proj.meta.scale_q', 'model.layers.13.self_attn.v_proj.meta.zero_q', 'model.layers.14.block_sparse_moe.gate.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.W_q', 'model.layers.14.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.k_proj.meta.scale_q', 'model.layers.14.self_attn.k_proj.meta.zero_q', 'model.layers.14.self_attn.o_proj.W_q', 'model.layers.14.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.o_proj.meta.scale_q', 'model.layers.14.self_attn.o_proj.meta.zero_q', 'model.layers.14.self_attn.q_proj.W_q', 'model.layers.14.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.q_proj.meta.scale_q', 'model.layers.14.self_attn.q_proj.meta.zero_q', 'model.layers.14.self_attn.v_proj.W_q', 'model.layers.14.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.v_proj.meta.scale_q', 'model.layers.14.self_attn.v_proj.meta.zero_q', 'model.layers.15.block_sparse_moe.gate.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.W_q', 'model.layers.15.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.k_proj.meta.scale_q', 'model.layers.15.self_attn.k_proj.meta.zero_q', 'model.layers.15.self_attn.o_proj.W_q', 'model.layers.15.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.o_proj.meta.scale_q', 'model.layers.15.self_attn.o_proj.meta.zero_q', 'model.layers.15.self_attn.q_proj.W_q', 'model.layers.15.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.q_proj.meta.scale_q', 'model.layers.15.self_attn.q_proj.meta.zero_q', 'model.layers.15.self_attn.v_proj.W_q', 'model.layers.15.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.v_proj.meta.scale_q', 'model.layers.15.self_attn.v_proj.meta.zero_q', 'model.layers.16.block_sparse_moe.gate.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.W_q', 'model.layers.16.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.k_proj.meta.scale_q', 'model.layers.16.self_attn.k_proj.meta.zero_q', 'model.layers.16.self_attn.o_proj.W_q', 'model.layers.16.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.o_proj.meta.scale_q', 'model.layers.16.self_attn.o_proj.meta.zero_q', 'model.layers.16.self_attn.q_proj.W_q', 'model.layers.16.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.q_proj.meta.scale_q', 'model.layers.16.self_attn.q_proj.meta.zero_q', 'model.layers.16.self_attn.v_proj.W_q', 'model.layers.16.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.v_proj.meta.scale_q', 'model.layers.16.self_attn.v_proj.meta.zero_q', 'model.layers.17.block_sparse_moe.gate.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.W_q', 'model.layers.17.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.k_proj.meta.scale_q', 'model.layers.17.self_attn.k_proj.meta.zero_q', 'model.layers.17.self_attn.o_proj.W_q', 'model.layers.17.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.o_proj.meta.scale_q', 'model.layers.17.self_attn.o_proj.meta.zero_q', 'model.layers.17.self_attn.q_proj.W_q', 'model.layers.17.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.q_proj.meta.scale_q', 'model.layers.17.self_attn.q_proj.meta.zero_q', 'model.layers.17.self_attn.v_proj.W_q', 'model.layers.17.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.v_proj.meta.scale_q', 'model.layers.17.self_attn.v_proj.meta.zero_q', 'model.layers.18.block_sparse_moe.gate.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.W_q', 'model.layers.18.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.k_proj.meta.scale_q', 'model.layers.18.self_attn.k_proj.meta.zero_q', 'model.layers.18.self_attn.o_proj.W_q', 'model.layers.18.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.o_proj.meta.scale_q', 'model.layers.18.self_attn.o_proj.meta.zero_q', 'model.layers.18.self_attn.q_proj.W_q', 'model.layers.18.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.q_proj.meta.scale_q', 'model.layers.18.self_attn.q_proj.meta.zero_q', 'model.layers.18.self_attn.v_proj.W_q', 'model.layers.18.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.v_proj.meta.scale_q', 'model.layers.18.self_attn.v_proj.meta.zero_q', 'model.layers.19.block_sparse_moe.gate.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.W_q', 'model.layers.19.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.k_proj.meta.scale_q', 'model.layers.19.self_attn.k_proj.meta.zero_q', 'model.layers.19.self_attn.o_proj.W_q', 'model.layers.19.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.o_proj.meta.scale_q', 'model.layers.19.self_attn.o_proj.meta.zero_q', 'model.layers.19.self_attn.q_proj.W_q', 'model.layers.19.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.q_proj.meta.scale_q', 'model.layers.19.self_attn.q_proj.meta.zero_q', 'model.layers.19.self_attn.v_proj.W_q', 'model.layers.19.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.v_proj.meta.scale_q', 'model.layers.19.self_attn.v_proj.meta.zero_q', 'model.layers.2.block_sparse_moe.gate.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.W_q', 'model.layers.2.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.k_proj.meta.scale_q', 'model.layers.2.self_attn.k_proj.meta.zero_q', 'model.layers.2.self_attn.o_proj.W_q', 'model.layers.2.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.o_proj.meta.scale_q', 'model.layers.2.self_attn.o_proj.meta.zero_q', 'model.layers.2.self_attn.q_proj.W_q', 'model.layers.2.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.q_proj.meta.scale_q', 'model.layers.2.self_attn.q_proj.meta.zero_q', 'model.layers.2.self_attn.v_proj.W_q', 'model.layers.2.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.v_proj.meta.scale_q', 'model.layers.2.self_attn.v_proj.meta.zero_q', 'model.layers.20.block_sparse_moe.gate.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.W_q', 'model.layers.20.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.k_proj.meta.scale_q', 'model.layers.20.self_attn.k_proj.meta.zero_q', 'model.layers.20.self_attn.o_proj.W_q', 'model.layers.20.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.o_proj.meta.scale_q', 'model.layers.20.self_attn.o_proj.meta.zero_q', 'model.layers.20.self_attn.q_proj.W_q', 'model.layers.20.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.q_proj.meta.scale_q', 'model.layers.20.self_attn.q_proj.meta.zero_q', 'model.layers.20.self_attn.v_proj.W_q', 'model.layers.20.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.v_proj.meta.scale_q', 'model.layers.20.self_attn.v_proj.meta.zero_q', 'model.layers.21.block_sparse_moe.gate.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.W_q', 'model.layers.21.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.k_proj.meta.scale_q', 'model.layers.21.self_attn.k_proj.meta.zero_q', 'model.layers.21.self_attn.o_proj.W_q', 'model.layers.21.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.o_proj.meta.scale_q', 'model.layers.21.self_attn.o_proj.meta.zero_q', 'model.layers.21.self_attn.q_proj.W_q', 'model.layers.21.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.q_proj.meta.scale_q', 'model.layers.21.self_attn.q_proj.meta.zero_q', 'model.layers.21.self_attn.v_proj.W_q', 'model.layers.21.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.v_proj.meta.scale_q', 'model.layers.21.self_attn.v_proj.meta.zero_q', 'model.layers.22.block_sparse_moe.gate.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.W_q', 'model.layers.22.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.k_proj.meta.scale_q', 'model.layers.22.self_attn.k_proj.meta.zero_q', 'model.layers.22.self_attn.o_proj.W_q', 'model.layers.22.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.o_proj.meta.scale_q', 'model.layers.22.self_attn.o_proj.meta.zero_q', 'model.layers.22.self_attn.q_proj.W_q', 'model.layers.22.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.q_proj.meta.scale_q', 'model.layers.22.self_attn.q_proj.meta.zero_q', 'model.layers.22.self_attn.v_proj.W_q', 'model.layers.22.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.v_proj.meta.scale_q', 'model.layers.22.self_attn.v_proj.meta.zero_q', 'model.layers.23.block_sparse_moe.gate.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.W_q', 'model.layers.23.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.k_proj.meta.scale_q', 'model.layers.23.self_attn.k_proj.meta.zero_q', 'model.layers.23.self_attn.o_proj.W_q', 'model.layers.23.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.o_proj.meta.scale_q', 'model.layers.23.self_attn.o_proj.meta.zero_q', 'model.layers.23.self_attn.q_proj.W_q', 'model.layers.23.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.q_proj.meta.scale_q', 'model.layers.23.self_attn.q_proj.meta.zero_q', 'model.layers.23.self_attn.v_proj.W_q', 'model.layers.23.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.v_proj.meta.scale_q', 'model.layers.23.self_attn.v_proj.meta.zero_q', 'model.layers.24.block_sparse_moe.gate.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.W_q', 'model.layers.24.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.k_proj.meta.scale_q', 'model.layers.24.self_attn.k_proj.meta.zero_q', 'model.layers.24.self_attn.o_proj.W_q', 'model.layers.24.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.o_proj.meta.scale_q', 'model.layers.24.self_attn.o_proj.meta.zero_q', 'model.layers.24.self_attn.q_proj.W_q', 'model.layers.24.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.q_proj.meta.scale_q', 'model.layers.24.self_attn.q_proj.meta.zero_q', 'model.layers.24.self_attn.v_proj.W_q', 'model.layers.24.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.v_proj.meta.scale_q', 'model.layers.24.self_attn.v_proj.meta.zero_q', 'model.layers.25.block_sparse_moe.gate.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.W_q', 'model.layers.25.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.k_proj.meta.scale_q', 'model.layers.25.self_attn.k_proj.meta.zero_q', 'model.layers.25.self_attn.o_proj.W_q', 'model.layers.25.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.o_proj.meta.scale_q', 'model.layers.25.self_attn.o_proj.meta.zero_q', 'model.layers.25.self_attn.q_proj.W_q', 'model.layers.25.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.q_proj.meta.scale_q', 'model.layers.25.self_attn.q_proj.meta.zero_q', 'model.layers.25.self_attn.v_proj.W_q', 'model.layers.25.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.v_proj.meta.scale_q', 'model.layers.25.self_attn.v_proj.meta.zero_q', 'model.layers.26.block_sparse_moe.gate.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.W_q', 'model.layers.26.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.k_proj.meta.scale_q', 'model.layers.26.self_attn.k_proj.meta.zero_q', 'model.layers.26.self_attn.o_proj.W_q', 'model.layers.26.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.o_proj.meta.scale_q', 'model.layers.26.self_attn.o_proj.meta.zero_q', 'model.layers.26.self_attn.q_proj.W_q', 'model.layers.26.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.q_proj.meta.scale_q', 'model.layers.26.self_attn.q_proj.meta.zero_q', 'model.layers.26.self_attn.v_proj.W_q', 'model.layers.26.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.v_proj.meta.scale_q', 'model.layers.26.self_attn.v_proj.meta.zero_q', 'model.layers.27.block_sparse_moe.gate.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.W_q', 'model.layers.27.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.k_proj.meta.scale_q', 'model.layers.27.self_attn.k_proj.meta.zero_q', 'model.layers.27.self_attn.o_proj.W_q', 'model.layers.27.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.o_proj.meta.scale_q', 'model.layers.27.self_attn.o_proj.meta.zero_q', 'model.layers.27.self_attn.q_proj.W_q', 'model.layers.27.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.q_proj.meta.scale_q', 'model.layers.27.self_attn.q_proj.meta.zero_q', 'model.layers.27.self_attn.v_proj.W_q', 'model.layers.27.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.v_proj.meta.scale_q', 'model.layers.27.self_attn.v_proj.meta.zero_q', 'model.layers.28.block_sparse_moe.gate.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.W_q', 'model.layers.28.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.k_proj.meta.scale_q', 'model.layers.28.self_attn.k_proj.meta.zero_q', 'model.layers.28.self_attn.o_proj.W_q', 'model.layers.28.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.o_proj.meta.scale_q', 'model.layers.28.self_attn.o_proj.meta.zero_q', 'model.layers.28.self_attn.q_proj.W_q', 'model.layers.28.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.q_proj.meta.scale_q', 'model.layers.28.self_attn.q_proj.meta.zero_q', 'model.layers.28.self_attn.v_proj.W_q', 'model.layers.28.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.v_proj.meta.scale_q', 'model.layers.28.self_attn.v_proj.meta.zero_q', 'model.layers.29.block_sparse_moe.gate.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.W_q', 'model.layers.29.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.k_proj.meta.scale_q', 'model.layers.29.self_attn.k_proj.meta.zero_q', 'model.layers.29.self_attn.o_proj.W_q', 'model.layers.29.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.o_proj.meta.scale_q', 'model.layers.29.self_attn.o_proj.meta.zero_q', 'model.layers.29.self_attn.q_proj.W_q', 'model.layers.29.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.q_proj.meta.scale_q', 'model.layers.29.self_attn.q_proj.meta.zero_q', 'model.layers.29.self_attn.v_proj.W_q', 'model.layers.29.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.v_proj.meta.scale_q', 'model.layers.29.self_attn.v_proj.meta.zero_q', 'model.layers.3.block_sparse_moe.gate.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.W_q', 'model.layers.3.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.k_proj.meta.scale_q', 'model.layers.3.self_attn.k_proj.meta.zero_q', 'model.layers.3.self_attn.o_proj.W_q', 'model.layers.3.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.o_proj.meta.scale_q', 'model.layers.3.self_attn.o_proj.meta.zero_q', 'model.layers.3.self_attn.q_proj.W_q', 'model.layers.3.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.q_proj.meta.scale_q', 'model.layers.3.self_attn.q_proj.meta.zero_q', 'model.layers.3.self_attn.v_proj.W_q', 'model.layers.3.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.v_proj.meta.scale_q', 'model.layers.3.self_attn.v_proj.meta.zero_q', 'model.layers.30.block_sparse_moe.gate.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.W_q', 'model.layers.30.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.k_proj.meta.scale_q', 'model.layers.30.self_attn.k_proj.meta.zero_q', 'model.layers.30.self_attn.o_proj.W_q', 'model.layers.30.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.o_proj.meta.scale_q', 'model.layers.30.self_attn.o_proj.meta.zero_q', 'model.layers.30.self_attn.q_proj.W_q', 'model.layers.30.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.q_proj.meta.scale_q', 'model.layers.30.self_attn.q_proj.meta.zero_q', 'model.layers.30.self_attn.v_proj.W_q', 'model.layers.30.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.v_proj.meta.scale_q', 'model.layers.30.self_attn.v_proj.meta.zero_q', 'model.layers.31.block_sparse_moe.gate.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.W_q', 'model.layers.31.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.k_proj.meta.scale_q', 'model.layers.31.self_attn.k_proj.meta.zero_q', 'model.layers.31.self_attn.o_proj.W_q', 'model.layers.31.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.o_proj.meta.scale_q', 'model.layers.31.self_attn.o_proj.meta.zero_q', 'model.layers.31.self_attn.q_proj.W_q', 'model.layers.31.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.q_proj.meta.scale_q', 'model.layers.31.self_attn.q_proj.meta.zero_q', 'model.layers.31.self_attn.v_proj.W_q', 'model.layers.31.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.v_proj.meta.scale_q', 'model.layers.31.self_attn.v_proj.meta.zero_q', 'model.layers.4.block_sparse_moe.gate.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.W_q', 'model.layers.4.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.k_proj.meta.scale_q', 'model.layers.4.self_attn.k_proj.meta.zero_q', 'model.layers.4.self_attn.o_proj.W_q', 'model.layers.4.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.o_proj.meta.scale_q', 'model.layers.4.self_attn.o_proj.meta.zero_q', 'model.layers.4.self_attn.q_proj.W_q', 'model.layers.4.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.q_proj.meta.scale_q', 'model.layers.4.self_attn.q_proj.meta.zero_q', 'model.layers.4.self_attn.v_proj.W_q', 'model.layers.4.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.v_proj.meta.scale_q', 'model.layers.4.self_attn.v_proj.meta.zero_q', 'model.layers.5.block_sparse_moe.gate.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.W_q', 'model.layers.5.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.k_proj.meta.scale_q', 'model.layers.5.self_attn.k_proj.meta.zero_q', 'model.layers.5.self_attn.o_proj.W_q', 'model.layers.5.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.o_proj.meta.scale_q', 'model.layers.5.self_attn.o_proj.meta.zero_q', 'model.layers.5.self_attn.q_proj.W_q', 'model.layers.5.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.q_proj.meta.scale_q', 'model.layers.5.self_attn.q_proj.meta.zero_q', 'model.layers.5.self_attn.v_proj.W_q', 'model.layers.5.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.v_proj.meta.scale_q', 'model.layers.5.self_attn.v_proj.meta.zero_q', 'model.layers.6.block_sparse_moe.gate.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.W_q', 'model.layers.6.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.k_proj.meta.scale_q', 'model.layers.6.self_attn.k_proj.meta.zero_q', 'model.layers.6.self_attn.o_proj.W_q', 'model.layers.6.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.o_proj.meta.scale_q', 'model.layers.6.self_attn.o_proj.meta.zero_q', 'model.layers.6.self_attn.q_proj.W_q', 'model.layers.6.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.q_proj.meta.scale_q', 'model.layers.6.self_attn.q_proj.meta.zero_q', 'model.layers.6.self_attn.v_proj.W_q', 'model.layers.6.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.v_proj.meta.scale_q', 'model.layers.6.self_attn.v_proj.meta.zero_q', 'model.layers.7.block_sparse_moe.gate.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.W_q', 'model.layers.7.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.k_proj.meta.scale_q', 'model.layers.7.self_attn.k_proj.meta.zero_q', 'model.layers.7.self_attn.o_proj.W_q', 'model.layers.7.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.o_proj.meta.scale_q', 'model.layers.7.self_attn.o_proj.meta.zero_q', 'model.layers.7.self_attn.q_proj.W_q', 'model.layers.7.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.q_proj.meta.scale_q', 'model.layers.7.self_attn.q_proj.meta.zero_q', 'model.layers.7.self_attn.v_proj.W_q', 'model.layers.7.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.v_proj.meta.scale_q', 'model.layers.7.self_attn.v_proj.meta.zero_q', 'model.layers.8.block_sparse_moe.gate.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.W_q', 'model.layers.8.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.k_proj.meta.scale_q', 'model.layers.8.self_attn.k_proj.meta.zero_q', 'model.layers.8.self_attn.o_proj.W_q', 'model.layers.8.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.o_proj.meta.scale_q', 'model.layers.8.self_attn.o_proj.meta.zero_q', 'model.layers.8.self_attn.q_proj.W_q', 'model.layers.8.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.q_proj.meta.scale_q', 'model.layers.8.self_attn.q_proj.meta.zero_q', 'model.layers.8.self_attn.v_proj.W_q', 'model.layers.8.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.v_proj.meta.scale_q', 'model.layers.8.self_attn.v_proj.meta.zero_q', 'model.layers.9.block_sparse_moe.gate.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.W_q', 'model.layers.9.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.k_proj.meta.scale_q', 'model.layers.9.self_attn.k_proj.meta.zero_q', 'model.layers.9.self_attn.o_proj.W_q', 'model.layers.9.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.o_proj.meta.scale_q', 'model.layers.9.self_attn.o_proj.meta.zero_q', 'model.layers.9.self_attn.q_proj.W_q', 'model.layers.9.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.q_proj.meta.scale_q', 'model.layers.9.self_attn.q_proj.meta.zero_q', 'model.layers.9.self_attn.v_proj.W_q', 'model.layers.9.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.v_proj.meta.scale_q', 'model.layers.9.self_attn.v_proj.meta.zero_q', 'model.norm.weight'])\n",
            "start\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "1\n",
            "2\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.884068489074707 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.946568489074707 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.009068489074707 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.071568489074707 GB\n",
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.386344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   3%|▎         | 1/32 [00:04<02:29,  4.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.448844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.448844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.448844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   6%|▋         | 2/32 [00:11<03:00,  6.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.573844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   9%|▉         | 3/32 [00:18<03:01,  6.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except NameError:\n",
        "    pass\n",
        "from src.build_model import OffloadConfig, QuantConfig, build_model\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "# state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(quantized_model_name)\n",
        "config.num_experts_per_tok = 1\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "print(f\"initial GPU Memory Usage: {peak_memory_usage} GB\") \n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
        "offload_per_layer = 7\n",
        "# offload_per_layer = 5\n",
        "###############################################################\n",
        "\n",
        "num_experts = config.num_local_experts\n",
        "print(\"number experts:\", num_experts)\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "\n",
        "attn_config = BaseQuantizeConfig(\n",
        "    nbits=4,\n",
        "    group_size=64,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
        "\n",
        "\n",
        "ffn_config = BaseQuantizeConfig(\n",
        "    nbits=2,\n",
        "    group_size=16,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "    device=device,\n",
        "    quant_config=quant_config,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4hBFYtPTUzT"
      },
      "source": [
        "## Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zf4GkspecSm8",
        "outputId": "a4ce58f9-30cc-414d-fa66-e5a28262d666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mixtral: Sure! Here's a light-hearted joke for you:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "I hope that gave you a chuckle. Do you have any specific topic you'd like to hear a joke about, or just a general joke? I'm here to help with any question you have to the best of my ability.\n",
            "\n",
            "\n",
            "Peak GPU Memory Usage: 36.90357971191406 GB\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "past_key_values = None\n",
        "sequence = None\n",
        "\n",
        "seq_len = 0\n",
        "user_input = \"tell me a joke\"\n",
        "print(\"\\n\")\n",
        "\n",
        "user_entry = dict(role=\"user\", content=user_input)\n",
        "input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n",
        "\n",
        "if past_key_values is None:\n",
        "  attention_mask = torch.ones_like(input_ids)\n",
        "else:\n",
        "  seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
        "  attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n",
        "\n",
        "print(\"Mixtral: \", end=\"\")\n",
        "result = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  attention_mask=attention_mask,\n",
        "  past_key_values=past_key_values,\n",
        "  streamer=streamer,\n",
        "  do_sample=True,\n",
        "  temperature=0.9,\n",
        "  top_p=0.9,\n",
        "  max_new_tokens=512,\n",
        "  pad_token_id=tokenizer.eos_token_id,\n",
        "  return_dict_in_generate=True,\n",
        "  output_hidden_states=True,\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "sequence = result[\"sequences\"]\n",
        "past_key_values = result[\"past_key_values\"]\n",
        "peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "print(f\"Peak GPU Memory Usage: {peak_memory_usage} GB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03638a96888649dd9dc625a623eb6c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffa7a23d200843068267e4e21553e98d",
              "IPY_MODEL_ad60a3884afe47a9ab9b4a70a461d49b",
              "IPY_MODEL_2248396d844b4f159fd895fa100f5875"
            ],
            "layout": "IPY_MODEL_073cc65432b44fe29eae1c91471ff970"
          }
        },
        "073cc65432b44fe29eae1c91471ff970": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108098be0bdc49e791fb784d487fe175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "137e376507144fccbabb12bf1268f699": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bfcbb901574f0583b8af3733f58b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a6a03868cdc4fb286b4ec580da9a391": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2248396d844b4f159fd895fa100f5875": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddc0e14f4448489499a534c5cf2f5945",
            "placeholder": "​",
            "style": "IPY_MODEL_6e8a37d65bf64bbaac84230613164936",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "243efd24b8994278a406c597039d0fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_325f9b174633405483cf60cff12d652b",
            "placeholder": "​",
            "style": "IPY_MODEL_e60bfa41b7454322a908e01bb9caed65",
            "value": " 720/720 [00:00&lt;00:00, 20.2kB/s]"
          }
        },
        "325f9b174633405483cf60cff12d652b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39efadb9ccb048deab07969016e7bd38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494bb8b09d1f4f2abbc111eb5e6da130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a458bdef43543d0b7b7edc2883e2dc1",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f85bc3a52a540558999a1ff25a813b3",
            "value": 720
          }
        },
        "67dc7ff4d0fe4af796308b2c8355d150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b41dad78195344a6a77b570b1783f7a0",
              "IPY_MODEL_494bb8b09d1f4f2abbc111eb5e6da130",
              "IPY_MODEL_243efd24b8994278a406c597039d0fc4"
            ],
            "layout": "IPY_MODEL_c2ccdf34885d41a0ab3f9b40c775b25a"
          }
        },
        "6a458bdef43543d0b7b7edc2883e2dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8a37d65bf64bbaac84230613164936": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77eb47b1a453410b860b91a01357e98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b7b50c298de414b85cc622ef5660dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1e82a41293d41d6aa8807d9d575e86b",
              "IPY_MODEL_a19471c4ecce428bbebab2a90f535861",
              "IPY_MODEL_8a2c815f41b8483692a79165ebeaf3f6"
            ],
            "layout": "IPY_MODEL_9782a56e37e24892a78b282295ae5ac0"
          }
        },
        "7f85bc3a52a540558999a1ff25a813b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83bcda80b79a4e6eaf5b0f27bcbc0ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a2c815f41b8483692a79165ebeaf3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b792acc1fff049fca658f836557c8e6f",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf2b97ed0b7415fa02f38c8bb0e3009",
            "value": " 32/32 [01:36&lt;00:00,  3.03s/it]"
          }
        },
        "9782a56e37e24892a78b282295ae5ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be1146cefb9416cb875741640dca611": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a19471c4ecce428bbebab2a90f535861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0dba9bd6ee94ca1a0084eabeac0e1ca",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83bcda80b79a4e6eaf5b0f27bcbc0ee0",
            "value": 32
          }
        },
        "ad60a3884afe47a9ab9b4a70a461d49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_108098be0bdc49e791fb784d487fe175",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77eb47b1a453410b860b91a01357e98f",
            "value": 0
          }
        },
        "b41dad78195344a6a77b570b1783f7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39efadb9ccb048deab07969016e7bd38",
            "placeholder": "​",
            "style": "IPY_MODEL_d36446b0a9534f41a9f21b865b0a08f4",
            "value": "config.json: 100%"
          }
        },
        "b792acc1fff049fca658f836557c8e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0dba9bd6ee94ca1a0084eabeac0e1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ccdf34885d41a0ab3f9b40c775b25a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e82a41293d41d6aa8807d9d575e86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be1146cefb9416cb875741640dca611",
            "placeholder": "​",
            "style": "IPY_MODEL_15bfcbb901574f0583b8af3733f58b7a",
            "value": "Loading experts: 100%"
          }
        },
        "d36446b0a9534f41a9f21b865b0a08f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcf2b97ed0b7415fa02f38c8bb0e3009": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddc0e14f4448489499a534c5cf2f5945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e60bfa41b7454322a908e01bb9caed65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffa7a23d200843068267e4e21553e98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137e376507144fccbabb12bf1268f699",
            "placeholder": "​",
            "style": "IPY_MODEL_1a6a03868cdc4fb286b4ec580da9a391",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
