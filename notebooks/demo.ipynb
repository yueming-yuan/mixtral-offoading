{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1moHJ1TdhO"
      },
      "source": [
        "# Mixtral in Colab\n",
        "\n",
        "Welcome! In this notebook you can run [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) with decent generation speed **right in Google Colab or on a consumer-grade GPU**. This was made possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy.\n",
        "\n",
        "To learn more, read our [tech report](https://arxiv.org/abs/2312.17238) or check out the [repo](https://github.com/dvmazur/mixtral-offloading) on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-dvAX_hKZT4"
      },
      "source": [
        "One will need approximately 16 GB of VRAM and 11 GB of RAM to run this notebook and generate somewhat long texts.\n",
        "\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>How to balance between RAM and GPU VRAM usage</summary>\n",
        "\n",
        "You can balance between RAM and GPU VRAM usage by changing <code>offload_per_layer</code> variable in the <a href=\"#scrollTo=_mIpePTMFyRY&line=10&uniqifier=1\">Initialize model</a> section. Increasing <code>offload_per_layer</code> will decrease GPU VRAM usage, increase RAM usage and decrease generation speed. Decreasing <code>offload_per_layer</code> will have the opposite effect.\n",
        "\n",
        "Note that this notebook should run normally in Google Colab with <code>offload_per_layer = 4</code>, but may crush with other values. However, if you run this somewhere else, you're free to play with this variable.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MhvkC7TKEL"
      },
      "source": [
        "## Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f7qY7ebqX7T7",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# fix numpy in colab\n",
        "import numpy\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# fix triton in colab\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export HF_HOME=/scratch/bcjw/yyuan6/.cache\n",
        "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "#!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "# !ldconfig /usr/lib64-nvidia\n",
        "\n",
        "#!git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n",
        "#!cd mixtral-offloading && pip install -q -r requirements.txt\n",
        "#!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
        "\n",
        "# clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "03638a96888649dd9dc625a623eb6c34",
            "ffa7a23d200843068267e4e21553e98d",
            "ad60a3884afe47a9ab9b4a70a461d49b",
            "2248396d844b4f159fd895fa100f5875",
            "073cc65432b44fe29eae1c91471ff970",
            "137e376507144fccbabb12bf1268f699",
            "1a6a03868cdc4fb286b4ec580da9a391",
            "108098be0bdc49e791fb784d487fe175",
            "77eb47b1a453410b860b91a01357e98f",
            "ddc0e14f4448489499a534c5cf2f5945",
            "6e8a37d65bf64bbaac84230613164936"
          ]
        },
        "id": "GgpjnV7fV49W",
        "outputId": "1c10ea85-61e5-4572-aac9-0f02c4d2cb30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36mhqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/bcjw/yyuan6/miniconda3/envs/cuda122/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading\")\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from hqq.core.quantize import BaseQuantizeConfig\n",
        "# from huggingface_hub import snapshot_download\n",
        "# from IPython.display import clear_output\n",
        "# from tqdm.auto import trange\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "# from transformers.utils import logging as hf_logging\n",
        "from src.build_model import OffloadConfig, QuantConfig, build_model, build_model_without_quant\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkSYibHcTQsH"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial GPU Memory Usage: 0.0 GB\n",
            "number experts: 8\n",
            "OffloadConfig(main_size=64, offload_size=192, buffer_size=4, offload_per_layer=6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/bcjw/yyuan6/miniconda3/envs/cuda122/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/test_dir/model-00001-of-00257.safetensors\n",
            "{'lm_head.weight': tensor([[-0.0006, -0.0018,  0.0006,  ..., -0.0029, -0.0026, -0.0023],\n",
            "        [-0.0006, -0.0018,  0.0006,  ..., -0.0029, -0.0025, -0.0023],\n",
            "        [ 0.0043,  0.0060,  0.0060,  ..., -0.0079, -0.0052,  0.0060],\n",
            "        ...,\n",
            "        [-0.0014,  0.0058, -0.0052,  ...,  0.0016, -0.0154, -0.0018],\n",
            "        [ 0.0107, -0.0035, -0.0057,  ...,  0.0097, -0.0006, -0.0059],\n",
            "        [ 0.0095,  0.0016,  0.0001,  ..., -0.0151, -0.0116, -0.0032]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.embed_tokens.weight': tensor([[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
            "         -0.0000e+00, -0.0000e+00],\n",
            "        [-1.5259e-02,  9.2030e-05, -1.6113e-02,  ...,  4.2915e-05,\n",
            "         -1.7212e-02,  2.2125e-03],\n",
            "        [-7.5531e-04, -4.6349e-04, -3.0136e-04,  ..., -2.0218e-04,\n",
            "         -3.6812e-04, -4.6730e-05],\n",
            "        ...,\n",
            "        [ 8.9111e-03, -1.2817e-02,  5.3101e-03,  ...,  5.4626e-03,\n",
            "          3.7384e-03,  3.6926e-03],\n",
            "        [ 1.9455e-03,  1.8677e-02, -4.9133e-03,  ...,  1.3245e-02,\n",
            "          4.6692e-03,  5.5847e-03],\n",
            "        [ 3.8605e-03,  1.6357e-02,  1.5831e-04,  ..., -7.9346e-03,\n",
            "         -9.3384e-03, -2.4414e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.0.block_sparse_moe.gate.weight': tensor([[ 0.0177, -0.0320, -0.0007,  ...,  0.0050, -0.0262, -0.0253],\n",
            "        [ 0.0135,  0.0006,  0.0107,  ...,  0.0032,  0.0066,  0.0006],\n",
            "        [-0.0042,  0.0339,  0.0159,  ..., -0.0034,  0.0119, -0.0090],\n",
            "        ...,\n",
            "        [-0.0025,  0.0315, -0.0129,  ..., -0.0064,  0.0070,  0.0234],\n",
            "        [ 0.0047,  0.0149, -0.0081,  ..., -0.0175, -0.0049,  0.0095],\n",
            "        [-0.0525, -0.0233,  0.0053,  ...,  0.0181, -0.0066,  0.0033]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.0.input_layernorm.weight': tensor([-0.0003, -0.0041,  0.0002,  ...,  0.0153,  0.0001,  0.0005],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.0.post_attention_layernorm.weight': tensor([0.1816, 0.1875, 0.1816,  ..., 0.1875, 0.1748, 0.1797], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.0.self_attn.k_proj.weight': tensor([[ 2.7084e-04,  6.1035e-03, -5.3406e-04,  ..., -2.8687e-03,\n",
            "         -3.4332e-04, -3.2425e-04],\n",
            "        [-1.0395e-04, -5.9509e-03,  5.7983e-04,  ..., -1.2131e-03,\n",
            "         -1.6212e-04,  1.3638e-04],\n",
            "        [-1.6212e-04,  2.9602e-03,  1.8120e-04,  ...,  1.6724e-02,\n",
            "          5.9891e-04,  2.3246e-05],\n",
            "        ...,\n",
            "        [-1.2589e-04,  1.2207e-02,  2.1076e-04,  ...,  9.1553e-03,\n",
            "         -2.0504e-04, -6.5565e-07],\n",
            "        [ 1.2665e-03, -9.8877e-03, -1.0395e-04,  ...,  8.5449e-03,\n",
            "         -4.4632e-04,  2.0599e-03],\n",
            "        [-6.3896e-05, -1.0315e-02, -1.8787e-04,  ..., -8.0566e-03,\n",
            "          1.8787e-04, -2.0504e-05]], device='cuda:0', dtype=torch.float16), 'model.layers.0.self_attn.o_proj.weight': tensor([[ 0.0082, -0.0011,  0.0118,  ..., -0.0047,  0.0009, -0.0050],\n",
            "        [-0.0061, -0.0010,  0.0079,  ...,  0.0016,  0.0058,  0.0031],\n",
            "        [-0.0133, -0.0024, -0.0096,  ...,  0.0017,  0.0098, -0.0110],\n",
            "        ...,\n",
            "        [ 0.0012, -0.0058,  0.0142,  ..., -0.0142,  0.0028,  0.0153],\n",
            "        [ 0.0059, -0.0025, -0.0072,  ..., -0.0094, -0.0060,  0.0056],\n",
            "        [ 0.0116,  0.0004, -0.0006,  ..., -0.0007,  0.0081,  0.0048]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.0.self_attn.q_proj.weight': tensor([[-2.6941e-05, -3.0518e-03,  5.1880e-04,  ...,  7.0496e-03,\n",
            "          1.3471e-05,  5.2643e-04],\n",
            "        [ 1.2207e-04,  2.3041e-03, -1.4496e-04,  ..., -6.9885e-03,\n",
            "         -2.2125e-04, -5.8746e-04],\n",
            "        [-4.6492e-05, -6.6223e-03,  2.5749e-04,  ..., -6.3477e-03,\n",
            "          4.4060e-04, -6.6757e-04],\n",
            "        ...,\n",
            "        [-1.8358e-05, -3.7842e-03, -5.1498e-04,  ...,  1.2970e-03,\n",
            "          1.4782e-04, -1.4591e-04],\n",
            "        [ 7.7248e-05,  7.8583e-04, -4.1962e-04,  ..., -1.4496e-03,\n",
            "          9.4891e-05,  3.6001e-05],\n",
            "        [ 1.7881e-05,  3.6926e-03,  5.4550e-04,  ..., -1.5793e-03,\n",
            "         -1.7071e-04,  2.3174e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.0.self_attn.v_proj.weight': tensor([[ 0.0007, -0.0043, -0.0010,  ...,  0.0129,  0.0003, -0.0002],\n",
            "        [-0.0002, -0.0014, -0.0009,  ...,  0.0095,  0.0019,  0.0017],\n",
            "        [-0.0015, -0.0057,  0.0020,  ..., -0.0007, -0.0016, -0.0005],\n",
            "        ...,\n",
            "        [ 0.0023,  0.0054, -0.0021,  ...,  0.0061,  0.0039,  0.0034],\n",
            "        [ 0.0011, -0.0084,  0.0015,  ...,  0.0070, -0.0034, -0.0008],\n",
            "        [ 0.0018,  0.0040, -0.0027,  ..., -0.0003,  0.0038, -0.0004]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.1.block_sparse_moe.gate.weight': tensor([[ 0.0503, -0.0124, -0.0127,  ..., -0.0157, -0.0076,  0.0410],\n",
            "        [ 0.0408, -0.0062, -0.0126,  ..., -0.0177,  0.0325,  0.0239],\n",
            "        [-0.0322,  0.0078, -0.0007,  ...,  0.0068, -0.0171, -0.0048],\n",
            "        ...,\n",
            "        [-0.0137,  0.0135, -0.0041,  ...,  0.0098, -0.0255,  0.0071],\n",
            "        [-0.0233, -0.0104,  0.0160,  ...,  0.0188,  0.0164, -0.0238],\n",
            "        [ 0.0146,  0.0156,  0.0093,  ..., -0.0054,  0.0037,  0.0047]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.1.input_layernorm.weight': tensor([0.0830, 0.0518, 0.0007,  ..., 0.0884, 0.0591, 0.0481], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.1.post_attention_layernorm.weight': tensor([0.3945, 0.3984, 0.4004,  ..., 0.3965, 0.3945, 0.3945], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.1.self_attn.k_proj.weight': tensor([[-2.5482e-03,  1.5411e-03, -5.3024e-04,  ..., -4.5776e-03,\n",
            "          2.4872e-03,  3.1586e-03],\n",
            "        [ 9.1553e-03,  1.0620e-02,  1.3065e-04,  ..., -8.2016e-04,\n",
            "         -1.2817e-02, -1.5106e-03],\n",
            "        [-2.7466e-03, -1.0252e-04,  4.7493e-04,  ..., -4.3640e-03,\n",
            "          3.8605e-03, -1.7395e-03],\n",
            "        ...,\n",
            "        [-3.7079e-03,  5.2185e-03, -7.5340e-05,  ..., -1.4099e-02,\n",
            "         -1.9264e-04, -1.1368e-03],\n",
            "        [ 5.0964e-03, -1.0254e-02,  7.7057e-04,  ..., -3.0327e-04,\n",
            "          7.6599e-03,  2.4902e-02],\n",
            "        [ 1.8188e-02, -1.0376e-02, -1.7071e-04,  ...,  3.2501e-03,\n",
            "          2.7344e-02,  1.9287e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.1.self_attn.o_proj.weight': tensor([[ 0.0071, -0.0021, -0.0125,  ..., -0.0055, -0.0022,  0.0050],\n",
            "        [-0.0063,  0.0027,  0.0062,  ...,  0.0029,  0.0009, -0.0001],\n",
            "        [-0.0011, -0.0017,  0.0161,  ...,  0.0041, -0.0125, -0.0043],\n",
            "        ...,\n",
            "        [ 0.0002, -0.0074, -0.0008,  ...,  0.0045, -0.0042,  0.0054],\n",
            "        [-0.0045,  0.0054,  0.0032,  ...,  0.0079,  0.0012, -0.0057],\n",
            "        [-0.0011,  0.0067, -0.0088,  ...,  0.0077, -0.0004, -0.0017]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.1.self_attn.q_proj.weight': tensor([[ 2.5177e-03, -1.2939e-02, -2.7418e-05,  ..., -6.5231e-04,\n",
            "         -2.7771e-03,  1.4771e-02],\n",
            "        [ 1.9379e-03, -5.8594e-03, -6.0272e-04,  ..., -5.3711e-03,\n",
            "          6.5308e-03, -9.3384e-03],\n",
            "        [ 5.4321e-03, -1.0132e-02,  4.0436e-04,  ...,  5.2490e-03,\n",
            "         -8.4229e-03,  1.3611e-02],\n",
            "        ...,\n",
            "        [ 2.7008e-03, -4.5471e-03, -7.8201e-05,  ..., -6.5231e-04,\n",
            "         -2.3438e-02, -1.7334e-02],\n",
            "        [ 6.5613e-03,  3.7994e-03,  1.1539e-04,  ..., -1.4221e-02,\n",
            "          1.1658e-02,  1.4465e-02],\n",
            "        [ 1.6357e-02, -3.0518e-03, -7.0572e-05,  ..., -7.4463e-03,\n",
            "         -1.7700e-02, -1.1963e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.1.self_attn.v_proj.weight': tensor([[-9.6436e-03, -5.9204e-03,  1.1749e-03,  ..., -3.7689e-03,\n",
            "         -7.7820e-03,  1.8463e-03],\n",
            "        [ 8.6060e-03, -2.8419e-04,  5.7602e-04,  ..., -1.4221e-02,\n",
            "         -9.2773e-03,  5.5847e-03],\n",
            "        [ 7.6904e-03, -2.5787e-03,  3.0518e-04,  ...,  5.3101e-03,\n",
            "          1.5381e-02,  2.3071e-02],\n",
            "        ...,\n",
            "        [ 3.5553e-03, -3.2806e-03,  1.9684e-03,  ..., -1.1841e-02,\n",
            "          1.6235e-02, -1.8311e-02],\n",
            "        [ 1.0925e-02, -8.2397e-03,  1.1384e-05,  ..., -9.8877e-03,\n",
            "          8.1177e-03,  8.5449e-03],\n",
            "        [ 3.9978e-03,  1.8433e-02,  5.9128e-04,  ...,  3.1891e-03,\n",
            "         -7.2937e-03,  1.9264e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.10.block_sparse_moe.gate.weight': tensor([[-2.5558e-04,  4.9133e-03,  9.6512e-04,  ..., -1.1169e-02,\n",
            "          6.2256e-03, -4.1008e-05],\n",
            "        [-5.3101e-03,  3.0327e-04,  9.9182e-04,  ...,  6.8054e-03,\n",
            "         -8.0566e-03, -3.0670e-03],\n",
            "        [ 4.8523e-03,  9.3079e-04,  2.1667e-03,  ..., -1.0132e-02,\n",
            "          6.4392e-03,  6.8359e-03],\n",
            "        ...,\n",
            "        [-2.6245e-03, -3.5400e-03,  1.1826e-03,  ...,  1.4954e-02,\n",
            "         -5.0049e-03, -1.0223e-03],\n",
            "        [ 8.7891e-03, -3.8147e-03, -4.7302e-03,  ..., -2.6245e-03,\n",
            "         -8.4229e-03,  6.2256e-03],\n",
            "        [ 8.7357e-04,  5.5695e-04,  1.8234e-03,  ...,  2.1667e-03,\n",
            "          9.0332e-03, -5.0659e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.10.input_layernorm.weight': tensor([1.3281, 1.6016, 1.8203,  ..., 1.0781, 1.3516, 1.4531], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.10.post_attention_layernorm.weight': tensor([1.4297, 1.8828, 1.8359,  ..., 1.2812, 1.4531, 1.5547], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.10.self_attn.k_proj.weight': tensor([[ 8.1787e-03, -8.5449e-03, -9.4223e-04,  ...,  6.0730e-03,\n",
            "         -2.3499e-03, -5.3406e-05],\n",
            "        [ 4.1504e-03, -9.4604e-03, -1.1719e-02,  ..., -6.6223e-03,\n",
            "          8.3618e-03, -2.1362e-03],\n",
            "        [-1.2016e-04, -5.0049e-03, -9.9182e-04,  ..., -2.0020e-02,\n",
            "         -9.4604e-04,  1.1047e-02],\n",
            "        ...,\n",
            "        [-2.2339e-02,  4.3335e-03,  3.5095e-03,  ...,  7.7209e-03,\n",
            "          1.4465e-02, -2.0752e-02],\n",
            "        [-4.1504e-03, -8.0566e-03,  1.1047e-02,  ..., -3.6469e-03,\n",
            "         -2.3560e-02,  5.4932e-03],\n",
            "        [ 1.2695e-02,  2.1851e-02,  2.1362e-02,  ..., -1.2512e-02,\n",
            "          5.1270e-03, -2.1973e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.10.self_attn.o_proj.weight': tensor([[ 1.1536e-02, -1.3065e-04,  3.4027e-03,  ..., -1.3611e-02,\n",
            "          2.1515e-03, -9.1553e-03],\n",
            "        [ 1.5076e-02, -1.3306e-02, -1.0132e-02,  ...,  1.6861e-03,\n",
            "         -5.2795e-03, -1.0620e-02],\n",
            "        [ 7.2632e-03,  3.5286e-04,  3.6469e-03,  ..., -2.4170e-02,\n",
            "          1.6724e-02,  1.4282e-02],\n",
            "        ...,\n",
            "        [ 1.7212e-02, -4.8828e-03,  1.0559e-02,  ..., -7.0190e-03,\n",
            "          4.1809e-03, -5.6458e-03],\n",
            "        [ 1.6113e-02, -1.2207e-02, -3.1738e-03,  ...,  8.1787e-03,\n",
            "          5.9509e-04, -1.1475e-02],\n",
            "        [-1.6570e-05,  1.3657e-03, -2.6894e-04,  ...,  6.0120e-03,\n",
            "          4.6387e-03,  9.3994e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.10.self_attn.q_proj.weight': tensor([[ 0.0082, -0.0042,  0.0078,  ...,  0.0123,  0.0057, -0.0004],\n",
            "        [-0.0032, -0.0027,  0.0026,  ..., -0.0093,  0.0023,  0.0047],\n",
            "        [ 0.0167, -0.0264,  0.0039,  ...,  0.0091,  0.0099,  0.0083],\n",
            "        ...,\n",
            "        [-0.0149, -0.0014, -0.0209,  ..., -0.0019,  0.0042,  0.0113],\n",
            "        [-0.0060, -0.0043, -0.0116,  ...,  0.0305,  0.0003, -0.0049],\n",
            "        [ 0.0047,  0.0001, -0.0058,  ...,  0.0127,  0.0058, -0.0117]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.10.self_attn.v_proj.weight': tensor([[-0.0332, -0.0175, -0.0013,  ..., -0.0046, -0.0175,  0.0074],\n",
            "        [ 0.0036,  0.0124,  0.0024,  ...,  0.0256,  0.0327, -0.0041],\n",
            "        [ 0.0002,  0.0080,  0.0027,  ..., -0.0062,  0.0089, -0.0037],\n",
            "        ...,\n",
            "        [-0.0048,  0.0138, -0.0194,  ...,  0.0022, -0.0062,  0.0011],\n",
            "        [ 0.0002, -0.0040, -0.0050,  ..., -0.0012, -0.0081,  0.0061],\n",
            "        [-0.0130,  0.0008,  0.0081,  ..., -0.0232,  0.0077, -0.0060]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.11.block_sparse_moe.gate.weight': tensor([[-0.0019, -0.0040,  0.0053,  ...,  0.0114,  0.0073,  0.0052],\n",
            "        [-0.0054, -0.0035,  0.0048,  ...,  0.0020,  0.0014,  0.0049],\n",
            "        [ 0.0023, -0.0052,  0.0045,  ...,  0.0073,  0.0003,  0.0037],\n",
            "        ...,\n",
            "        [ 0.0040,  0.0104, -0.0063,  ..., -0.0082, -0.0068,  0.0008],\n",
            "        [-0.0041, -0.0049, -0.0016,  ...,  0.0031, -0.0020, -0.0045],\n",
            "        [ 0.0055, -0.0056, -0.0019,  ..., -0.0029, -0.0087,  0.0090]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.11.input_layernorm.weight': tensor([1.5000, 2.1094, 2.1406,  ..., 1.2109, 1.5938, 1.6797], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.11.post_attention_layernorm.weight': tensor([1.5000, 2.1719, 1.9609,  ..., 1.3125, 1.5000, 1.6094], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.11.self_attn.k_proj.weight': tensor([[ 1.9684e-03, -1.2146e-02, -3.0518e-03,  ..., -9.8877e-03,\n",
            "          7.9346e-03,  3.3569e-03],\n",
            "        [-5.0659e-03,  3.2654e-03,  4.8828e-03,  ..., -1.7395e-03,\n",
            "          1.9932e-04, -1.1169e-02],\n",
            "        [-3.6926e-03,  3.5095e-03, -5.4169e-04,  ...,  3.3875e-03,\n",
            "          4.2419e-03, -9.2316e-04],\n",
            "        ...,\n",
            "        [ 1.1292e-02, -1.0986e-02, -9.5215e-03,  ...,  3.8452e-03,\n",
            "          1.9653e-02,  4.5776e-03],\n",
            "        [-7.6294e-05,  1.6357e-02, -1.5198e-02,  ..., -5.5847e-03,\n",
            "         -1.0757e-03,  1.1749e-03],\n",
            "        [-2.5787e-03,  1.3672e-02,  2.6123e-02,  ..., -3.3569e-03,\n",
            "          2.2461e-02, -7.4463e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.11.self_attn.o_proj.weight': tensor([[-0.0065,  0.0132,  0.0159,  ..., -0.0008, -0.0007, -0.0092],\n",
            "        [ 0.0019,  0.0081,  0.0162,  ..., -0.0029,  0.0023,  0.0146],\n",
            "        [-0.0099, -0.0153, -0.0091,  ...,  0.0052,  0.0024,  0.0056],\n",
            "        ...,\n",
            "        [ 0.0071,  0.0124, -0.0219,  ...,  0.0084, -0.0145,  0.0117],\n",
            "        [-0.0143,  0.0073, -0.0075,  ...,  0.0078,  0.0054,  0.0162],\n",
            "        [-0.0009, -0.0189,  0.0094,  ..., -0.0071, -0.0095,  0.0103]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.11.self_attn.q_proj.weight': tensor([[ 0.0030,  0.0073,  0.0107,  ...,  0.0024, -0.0050, -0.0084],\n",
            "        [ 0.0076, -0.0117, -0.0087,  ..., -0.0155,  0.0031,  0.0076],\n",
            "        [-0.0001, -0.0063,  0.0048,  ..., -0.0135, -0.0055,  0.0084],\n",
            "        ...,\n",
            "        [ 0.0009,  0.0173,  0.0025,  ..., -0.0036, -0.0090, -0.0168],\n",
            "        [-0.0056, -0.0096, -0.0051,  ...,  0.0041,  0.0083, -0.0085],\n",
            "        [-0.0081, -0.0133, -0.0038,  ...,  0.0002, -0.0028,  0.0025]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.11.self_attn.v_proj.weight': tensor([[-0.0031, -0.0175, -0.0021,  ..., -0.0004, -0.0164, -0.0004],\n",
            "        [-0.0099, -0.0190,  0.0479,  ..., -0.0032,  0.0030, -0.0124],\n",
            "        [ 0.0105, -0.0175,  0.0190,  ..., -0.0044, -0.0064,  0.0004],\n",
            "        ...,\n",
            "        [ 0.0067,  0.0105, -0.0026,  ..., -0.0135, -0.0139,  0.0078],\n",
            "        [ 0.0113,  0.0076, -0.0007,  ...,  0.0093, -0.0046,  0.0067],\n",
            "        [ 0.0120, -0.0128,  0.0035,  ..., -0.0123, -0.0065, -0.0040]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.12.block_sparse_moe.gate.weight': tensor([[ 0.0026,  0.0107,  0.0016,  ..., -0.0066,  0.0036, -0.0089],\n",
            "        [ 0.0012, -0.0016, -0.0092,  ..., -0.0116,  0.0084, -0.0103],\n",
            "        [-0.0054, -0.0022,  0.0032,  ..., -0.0005, -0.0092, -0.0140],\n",
            "        ...,\n",
            "        [-0.0049,  0.0007, -0.0042,  ..., -0.0010, -0.0043,  0.0090],\n",
            "        [-0.0028, -0.0178,  0.0005,  ...,  0.0074, -0.0005,  0.0072],\n",
            "        [-0.0068,  0.0041, -0.0004,  ..., -0.0026,  0.0161,  0.0121]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.12.input_layernorm.weight': tensor([1.6250, 2.2969, 2.3750,  ..., 1.2734, 1.6719, 1.8906], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.12.post_attention_layernorm.weight': tensor([1.6484, 2.5312, 2.2500,  ..., 1.4062, 1.6562, 1.7656], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.12.self_attn.k_proj.weight': tensor([[-1.8311e-04, -1.7624e-03,  9.8705e-05,  ..., -7.7820e-03,\n",
            "         -1.5869e-03,  3.3722e-03],\n",
            "        [-1.6556e-03,  4.4556e-03,  1.1520e-03,  ..., -3.5248e-03,\n",
            "         -1.0925e-02, -3.1586e-03],\n",
            "        [-2.3956e-03, -2.3651e-03,  4.5471e-03,  ..., -5.4321e-03,\n",
            "         -2.6093e-03, -3.0327e-04],\n",
            "        ...,\n",
            "        [-1.2741e-03, -5.9204e-03,  4.4250e-03,  ...,  3.8147e-03,\n",
            "          2.4567e-03, -6.5308e-03],\n",
            "        [-1.8539e-03,  3.8147e-03,  1.7471e-03,  ...,  8.6594e-04,\n",
            "          2.3956e-03,  3.1128e-03],\n",
            "        [-1.6357e-02, -1.9226e-03, -8.0566e-03,  ...,  6.2943e-04,\n",
            "         -5.8899e-03, -6.1035e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.12.self_attn.o_proj.weight': tensor([[-1.1414e-02,  5.9509e-03,  1.0147e-03,  ...,  1.0132e-02,\n",
            "          9.9487e-03,  6.6223e-03],\n",
            "        [ 1.3611e-02, -2.5635e-03, -1.4221e-02,  ..., -6.8665e-03,\n",
            "         -4.1504e-03, -1.9455e-03],\n",
            "        [ 9.3994e-03, -1.0147e-03, -4.8828e-03,  ..., -3.0396e-02,\n",
            "         -7.5684e-03, -2.1484e-02],\n",
            "        ...,\n",
            "        [-4.5395e-04,  1.4572e-03, -2.9449e-03,  ...,  4.5166e-03,\n",
            "          8.4229e-03,  1.8005e-03],\n",
            "        [ 6.2943e-04, -4.1199e-03, -1.0315e-02,  ...,  9.6798e-05,\n",
            "          9.8267e-03,  1.1292e-02],\n",
            "        [ 7.1411e-03,  1.2268e-02, -4.7913e-03,  ..., -2.3804e-03,\n",
            "         -2.8320e-02,  5.3711e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.12.self_attn.q_proj.weight': tensor([[-1.3306e-02, -3.5706e-03, -4.9744e-03,  ...,  1.0132e-02,\n",
            "          6.1340e-03, -1.8954e-05],\n",
            "        [ 1.4221e-02, -1.4526e-02, -6.8054e-03,  ..., -8.8501e-03,\n",
            "         -1.0559e-02, -1.4877e-03],\n",
            "        [-2.5630e-05, -1.6479e-02, -2.8229e-03,  ..., -2.6093e-03,\n",
            "          3.8300e-03,  3.8452e-03],\n",
            "        ...,\n",
            "        [-8.4229e-03,  4.3640e-03,  9.8877e-03,  ..., -2.5940e-03,\n",
            "         -9.7046e-03,  5.7373e-03],\n",
            "        [-6.4392e-03, -2.6245e-03, -2.8839e-03,  ...,  1.4465e-02,\n",
            "          1.8677e-02,  6.6528e-03],\n",
            "        [-4.8828e-03,  1.3428e-02,  3.9673e-03,  ..., -1.1841e-02,\n",
            "         -1.0376e-02,  1.0620e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.12.self_attn.v_proj.weight': tensor([[ 0.0038,  0.0036, -0.0033,  ...,  0.0002,  0.0065,  0.0139],\n",
            "        [-0.0026,  0.0144, -0.0004,  ..., -0.0102, -0.0018,  0.0018],\n",
            "        [ 0.0023,  0.0072, -0.0038,  ..., -0.0112, -0.0120,  0.0055],\n",
            "        ...,\n",
            "        [ 0.0052, -0.0227, -0.0168,  ...,  0.0101, -0.0002, -0.0070],\n",
            "        [ 0.0104, -0.0131, -0.0044,  ...,  0.0050,  0.0012, -0.0098],\n",
            "        [ 0.0127, -0.0036, -0.0410,  ...,  0.0098, -0.0059,  0.0023]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.13.block_sparse_moe.gate.weight': tensor([[ 0.0022, -0.0012,  0.0024,  ...,  0.0024, -0.0021, -0.0026],\n",
            "        [ 0.0031, -0.0008,  0.0053,  ...,  0.0028, -0.0021, -0.0010],\n",
            "        [ 0.0025,  0.0106, -0.0064,  ..., -0.0056,  0.0083, -0.0092],\n",
            "        ...,\n",
            "        [ 0.0031, -0.0029,  0.0011,  ...,  0.0010, -0.0076, -0.0053],\n",
            "        [ 0.0004, -0.0004, -0.0012,  ..., -0.0046, -0.0027,  0.0137],\n",
            "        [ 0.0002,  0.0027, -0.0061,  ...,  0.0012,  0.0066,  0.0061]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.13.input_layernorm.weight': tensor([1.7266, 2.7969, 2.6875,  ..., 1.2969, 1.6953, 1.9453], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.13.post_attention_layernorm.weight': tensor([1.7578, 2.9688, 2.3594,  ..., 1.5078, 1.7422, 1.8594], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.13.self_attn.k_proj.weight': tensor([[ 0.0019, -0.0089, -0.0018,  ..., -0.0092, -0.0173,  0.0013],\n",
            "        [ 0.0025, -0.0017,  0.0043,  ..., -0.0061,  0.0041, -0.0005],\n",
            "        [-0.0008,  0.0064, -0.0026,  ..., -0.0004,  0.0083, -0.0022],\n",
            "        ...,\n",
            "        [ 0.0178, -0.0115, -0.0046,  ..., -0.0042, -0.0029, -0.0121],\n",
            "        [-0.0144, -0.0106, -0.0090,  ..., -0.0019,  0.0053,  0.0106],\n",
            "        [ 0.0033, -0.0071, -0.0044,  ..., -0.0066, -0.0016,  0.0080]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.13.self_attn.o_proj.weight': tensor([[ 0.0039, -0.0020,  0.0189,  ..., -0.0156, -0.0016,  0.0029],\n",
            "        [ 0.0120, -0.0063, -0.0036,  ..., -0.0009, -0.0068,  0.0004],\n",
            "        [-0.0031, -0.0020,  0.0080,  ..., -0.0124,  0.0039,  0.0049],\n",
            "        ...,\n",
            "        [ 0.0156,  0.0017,  0.0033,  ...,  0.0150, -0.0014, -0.0040],\n",
            "        [-0.0064, -0.0082, -0.0096,  ...,  0.0161, -0.0094,  0.0134],\n",
            "        [-0.0028, -0.0004, -0.0260,  ...,  0.0031, -0.0199,  0.0030]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.13.self_attn.q_proj.weight': tensor([[-7.5378e-03,  1.5503e-02,  8.9722e-03,  ...,  2.1973e-03,\n",
            "          3.6011e-03,  2.9907e-03],\n",
            "        [-9.0790e-04, -2.4414e-02,  3.6011e-03,  ...,  1.3245e-02,\n",
            "          7.5073e-03,  3.3875e-03],\n",
            "        [ 5.8594e-03,  1.5991e-02,  1.8692e-03,  ..., -4.4250e-03,\n",
            "         -1.7395e-03,  7.8583e-04],\n",
            "        ...,\n",
            "        [-3.5667e-04,  1.2146e-02,  1.0315e-02,  ...,  1.9836e-04,\n",
            "         -3.6011e-03, -8.4686e-04],\n",
            "        [ 1.0498e-02, -4.6968e-05,  7.3242e-03,  ...,  2.9602e-03,\n",
            "          1.1520e-03, -1.1414e-02],\n",
            "        [-8.4229e-03,  4.1504e-03,  9.6893e-04,  ...,  6.2561e-03,\n",
            "          8.2397e-03, -9.2163e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.13.self_attn.v_proj.weight': tensor([[-1.4725e-03,  1.8215e-04,  2.8229e-03,  ...,  2.0905e-03,\n",
            "         -1.5564e-02, -1.5488e-03],\n",
            "        [ 5.6152e-03, -1.1902e-02, -1.7334e-02,  ...,  9.6130e-04,\n",
            "         -1.1475e-02,  5.1270e-03],\n",
            "        [ 3.1738e-03,  4.9438e-03,  2.4536e-02,  ...,  1.3733e-02,\n",
            "          6.1798e-04, -2.5146e-02],\n",
            "        ...,\n",
            "        [-1.7471e-03, -8.7891e-03, -1.2741e-03,  ...,  1.3672e-02,\n",
            "          1.0803e-02, -5.1022e-05],\n",
            "        [-1.3351e-03,  1.1230e-02, -1.1673e-03,  ..., -1.3184e-02,\n",
            "         -5.4932e-03, -5.2795e-03],\n",
            "        [ 1.2939e-02,  9.3994e-03, -1.0498e-02,  ..., -1.8921e-02,\n",
            "         -1.4496e-04, -2.3746e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.14.block_sparse_moe.gate.weight': tensor([[ 2.4796e-04,  2.9449e-03,  1.8845e-03,  ...,  9.0332e-03,\n",
            "          4.0283e-03, -7.0496e-03],\n",
            "        [ 2.8076e-03, -2.4261e-03,  8.7891e-03,  ..., -6.8054e-03,\n",
            "         -2.8839e-03,  3.9673e-03],\n",
            "        [-1.0376e-02,  7.5378e-03,  2.0981e-04,  ...,  8.3008e-03,\n",
            "          5.5313e-04, -5.6028e-05],\n",
            "        ...,\n",
            "        [ 2.7618e-03, -1.0315e-02, -3.1128e-03,  ..., -1.2878e-02,\n",
            "          5.7983e-03,  3.6316e-03],\n",
            "        [-1.2894e-03,  8.8501e-03,  4.5013e-04,  ...,  5.9204e-03,\n",
            "         -8.7891e-03, -1.8024e-04],\n",
            "        [ 6.7749e-03, -3.4485e-03,  4.1504e-03,  ...,  3.0670e-03,\n",
            "          1.0061e-04,  4.6692e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.14.input_layernorm.weight': tensor([1.8828, 3.1406, 2.8594,  ..., 1.3906, 1.8125, 2.1094], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.14.post_attention_layernorm.weight': tensor([1.7969, 3.5781, 2.4531,  ..., 1.5859, 1.7891, 1.9219], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.14.self_attn.k_proj.weight': tensor([[ 2.1515e-03,  1.4343e-03,  1.7262e-04,  ..., -8.9111e-03,\n",
            "          3.3379e-04, -1.5793e-03],\n",
            "        [-6.4392e-03,  4.5300e-05, -4.6082e-03,  ..., -8.0566e-03,\n",
            "          4.2114e-03, -1.0490e-04],\n",
            "        [-1.5640e-03,  1.1206e-05, -3.4790e-03,  ...,  1.2589e-03,\n",
            "         -3.0518e-03,  2.4567e-03],\n",
            "        ...,\n",
            "        [ 1.5442e-02,  1.4801e-03,  2.2888e-03,  ...,  2.6398e-03,\n",
            "         -1.4099e-02,  1.0315e-02],\n",
            "        [-5.1880e-03, -7.3242e-03,  6.8359e-03,  ..., -5.3406e-03,\n",
            "         -2.7100e-02,  1.5015e-02],\n",
            "        [-1.0742e-02, -8.0872e-04,  6.7444e-03,  ...,  5.4550e-04,\n",
            "          1.2451e-02,  3.9368e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.14.self_attn.o_proj.weight': tensor([[-0.0140, -0.0106, -0.0002,  ...,  0.0078,  0.0072,  0.0051],\n",
            "        [-0.0114, -0.0134, -0.0010,  ..., -0.0170, -0.0103, -0.0045],\n",
            "        [-0.0054, -0.0040,  0.0087,  ..., -0.0013, -0.0015, -0.0126],\n",
            "        ...,\n",
            "        [-0.0157,  0.0127,  0.0023,  ...,  0.0186,  0.0125,  0.0101],\n",
            "        [ 0.0117,  0.0072,  0.0125,  ...,  0.0031,  0.0094,  0.0004],\n",
            "        [ 0.0073, -0.0029, -0.0097,  ..., -0.0093, -0.0126, -0.0022]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.14.self_attn.q_proj.weight': tensor([[-0.0089,  0.0110, -0.0042,  ...,  0.0001, -0.0012, -0.0039],\n",
            "        [ 0.0013,  0.0126, -0.0050,  ...,  0.0076,  0.0024, -0.0008],\n",
            "        [ 0.0114, -0.0183,  0.0106,  ..., -0.0015, -0.0073,  0.0036],\n",
            "        ...,\n",
            "        [ 0.0038,  0.0026,  0.0233,  ...,  0.0405,  0.0164, -0.0094],\n",
            "        [ 0.0187,  0.0042, -0.0270,  ...,  0.0104, -0.0269, -0.0099],\n",
            "        [-0.0221, -0.0227, -0.0033,  ..., -0.0150,  0.0044,  0.0017]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.14.self_attn.v_proj.weight': tensor([[ 0.0030, -0.0109,  0.0118,  ..., -0.0170, -0.0060,  0.0084],\n",
            "        [-0.0103,  0.0016,  0.0092,  ...,  0.0105,  0.0092,  0.0027],\n",
            "        [-0.0170,  0.0017,  0.0010,  ...,  0.0217, -0.0041,  0.0094],\n",
            "        ...,\n",
            "        [-0.0058,  0.0017, -0.0080,  ..., -0.0129, -0.0089, -0.0037],\n",
            "        [ 0.0183, -0.0079,  0.0100,  ...,  0.0084, -0.0009, -0.0080],\n",
            "        [-0.0034, -0.0040, -0.0027,  ...,  0.0234,  0.0027, -0.0142]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.15.block_sparse_moe.gate.weight': tensor([[-0.0080,  0.0048,  0.0012,  ...,  0.0081, -0.0021, -0.0002],\n",
            "        [ 0.0079, -0.0049,  0.0041,  ..., -0.0061,  0.0039,  0.0091],\n",
            "        [ 0.0059,  0.0007,  0.0052,  ..., -0.0092,  0.0081, -0.0014],\n",
            "        ...,\n",
            "        [-0.0119,  0.0025,  0.0092,  ...,  0.0033, -0.0036, -0.0061],\n",
            "        [ 0.0048, -0.0022, -0.0026,  ..., -0.0038, -0.0041, -0.0002],\n",
            "        [ 0.0079,  0.0051, -0.0019,  ...,  0.0057,  0.0010, -0.0017]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.15.input_layernorm.weight': tensor([2.1250, 3.4688, 3.0781,  ..., 1.7812, 2.1406, 2.3438], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.15.post_attention_layernorm.weight': tensor([1.9844, 4.5625, 2.5469,  ..., 1.7891, 1.9844, 2.0625], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.15.self_attn.k_proj.weight': tensor([[ 2.7313e-03, -2.6245e-03, -2.1839e-04,  ..., -3.8605e-03,\n",
            "         -2.3041e-03, -9.1934e-04],\n",
            "        [-3.1281e-03,  4.6997e-03,  2.1973e-03,  ..., -6.5613e-03,\n",
            "         -3.7689e-03,  3.7537e-03],\n",
            "        [-5.0964e-03, -4.8218e-03, -3.8605e-03,  ...,  2.2888e-03,\n",
            "          4.9973e-04,  1.1215e-03],\n",
            "        ...,\n",
            "        [-1.6357e-02, -8.2397e-03,  1.8921e-02,  ...,  1.1475e-02,\n",
            "          1.4099e-02, -8.3008e-03],\n",
            "        [ 1.0193e-02,  2.0905e-03, -2.8839e-03,  ..., -1.0132e-02,\n",
            "          3.5286e-04,  6.9885e-03],\n",
            "        [-5.5847e-03, -5.5432e-06,  1.3306e-02,  ...,  8.6975e-04,\n",
            "          3.8574e-02,  1.2329e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.15.self_attn.o_proj.weight': tensor([[ 0.0120, -0.0056,  0.0021,  ..., -0.0086,  0.0162,  0.0060],\n",
            "        [ 0.0009,  0.0014, -0.0039,  ...,  0.0079,  0.0118,  0.0045],\n",
            "        [ 0.0005,  0.0134,  0.0068,  ..., -0.0056,  0.0172,  0.0071],\n",
            "        ...,\n",
            "        [-0.0141,  0.0002,  0.0040,  ..., -0.0219, -0.0038, -0.0128],\n",
            "        [-0.0049,  0.0084,  0.0021,  ...,  0.0022, -0.0069,  0.0103],\n",
            "        [ 0.0081,  0.0112, -0.0007,  ..., -0.0079, -0.0038,  0.0102]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.15.self_attn.q_proj.weight': tensor([[ 0.0042, -0.0058,  0.0050,  ...,  0.0111, -0.0079, -0.0016],\n",
            "        [ 0.0029, -0.0141, -0.0055,  ...,  0.0056,  0.0078,  0.0031],\n",
            "        [-0.0023,  0.0165,  0.0026,  ..., -0.0049, -0.0031,  0.0004],\n",
            "        ...,\n",
            "        [-0.0171,  0.0160,  0.0198,  ..., -0.0066,  0.0142,  0.0077],\n",
            "        [ 0.0047,  0.0107,  0.0211,  ...,  0.0178, -0.0188,  0.0077],\n",
            "        [ 0.0084,  0.0092,  0.0095,  ..., -0.0164,  0.0142, -0.0162]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.15.self_attn.v_proj.weight': tensor([[ 0.0198,  0.0004, -0.0043,  ..., -0.0131, -0.0018,  0.0153],\n",
            "        [-0.0041, -0.0009,  0.0149,  ..., -0.0031, -0.0049,  0.0092],\n",
            "        [ 0.0080, -0.0325,  0.0096,  ...,  0.0102,  0.0155, -0.0050],\n",
            "        ...,\n",
            "        [-0.0044,  0.0004, -0.0058,  ..., -0.0166, -0.0078, -0.0082],\n",
            "        [ 0.0084,  0.0006,  0.0047,  ..., -0.0117,  0.0096,  0.0109],\n",
            "        [ 0.0124,  0.0043, -0.0054,  ...,  0.0010, -0.0013, -0.0077]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.16.block_sparse_moe.gate.weight': tensor([[-4.3030e-03,  1.6632e-03, -5.3024e-04,  ...,  2.5330e-03,\n",
            "          1.0132e-02,  8.3618e-03],\n",
            "        [ 2.1973e-03,  2.7161e-03,  2.3041e-03,  ...,  5.6458e-03,\n",
            "          1.6022e-03,  7.3242e-03],\n",
            "        [ 4.9438e-03,  2.4109e-03,  3.2501e-03,  ...,  7.1716e-03,\n",
            "         -9.7656e-03,  1.2016e-04],\n",
            "        ...,\n",
            "        [ 1.3657e-03,  2.3651e-03, -8.5449e-04,  ...,  7.8201e-05,\n",
            "          4.9438e-03, -8.7280e-03],\n",
            "        [-1.1673e-03, -1.0395e-04, -4.2419e-03,  ..., -7.7820e-03,\n",
            "         -3.2654e-03, -1.1368e-03],\n",
            "        [-3.7689e-03,  3.3722e-03,  2.7618e-03,  ..., -8.3008e-03,\n",
            "         -7.7209e-03, -2.5177e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.16.input_layernorm.weight': tensor([2.1719, 3.9375, 2.9375,  ..., 1.8203, 2.1562, 2.3125], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.16.post_attention_layernorm.weight': tensor([2.1875, 5.2188, 2.7188,  ..., 1.9688, 2.1719, 2.2656], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.16.self_attn.k_proj.weight': tensor([[ 1.6174e-03,  5.8899e-03,  5.0964e-03,  ...,  6.8054e-03,\n",
            "          2.0447e-03, -7.1411e-03],\n",
            "        [-4.6692e-03, -1.2756e-02, -1.5335e-03,  ..., -3.3722e-03,\n",
            "         -5.2185e-03,  6.5002e-03],\n",
            "        [ 3.2902e-05, -8.0566e-03,  1.6556e-03,  ...,  4.5471e-03,\n",
            "          5.8899e-03, -6.3782e-03],\n",
            "        ...,\n",
            "        [ 3.4790e-03,  3.0518e-05, -5.4626e-03,  ..., -5.3711e-03,\n",
            "         -4.5166e-03, -1.2894e-03],\n",
            "        [-9.7046e-03, -2.9449e-03, -2.7313e-03,  ...,  5.5237e-03,\n",
            "         -2.6855e-03,  5.1117e-04],\n",
            "        [ 5.0659e-03,  5.3406e-03, -4.3945e-03,  ..., -2.1973e-02,\n",
            "          1.3580e-03, -1.7090e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.16.self_attn.o_proj.weight': tensor([[ 0.0072, -0.0074,  0.0170,  ...,  0.0145, -0.0101, -0.0005],\n",
            "        [-0.0024,  0.0068,  0.0125,  ..., -0.0087, -0.0055,  0.0021],\n",
            "        [-0.0277, -0.0027,  0.0073,  ...,  0.0036, -0.0035, -0.0057],\n",
            "        ...,\n",
            "        [-0.0184, -0.0060,  0.0019,  ..., -0.0058,  0.0053,  0.0099],\n",
            "        [ 0.0046, -0.0038, -0.0131,  ..., -0.0038,  0.0160,  0.0134],\n",
            "        [-0.0153,  0.0067, -0.0081,  ..., -0.0078, -0.0023,  0.0114]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.16.self_attn.q_proj.weight': tensor([[-0.0028,  0.0042,  0.0025,  ...,  0.0006, -0.0032,  0.0015],\n",
            "        [ 0.0076, -0.0047, -0.0017,  ..., -0.0037,  0.0074,  0.0011],\n",
            "        [-0.0067,  0.0095, -0.0017,  ...,  0.0036, -0.0053,  0.0007],\n",
            "        ...,\n",
            "        [ 0.0041, -0.0003,  0.0076,  ..., -0.0181,  0.0043, -0.0017],\n",
            "        [-0.0030,  0.0030, -0.0010,  ..., -0.0046,  0.0099,  0.0070],\n",
            "        [ 0.0056,  0.0044,  0.0010,  ..., -0.0020, -0.0094, -0.0127]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.16.self_attn.v_proj.weight': tensor([[ 2.2650e-06,  3.7079e-03, -1.7090e-02,  ..., -1.4343e-02,\n",
            "          1.0437e-02, -4.7607e-03],\n",
            "        [-7.2327e-03,  4.3335e-03,  3.4790e-03,  ..., -8.2397e-03,\n",
            "         -1.4160e-02,  1.2695e-02],\n",
            "        [ 9.9487e-03, -7.8125e-03, -1.1108e-02,  ..., -9.6436e-03,\n",
            "         -1.7944e-02, -1.0315e-02],\n",
            "        ...,\n",
            "        [ 5.1575e-03,  2.7161e-03, -1.0620e-02,  ..., -1.3123e-03,\n",
            "          7.0801e-03, -1.7944e-02],\n",
            "        [-4.2419e-03, -2.8839e-03, -3.1891e-03,  ..., -7.9346e-03,\n",
            "          6.7139e-03,  9.4604e-03],\n",
            "        [-1.1353e-02,  3.9673e-03,  8.2397e-03,  ..., -5.7678e-03,\n",
            "          2.4902e-02,  1.2146e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.17.block_sparse_moe.gate.weight': tensor([[-1.1841e-02,  6.7749e-03,  3.9368e-03,  ..., -8.5831e-04,\n",
            "          1.3550e-02, -2.9907e-03],\n",
            "        [-7.3853e-03, -4.2114e-03, -1.0376e-03,  ..., -2.9144e-03,\n",
            "         -5.0964e-03,  1.2894e-03],\n",
            "        [-4.9133e-03, -7.4768e-04, -8.0490e-04,  ...,  2.8839e-03,\n",
            "          4.8523e-03,  3.0212e-03],\n",
            "        ...,\n",
            "        [ 2.7618e-03,  2.8229e-03, -4.2677e-05,  ...,  5.1117e-04,\n",
            "         -2.5940e-03, -2.2583e-03],\n",
            "        [ 3.7994e-03,  3.5553e-03, -5.0049e-03,  ..., -1.2329e-02,\n",
            "         -1.5411e-03, -5.4626e-03],\n",
            "        [ 5.9509e-03, -7.2861e-04,  2.9907e-03,  ..., -4.4823e-04,\n",
            "         -6.5918e-03,  6.1340e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.17.input_layernorm.weight': tensor([1.9062, 3.9375, 2.3594,  ..., 1.6328, 1.8203, 1.9922], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.17.post_attention_layernorm.weight': tensor([2.3906, 5.3438, 2.8438,  ..., 2.1562, 2.3750, 2.4219], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.17.self_attn.k_proj.weight': tensor([[ 7.9632e-05,  5.1575e-03, -7.0801e-03,  ..., -1.5625e-02,\n",
            "          2.9297e-03,  2.7924e-03],\n",
            "        [-8.8882e-04,  4.3640e-03, -9.0332e-03,  ..., -8.7280e-03,\n",
            "          9.3384e-03, -3.7994e-03],\n",
            "        [ 7.0801e-03, -8.6060e-03,  1.6251e-03,  ..., -1.9531e-03,\n",
            "         -1.8158e-03,  6.6757e-04],\n",
            "        ...,\n",
            "        [ 1.3184e-02,  1.8005e-03, -1.0925e-02,  ...,  2.7008e-03,\n",
            "          3.4637e-03,  1.5869e-02],\n",
            "        [-9.2773e-03,  1.2451e-02,  2.4048e-02,  ...,  1.7090e-02,\n",
            "         -1.1780e-02, -3.2806e-03],\n",
            "        [ 1.1169e-02,  6.1951e-03,  2.2583e-03,  ...,  1.0742e-02,\n",
            "         -8.3008e-03, -1.8311e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.17.self_attn.o_proj.weight': tensor([[-0.0026, -0.0080, -0.0029,  ...,  0.0240,  0.0072,  0.0045],\n",
            "        [ 0.0036, -0.0049,  0.0006,  ..., -0.0106,  0.0002, -0.0070],\n",
            "        [-0.0002,  0.0009,  0.0035,  ..., -0.0011,  0.0194,  0.0143],\n",
            "        ...,\n",
            "        [-0.0085, -0.0179, -0.0071,  ...,  0.0189,  0.0032, -0.0152],\n",
            "        [ 0.0089, -0.0021, -0.0210,  ...,  0.0030, -0.0078, -0.0109],\n",
            "        [ 0.0129,  0.0159, -0.0093,  ..., -0.0103,  0.0114,  0.0111]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.17.self_attn.q_proj.weight': tensor([[ 0.0139,  0.0032, -0.0019,  ...,  0.0024, -0.0165, -0.0032],\n",
            "        [ 0.0065, -0.0093,  0.0124,  ...,  0.0140,  0.0070,  0.0089],\n",
            "        [ 0.0012,  0.0124, -0.0076,  ..., -0.0017, -0.0098, -0.0048],\n",
            "        ...,\n",
            "        [ 0.0005, -0.0209,  0.0071,  ..., -0.0115, -0.0004, -0.0032],\n",
            "        [ 0.0026,  0.0106,  0.0041,  ...,  0.0137, -0.0110, -0.0139],\n",
            "        [ 0.0167,  0.0093,  0.0072,  ..., -0.0111,  0.0227, -0.0109]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.17.self_attn.v_proj.weight': tensor([[-0.0028,  0.0117,  0.0022,  ..., -0.0189,  0.0080,  0.0298],\n",
            "        [-0.0102, -0.0094,  0.0023,  ..., -0.0135,  0.0249,  0.0272],\n",
            "        [-0.0063, -0.0044,  0.0067,  ..., -0.0366, -0.0031, -0.0016],\n",
            "        ...,\n",
            "        [ 0.0078,  0.0195,  0.0123,  ..., -0.0120,  0.0145, -0.0022],\n",
            "        [-0.0071,  0.0024,  0.0173,  ..., -0.0142,  0.0067,  0.0072],\n",
            "        [-0.0095, -0.0011,  0.0095,  ...,  0.0005,  0.0159,  0.0014]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.18.block_sparse_moe.gate.weight': tensor([[-4.6692e-03,  2.1057e-03,  3.3760e-04,  ..., -1.8616e-03,\n",
            "         -8.1635e-04,  4.5471e-03],\n",
            "        [ 7.8125e-03,  8.0109e-04, -1.3123e-03,  ...,  2.0447e-03,\n",
            "          3.7994e-03, -1.2512e-02],\n",
            "        [ 2.5482e-03,  2.0409e-04, -2.0294e-03,  ..., -2.1362e-03,\n",
            "          3.9673e-03,  5.0354e-03],\n",
            "        ...,\n",
            "        [ 6.1340e-03,  7.6294e-04,  2.5787e-03,  ...,  1.1169e-02,\n",
            "         -3.7670e-05, -4.0894e-03],\n",
            "        [-5.1270e-03,  3.9577e-05,  5.5847e-03,  ..., -2.8076e-03,\n",
            "          2.9907e-03,  4.0894e-03],\n",
            "        [ 9.7656e-03,  5.8174e-05,  2.4567e-03,  ...,  5.9204e-03,\n",
            "         -1.0529e-03, -1.0834e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.18.input_layernorm.weight': tensor([2.1562, 3.7344, 2.5312,  ..., 1.9766, 2.1406, 2.2031], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.18.post_attention_layernorm.weight': tensor([2.5469, 6.3438, 2.9531,  ..., 2.2969, 2.5156, 2.5156], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.18.self_attn.k_proj.weight': tensor([[-0.0003,  0.0029, -0.0025,  ...,  0.0022, -0.0053,  0.0068],\n",
            "        [ 0.0040, -0.0002, -0.0044,  ..., -0.0015,  0.0003,  0.0003],\n",
            "        [-0.0038,  0.0064,  0.0030,  ..., -0.0017, -0.0046, -0.0010],\n",
            "        ...,\n",
            "        [-0.0071, -0.0025, -0.0030,  ..., -0.0106,  0.0072, -0.0178],\n",
            "        [ 0.0008, -0.0032,  0.0037,  ...,  0.0046, -0.0091, -0.0055],\n",
            "        [ 0.0142, -0.0052,  0.0015,  ...,  0.0161, -0.0232, -0.0009]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.18.self_attn.o_proj.weight': tensor([[-0.0309,  0.0114,  0.0103,  ..., -0.0130,  0.0087, -0.0065],\n",
            "        [ 0.0028,  0.0010, -0.0095,  ...,  0.0023, -0.0012,  0.0171],\n",
            "        [ 0.0033, -0.0017,  0.0032,  ...,  0.0090,  0.0052, -0.0103],\n",
            "        ...,\n",
            "        [ 0.0216, -0.0032, -0.0018,  ..., -0.0036, -0.0046,  0.0004],\n",
            "        [ 0.0013, -0.0019, -0.0055,  ...,  0.0017,  0.0054, -0.0037],\n",
            "        [-0.0032,  0.0030, -0.0247,  ..., -0.0145,  0.0069, -0.0085]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.18.self_attn.q_proj.weight': tensor([[-0.0032, -0.0083, -0.0052,  ...,  0.0085, -0.0010,  0.0002],\n",
            "        [-0.0029,  0.0012, -0.0032,  ..., -0.0027,  0.0041, -0.0051],\n",
            "        [-0.0027, -0.0011, -0.0041,  ..., -0.0016,  0.0036, -0.0008],\n",
            "        ...,\n",
            "        [-0.0103,  0.0258, -0.0193,  ...,  0.0057, -0.0085,  0.0079],\n",
            "        [ 0.0088,  0.0033,  0.0026,  ..., -0.0114, -0.0139, -0.0058],\n",
            "        [ 0.0047,  0.0135,  0.0141,  ...,  0.0076,  0.0117,  0.0102]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.18.self_attn.v_proj.weight': tensor([[-0.0028, -0.0015,  0.0075,  ...,  0.0139,  0.0006,  0.0008],\n",
            "        [-0.0013, -0.0069,  0.0008,  ...,  0.0033,  0.0005,  0.0032],\n",
            "        [-0.0058, -0.0003, -0.0016,  ...,  0.0044,  0.0166,  0.0165],\n",
            "        ...,\n",
            "        [-0.0009,  0.0133,  0.0128,  ..., -0.0011,  0.0037, -0.0153],\n",
            "        [ 0.0056,  0.0007,  0.0108,  ..., -0.0031, -0.0088,  0.0035],\n",
            "        [-0.0140, -0.0036,  0.0014,  ...,  0.0154,  0.0038, -0.0091]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.19.block_sparse_moe.gate.weight': tensor([[-0.0022,  0.0045,  0.0075,  ...,  0.0050, -0.0141,  0.0054],\n",
            "        [-0.0070,  0.0079, -0.0002,  ..., -0.0025,  0.0087, -0.0026],\n",
            "        [-0.0009,  0.0008,  0.0012,  ...,  0.0028, -0.0047,  0.0042],\n",
            "        ...,\n",
            "        [ 0.0047,  0.0028, -0.0009,  ...,  0.0088,  0.0054, -0.0069],\n",
            "        [ 0.0033, -0.0039, -0.0034,  ..., -0.0003, -0.0007, -0.0013],\n",
            "        [ 0.0034,  0.0035, -0.0026,  ..., -0.0063, -0.0065, -0.0015]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.19.input_layernorm.weight': tensor([2.3125, 3.4844, 2.7031,  ..., 2.1406, 2.1875, 2.3125], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.19.post_attention_layernorm.weight': tensor([2.7031, 4.3750, 3.0781,  ..., 2.5000, 2.7344, 2.6719], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.19.self_attn.k_proj.weight': tensor([[-1.2817e-02,  6.6833e-03,  6.2256e-03,  ...,  7.5378e-03,\n",
            "          7.5989e-03,  3.0975e-03],\n",
            "        [-2.3651e-03,  4.7302e-03, -5.3711e-03,  ..., -5.0659e-03,\n",
            "          6.9046e-04, -1.3733e-03],\n",
            "        [ 1.8158e-03,  9.6321e-05, -6.0730e-03,  ...,  2.8992e-03,\n",
            "         -1.1169e-02, -2.0752e-03],\n",
            "        ...,\n",
            "        [ 6.5613e-04, -1.2817e-03, -4.7302e-03,  ..., -2.9755e-03,\n",
            "         -6.5918e-03,  5.9366e-05],\n",
            "        [ 1.0605e-03, -1.1658e-02,  9.9487e-03,  ..., -3.7079e-03,\n",
            "         -1.9775e-02, -3.0029e-02],\n",
            "        [ 4.8523e-03, -1.0620e-02, -1.0986e-02,  ..., -1.6235e-02,\n",
            "          1.8188e-02, -2.0142e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.19.self_attn.o_proj.weight': tensor([[-1.9684e-03, -5.9509e-03, -7.8125e-03,  ...,  1.1780e-02,\n",
            "          1.7700e-02, -2.3842e-06],\n",
            "        [ 8.3008e-03,  4.8218e-03, -1.0376e-02,  ...,  3.9368e-03,\n",
            "         -1.7929e-04, -1.1902e-02],\n",
            "        [ 3.7384e-03, -2.4414e-02,  1.6098e-03,  ...,  1.3977e-02,\n",
            "          5.5542e-03, -1.4038e-02],\n",
            "        ...,\n",
            "        [ 7.0190e-03, -5.2185e-03, -2.4902e-02,  ..., -2.2125e-03,\n",
            "         -8.4686e-04, -1.1353e-02],\n",
            "        [ 3.4943e-03, -8.4229e-03,  8.8501e-03,  ..., -1.1169e-02,\n",
            "         -2.5146e-02,  8.3008e-03],\n",
            "        [ 1.8921e-02, -4.9744e-03,  5.8289e-03,  ..., -8.9722e-03,\n",
            "         -2.1515e-03,  8.3923e-05]], device='cuda:0', dtype=torch.float16), 'model.layers.19.self_attn.q_proj.weight': tensor([[-0.0007, -0.0038,  0.0093,  ..., -0.0029,  0.0004,  0.0019],\n",
            "        [ 0.0042,  0.0078, -0.0093,  ...,  0.0018, -0.0006,  0.0064],\n",
            "        [ 0.0021, -0.0037,  0.0127,  ...,  0.0024,  0.0045,  0.0046],\n",
            "        ...,\n",
            "        [-0.0039,  0.0187,  0.0172,  ..., -0.0085,  0.0097,  0.0142],\n",
            "        [-0.0092, -0.0102,  0.0117,  ..., -0.0115,  0.0101,  0.0004],\n",
            "        [-0.0078,  0.0256,  0.0028,  ..., -0.0041,  0.0106,  0.0059]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.19.self_attn.v_proj.weight': tensor([[-0.0182, -0.0206,  0.0062,  ...,  0.0041,  0.0157,  0.0048],\n",
            "        [ 0.0075, -0.0126,  0.0095,  ..., -0.0060, -0.0020,  0.0118],\n",
            "        [-0.0028, -0.0204, -0.0220,  ...,  0.0234, -0.0007,  0.0082],\n",
            "        ...,\n",
            "        [-0.0085,  0.0055,  0.0197,  ...,  0.0098, -0.0356,  0.0117],\n",
            "        [ 0.0012,  0.0122, -0.0105,  ...,  0.0026,  0.0216, -0.0171],\n",
            "        [-0.0062, -0.0019, -0.0149,  ..., -0.0120,  0.0010, -0.0006]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.2.block_sparse_moe.gate.weight': tensor([[ 0.0173, -0.0075,  0.0117,  ...,  0.0308, -0.0080,  0.0118],\n",
            "        [-0.0284, -0.0100,  0.0021,  ..., -0.0038,  0.0413, -0.0272],\n",
            "        [-0.0211,  0.0109, -0.0041,  ..., -0.0330, -0.0471,  0.0200],\n",
            "        ...,\n",
            "        [ 0.0081, -0.0022,  0.0186,  ...,  0.0211,  0.0139,  0.0182],\n",
            "        [ 0.0437,  0.0008,  0.0123,  ...,  0.0237,  0.0199, -0.0281],\n",
            "        [ 0.0051,  0.0137,  0.0094,  ..., -0.0281,  0.0197,  0.0220]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.2.input_layernorm.weight': tensor([0.6055, 0.4512, 0.4531,  ..., 0.6367, 0.5117, 0.5664], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.2.post_attention_layernorm.weight': tensor([0.5508, 0.5430, 0.5508,  ..., 0.5430, 0.5469, 0.5391], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.2.self_attn.k_proj.weight': tensor([[ 0.0247,  0.0117,  0.0067,  ..., -0.0034, -0.0082,  0.0116],\n",
            "        [-0.0101,  0.0420,  0.0006,  ...,  0.0032,  0.0126, -0.0039],\n",
            "        [ 0.0043,  0.0051,  0.0064,  ..., -0.0044, -0.0117, -0.0030],\n",
            "        ...,\n",
            "        [ 0.0134,  0.0095,  0.0342,  ..., -0.0014,  0.0320, -0.0238],\n",
            "        [ 0.0092, -0.0110,  0.0256,  ..., -0.0112, -0.0139,  0.0049],\n",
            "        [ 0.0186,  0.0128,  0.0029,  ..., -0.0244, -0.0248, -0.0415]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.2.self_attn.o_proj.weight': tensor([[ 0.0048,  0.0024, -0.0039,  ...,  0.0171,  0.0045,  0.0044],\n",
            "        [-0.0032,  0.0012,  0.0098,  ..., -0.0020, -0.0004,  0.0110],\n",
            "        [-0.0053, -0.0007,  0.0030,  ..., -0.0140,  0.0034, -0.0077],\n",
            "        ...,\n",
            "        [ 0.0056,  0.0026,  0.0126,  ...,  0.0048,  0.0062, -0.0121],\n",
            "        [ 0.0049, -0.0110,  0.0048,  ..., -0.0130, -0.0037, -0.0133],\n",
            "        [ 0.0013, -0.0070,  0.0008,  ...,  0.0176, -0.0165,  0.0016]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.2.self_attn.q_proj.weight': tensor([[-0.0173, -0.0139, -0.0117,  ..., -0.0019,  0.0154,  0.0075],\n",
            "        [ 0.0038, -0.0061,  0.0031,  ...,  0.0138, -0.0192, -0.0093],\n",
            "        [-0.0045, -0.0010,  0.0099,  ..., -0.0008,  0.0036, -0.0014],\n",
            "        ...,\n",
            "        [ 0.0322, -0.0040,  0.0361,  ...,  0.0317,  0.0216, -0.0293],\n",
            "        [-0.0045, -0.0070, -0.0361,  ..., -0.0093,  0.0087, -0.0145],\n",
            "        [-0.0030,  0.0159, -0.0214,  ...,  0.0011, -0.0156, -0.0075]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.2.self_attn.v_proj.weight': tensor([[-1.6479e-02,  7.9727e-04,  3.3875e-03,  ...,  4.1809e-03,\n",
            "          1.1108e-02, -1.5869e-02],\n",
            "        [ 2.0752e-03, -4.7607e-03, -1.0498e-02,  ...,  5.8594e-03,\n",
            "          1.0925e-02, -1.3123e-02],\n",
            "        [-5.8899e-03, -4.2419e-03, -1.0620e-02,  ..., -6.4468e-04,\n",
            "          3.7537e-03,  1.1169e-02],\n",
            "        ...,\n",
            "        [ 1.2878e-02, -2.7618e-03, -2.3651e-03,  ...,  4.4556e-03,\n",
            "          4.5586e-04, -4.2419e-03],\n",
            "        [ 9.8267e-03,  3.4332e-05, -8.4229e-03,  ..., -7.8735e-03,\n",
            "         -1.4572e-03,  6.7444e-03],\n",
            "        [ 9.9487e-03, -1.2146e-02,  5.0354e-03,  ...,  9.2773e-03,\n",
            "          1.4404e-02, -5.0049e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.20.block_sparse_moe.gate.weight': tensor([[-0.0030,  0.0047,  0.0121,  ..., -0.0121, -0.0100, -0.0020],\n",
            "        [ 0.0092,  0.0038,  0.0096,  ..., -0.0066,  0.0121, -0.0138],\n",
            "        [ 0.0025, -0.0026,  0.0029,  ..., -0.0001, -0.0106,  0.0081],\n",
            "        ...,\n",
            "        [ 0.0091,  0.0003, -0.0085,  ...,  0.0120,  0.0018,  0.0055],\n",
            "        [-0.0054, -0.0022,  0.0025,  ..., -0.0035,  0.0189, -0.0098],\n",
            "        [-0.0101,  0.0020, -0.0045,  ...,  0.0117, -0.0112, -0.0002]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.20.input_layernorm.weight': tensor([2.4375, 3.5312, 2.3750,  ..., 2.0938, 2.2812, 2.2188], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.20.post_attention_layernorm.weight': tensor([2.9531, 5.3750, 3.2188,  ..., 2.7031, 2.9531, 2.8750], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.20.self_attn.k_proj.weight': tensor([[-0.0023, -0.0097, -0.0071,  ...,  0.0092,  0.0018,  0.0036],\n",
            "        [-0.0040, -0.0003,  0.0047,  ...,  0.0043, -0.0006, -0.0029],\n",
            "        [-0.0006,  0.0014,  0.0041,  ..., -0.0070, -0.0024, -0.0052],\n",
            "        ...,\n",
            "        [-0.0020,  0.0004,  0.0033,  ..., -0.0117,  0.0084, -0.0019],\n",
            "        [-0.0026,  0.0063, -0.0008,  ...,  0.0015,  0.0074, -0.0010],\n",
            "        [ 0.0064, -0.0019, -0.0069,  ...,  0.0032, -0.0005, -0.0041]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.20.self_attn.o_proj.weight': tensor([[-0.0089, -0.0032,  0.0076,  ..., -0.0028, -0.0115, -0.0160],\n",
            "        [-0.0048, -0.0102,  0.0060,  ..., -0.0040, -0.0081,  0.0020],\n",
            "        [ 0.0031, -0.0117,  0.0099,  ..., -0.0168, -0.0045, -0.0027],\n",
            "        ...,\n",
            "        [-0.0069,  0.0160, -0.0030,  ...,  0.0113, -0.0029,  0.0120],\n",
            "        [ 0.0050, -0.0139,  0.0190,  ..., -0.0035, -0.0144,  0.0253],\n",
            "        [-0.0112, -0.0092,  0.0106,  ...,  0.0001,  0.0018,  0.0203]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.20.self_attn.q_proj.weight': tensor([[-0.0038,  0.0012,  0.0019,  ...,  0.0064, -0.0029, -0.0018],\n",
            "        [ 0.0029, -0.0070,  0.0089,  ..., -0.0066, -0.0048, -0.0012],\n",
            "        [ 0.0024,  0.0042, -0.0058,  ...,  0.0008, -0.0021,  0.0023],\n",
            "        ...,\n",
            "        [-0.0150,  0.0079, -0.0050,  ..., -0.0021,  0.0060,  0.0059],\n",
            "        [-0.0051, -0.0016, -0.0036,  ...,  0.0031, -0.0059,  0.0100],\n",
            "        [ 0.0006,  0.0072, -0.0030,  ...,  0.0060,  0.0123, -0.0009]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.20.self_attn.v_proj.weight': tensor([[-1.0803e-02,  1.0986e-02, -2.8381e-03,  ..., -1.3275e-03,\n",
            "          3.9062e-03,  8.6784e-05],\n",
            "        [-2.3346e-03, -2.9755e-03,  1.2512e-02,  ...,  1.9684e-03,\n",
            "         -1.3916e-02,  3.6469e-03],\n",
            "        [ 1.0193e-02, -1.3306e-02,  5.6152e-03,  ...,  9.8267e-03,\n",
            "          1.8768e-03,  1.2589e-03],\n",
            "        ...,\n",
            "        [ 3.0975e-03,  1.5869e-02, -2.2217e-02,  ...,  1.3550e-02,\n",
            "          9.6436e-03, -1.4954e-02],\n",
            "        [-6.1951e-03, -4.7302e-03,  3.7994e-03,  ..., -9.5825e-03,\n",
            "          2.0264e-02,  5.4321e-03],\n",
            "        [-6.5002e-03,  6.5613e-03, -3.5400e-03,  ...,  3.1738e-03,\n",
            "          2.3804e-02, -1.1780e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.21.block_sparse_moe.gate.weight': tensor([[ 1.3245e-02,  2.8992e-03, -6.1035e-03,  ...,  3.1128e-03,\n",
            "         -8.7280e-03, -4.4861e-03],\n",
            "        [-6.0797e-05,  6.4392e-03, -9.0027e-04,  ...,  5.3101e-03,\n",
            "         -4.6082e-03, -4.3335e-03],\n",
            "        [ 1.1292e-02, -4.2114e-03, -3.5706e-03,  ..., -8.8501e-03,\n",
            "          6.9580e-03,  1.3733e-02],\n",
            "        ...,\n",
            "        [ 4.0894e-03, -8.1177e-03, -1.9836e-03,  ...,  1.1719e-02,\n",
            "          1.5198e-02,  6.4697e-03],\n",
            "        [-2.7466e-03,  3.6621e-03,  4.7607e-03,  ..., -3.3722e-03,\n",
            "         -3.6316e-03,  1.3123e-03],\n",
            "        [-8.4839e-03, -8.1787e-03,  8.0490e-04,  ..., -1.4709e-02,\n",
            "          1.9302e-03, -4.5471e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.21.input_layernorm.weight': tensor([2.2344, 2.7812, 2.3594,  ..., 2.0469, 2.1406, 2.2500], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.21.post_attention_layernorm.weight': tensor([3.1562, 4.1875, 3.4219,  ..., 2.9062, 3.1406, 3.0781], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.21.self_attn.k_proj.weight': tensor([[ 0.0011, -0.0022, -0.0096,  ..., -0.0007,  0.0049, -0.0038],\n",
            "        [-0.0028, -0.0006,  0.0009,  ..., -0.0128, -0.0040, -0.0002],\n",
            "        [ 0.0021, -0.0036,  0.0015,  ..., -0.0049, -0.0039, -0.0005],\n",
            "        ...,\n",
            "        [ 0.0012, -0.0112, -0.0037,  ..., -0.0076,  0.0017, -0.0039],\n",
            "        [-0.0126,  0.0006,  0.0006,  ..., -0.0200, -0.0123,  0.0008],\n",
            "        [ 0.0222,  0.0009, -0.0010,  ...,  0.0113,  0.0079,  0.0096]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.21.self_attn.o_proj.weight': tensor([[-0.0017,  0.0080,  0.0021,  ..., -0.0070, -0.0172,  0.0009],\n",
            "        [ 0.0057,  0.0049, -0.0073,  ...,  0.0077,  0.0012,  0.0046],\n",
            "        [-0.0041,  0.0095,  0.0104,  ..., -0.0050,  0.0052,  0.0067],\n",
            "        ...,\n",
            "        [-0.0139, -0.0050, -0.0188,  ...,  0.0024,  0.0109,  0.0004],\n",
            "        [ 0.0074,  0.0056,  0.0219,  ..., -0.0056,  0.0118,  0.0139],\n",
            "        [ 0.0059,  0.0176,  0.0096,  ..., -0.0007,  0.0054,  0.0028]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.21.self_attn.q_proj.weight': tensor([[-0.0007, -0.0135,  0.0020,  ..., -0.0120, -0.0021,  0.0008],\n",
            "        [-0.0012, -0.0018, -0.0096,  ..., -0.0018, -0.0035, -0.0059],\n",
            "        [-0.0026, -0.0040, -0.0063,  ...,  0.0033,  0.0052, -0.0011],\n",
            "        ...,\n",
            "        [ 0.0048, -0.0131,  0.0245,  ..., -0.0089,  0.0089,  0.0045],\n",
            "        [-0.0178, -0.0081,  0.0156,  ..., -0.0165,  0.0013,  0.0019],\n",
            "        [ 0.0056, -0.0172, -0.0092,  ..., -0.0063, -0.0099,  0.0036]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.21.self_attn.v_proj.weight': tensor([[ 1.6113e-02, -1.0071e-02, -9.2773e-03,  ...,  1.5015e-02,\n",
            "          1.8082e-03, -4.5166e-03],\n",
            "        [ 1.8555e-02,  2.9449e-03,  5.8289e-03,  ...,  2.5749e-04,\n",
            "          5.0964e-03,  2.6855e-02],\n",
            "        [ 1.4771e-02, -2.4902e-02, -3.2654e-03,  ..., -7.9956e-03,\n",
            "         -1.5442e-02, -1.2207e-02],\n",
            "        ...,\n",
            "        [-1.1719e-02,  1.5625e-02,  5.1270e-03,  ...,  1.4343e-02,\n",
            "          2.3346e-03, -1.0315e-02],\n",
            "        [-1.4771e-02,  1.1108e-02,  1.5198e-02,  ...,  4.8828e-04,\n",
            "         -7.1716e-03, -1.2878e-02],\n",
            "        [-7.2021e-03, -5.4932e-03,  6.7444e-03,  ...,  3.7670e-05,\n",
            "          5.7983e-04,  1.0498e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.22.block_sparse_moe.gate.weight': tensor([[ 0.0139, -0.0021,  0.0014,  ...,  0.0075,  0.0046, -0.0166],\n",
            "        [-0.0112, -0.0066,  0.0053,  ..., -0.0012, -0.0003,  0.0049],\n",
            "        [ 0.0007,  0.0068, -0.0006,  ..., -0.0144,  0.0060,  0.0129],\n",
            "        ...,\n",
            "        [ 0.0090, -0.0025, -0.0021,  ..., -0.0142,  0.0036,  0.0069],\n",
            "        [ 0.0019, -0.0032, -0.0002,  ...,  0.0159, -0.0005,  0.0110],\n",
            "        [-0.0054,  0.0052,  0.0016,  ..., -0.0007, -0.0016, -0.0075]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.22.input_layernorm.weight': tensor([2.2031, 2.3750, 2.2344,  ..., 2.1406, 2.0781, 2.1250], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.22.post_attention_layernorm.weight': tensor([3.2500, 3.9375, 3.4531,  ..., 3.0000, 3.2344, 3.1719], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.22.self_attn.k_proj.weight': tensor([[-0.0050,  0.0012,  0.0019,  ...,  0.0030,  0.0025,  0.0095],\n",
            "        [ 0.0036, -0.0015,  0.0013,  ..., -0.0006, -0.0007,  0.0009],\n",
            "        [ 0.0019, -0.0054,  0.0023,  ...,  0.0032, -0.0030,  0.0054],\n",
            "        ...,\n",
            "        [-0.0043, -0.0011, -0.0031,  ..., -0.0140,  0.0134, -0.0074],\n",
            "        [ 0.0176,  0.0082,  0.0052,  ...,  0.0033, -0.0059,  0.0047],\n",
            "        [-0.0014,  0.0064, -0.0072,  ..., -0.0063,  0.0064, -0.0027]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.22.self_attn.o_proj.weight': tensor([[-0.0038, -0.0123,  0.0205,  ...,  0.0090,  0.0043,  0.0082],\n",
            "        [ 0.0046,  0.0009, -0.0108,  ..., -0.0104,  0.0041,  0.0031],\n",
            "        [-0.0284, -0.0008, -0.0052,  ...,  0.0109, -0.0019,  0.0139],\n",
            "        ...,\n",
            "        [-0.0125, -0.0038,  0.0092,  ..., -0.0042,  0.0095,  0.0209],\n",
            "        [-0.0176,  0.0342,  0.0025,  ...,  0.0066,  0.0109, -0.0019],\n",
            "        [-0.0052,  0.0298,  0.0045,  ..., -0.0019, -0.0055,  0.0049]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.22.self_attn.q_proj.weight': tensor([[-0.0057,  0.0004, -0.0021,  ...,  0.0008, -0.0045, -0.0021],\n",
            "        [-0.0007, -0.0001, -0.0081,  ..., -0.0019,  0.0070, -0.0013],\n",
            "        [-0.0019, -0.0039,  0.0007,  ..., -0.0010,  0.0045, -0.0017],\n",
            "        ...,\n",
            "        [ 0.0132,  0.0091, -0.0080,  ...,  0.0108, -0.0034,  0.0050],\n",
            "        [-0.0040, -0.0032,  0.0048,  ..., -0.0054, -0.0093, -0.0033],\n",
            "        [-0.0216,  0.0077, -0.0041,  ...,  0.0173, -0.0001,  0.0054]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.22.self_attn.v_proj.weight': tensor([[-0.0177, -0.0164, -0.0032,  ...,  0.0046,  0.0079,  0.0025],\n",
            "        [ 0.0054, -0.0031,  0.0007,  ...,  0.0096,  0.0098,  0.0374],\n",
            "        [-0.0276, -0.0056,  0.0097,  ...,  0.0086, -0.0103,  0.0079],\n",
            "        ...,\n",
            "        [ 0.0247, -0.0112, -0.0022,  ...,  0.0115,  0.0022, -0.0036],\n",
            "        [-0.0173,  0.0177, -0.0018,  ...,  0.0210,  0.0062,  0.0112],\n",
            "        [-0.0049, -0.0011,  0.0108,  ..., -0.0078, -0.0221,  0.0138]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.23.block_sparse_moe.gate.weight': tensor([[-0.0038,  0.0003, -0.0091,  ...,  0.0093,  0.0082,  0.0077],\n",
            "        [-0.0049,  0.0042,  0.0010,  ...,  0.0002, -0.0063, -0.0084],\n",
            "        [ 0.0111, -0.0087,  0.0031,  ...,  0.0166,  0.0043,  0.0060],\n",
            "        ...,\n",
            "        [-0.0001,  0.0045,  0.0058,  ...,  0.0092, -0.0164,  0.0010],\n",
            "        [-0.0063,  0.0045, -0.0036,  ..., -0.0139,  0.0107,  0.0014],\n",
            "        [ 0.0025,  0.0040, -0.0050,  ..., -0.0096, -0.0005, -0.0025]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.23.input_layernorm.weight': tensor([2.3125, 2.5938, 2.3125,  ..., 2.1875, 2.2031, 2.2188], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.23.post_attention_layernorm.weight': tensor([3.3906, 3.9375, 3.5781,  ..., 3.1562, 3.3594, 3.3281], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.23.self_attn.k_proj.weight': tensor([[ 0.0049, -0.0005,  0.0002,  ...,  0.0033,  0.0020,  0.0040],\n",
            "        [ 0.0039,  0.0058, -0.0025,  ..., -0.0092, -0.0102,  0.0019],\n",
            "        [ 0.0056,  0.0090, -0.0053,  ..., -0.0054,  0.0072,  0.0001],\n",
            "        ...,\n",
            "        [ 0.0123,  0.0094, -0.0039,  ..., -0.0052, -0.0032,  0.0045],\n",
            "        [ 0.0014, -0.0104, -0.0103,  ..., -0.0142,  0.0006,  0.0005],\n",
            "        [ 0.0131,  0.0021,  0.0085,  ..., -0.0057,  0.0053, -0.0046]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.23.self_attn.o_proj.weight': tensor([[-0.0019, -0.0011, -0.0142,  ...,  0.0037,  0.0080, -0.0017],\n",
            "        [-0.0005,  0.0022,  0.0012,  ...,  0.0106, -0.0113,  0.0089],\n",
            "        [-0.0076, -0.0047,  0.0110,  ...,  0.0242, -0.0266,  0.0030],\n",
            "        ...,\n",
            "        [-0.0002,  0.0055, -0.0277,  ..., -0.0012, -0.0010, -0.0128],\n",
            "        [ 0.0037,  0.0198,  0.0087,  ...,  0.0026,  0.0075,  0.0056],\n",
            "        [ 0.0018, -0.0039, -0.0060,  ...,  0.0011,  0.0205,  0.0045]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.23.self_attn.q_proj.weight': tensor([[-0.0017, -0.0024,  0.0021,  ..., -0.0027,  0.0006,  0.0005],\n",
            "        [-0.0041,  0.0109, -0.0094,  ..., -0.0152,  0.0019,  0.0095],\n",
            "        [-0.0046, -0.0034, -0.0050,  ..., -0.0050,  0.0107, -0.0052],\n",
            "        ...,\n",
            "        [ 0.0012,  0.0055,  0.0040,  ..., -0.0041,  0.0010, -0.0030],\n",
            "        [ 0.0110, -0.0049,  0.0056,  ..., -0.0005, -0.0003,  0.0005],\n",
            "        [-0.0053,  0.0120,  0.0039,  ...,  0.0017, -0.0095,  0.0109]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.23.self_attn.v_proj.weight': tensor([[-0.0237,  0.0085,  0.0131,  ...,  0.0040,  0.0130,  0.0059],\n",
            "        [-0.0117,  0.0029, -0.0195,  ...,  0.0033,  0.0085,  0.0106],\n",
            "        [ 0.0036, -0.0311,  0.0043,  ...,  0.0045, -0.0183, -0.0069],\n",
            "        ...,\n",
            "        [ 0.0078,  0.0081,  0.0027,  ..., -0.0123,  0.0065, -0.0151],\n",
            "        [ 0.0109,  0.0034,  0.0249,  ..., -0.0022, -0.0164,  0.0132],\n",
            "        [ 0.0011,  0.0039, -0.0093,  ..., -0.0195,  0.0120,  0.0035]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.24.block_sparse_moe.gate.weight': tensor([[-1.0529e-03,  5.0659e-03,  1.3351e-03,  ...,  9.2163e-03,\n",
            "          4.6997e-03,  1.2390e-02],\n",
            "        [-1.3275e-03, -6.6528e-03, -2.1362e-03,  ...,  3.0975e-03,\n",
            "         -3.0365e-03,  1.2665e-03],\n",
            "        [-2.0752e-03,  4.3030e-03,  4.6387e-03,  ...,  5.3406e-03,\n",
            "          6.4392e-03,  9.1553e-04],\n",
            "        ...,\n",
            "        [ 2.1515e-03, -6.3477e-03, -4.2114e-03,  ..., -2.0264e-02,\n",
            "          6.3477e-03, -4.0588e-03],\n",
            "        [ 3.2959e-03,  3.9062e-03,  4.4632e-04,  ..., -5.8289e-03,\n",
            "         -1.2817e-02, -2.3499e-03],\n",
            "        [-3.1433e-03, -4.2200e-05,  7.1106e-03,  ...,  1.0620e-02,\n",
            "         -5.2185e-03, -8.9111e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.24.input_layernorm.weight': tensor([2.6875, 2.8125, 2.6875,  ..., 2.4375, 2.5625, 2.5625], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.24.post_attention_layernorm.weight': tensor([3.5000, 3.9688, 3.6875,  ..., 3.2812, 3.4531, 3.4062], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.24.self_attn.k_proj.weight': tensor([[ 0.0025, -0.0030,  0.0028,  ...,  0.0022,  0.0036, -0.0008],\n",
            "        [ 0.0026, -0.0043, -0.0010,  ..., -0.0002,  0.0054,  0.0042],\n",
            "        [-0.0041, -0.0105,  0.0037,  ...,  0.0059,  0.0046, -0.0104],\n",
            "        ...,\n",
            "        [ 0.0099, -0.0090, -0.0081,  ...,  0.0036, -0.0106,  0.0096],\n",
            "        [ 0.0119,  0.0063,  0.0164,  ...,  0.0132,  0.0128, -0.0036],\n",
            "        [-0.0056,  0.0211, -0.0045,  ..., -0.0082,  0.0177,  0.0136]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.24.self_attn.o_proj.weight': tensor([[ 0.0046, -0.0107,  0.0026,  ...,  0.0049, -0.0055,  0.0134],\n",
            "        [ 0.0127,  0.0032, -0.0007,  ..., -0.0001, -0.0204, -0.0028],\n",
            "        [-0.0102,  0.0172, -0.0096,  ..., -0.0059,  0.0062,  0.0069],\n",
            "        ...,\n",
            "        [ 0.0019, -0.0066, -0.0103,  ...,  0.0104,  0.0029,  0.0129],\n",
            "        [ 0.0053, -0.0228, -0.0088,  ..., -0.0052,  0.0048, -0.0109],\n",
            "        [ 0.0031,  0.0171,  0.0134,  ..., -0.0085,  0.0042, -0.0042]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.24.self_attn.q_proj.weight': tensor([[ 0.0068,  0.0006,  0.0073,  ...,  0.0027,  0.0016, -0.0006],\n",
            "        [-0.0087,  0.0043,  0.0075,  ..., -0.0069,  0.0072,  0.0029],\n",
            "        [ 0.0059,  0.0059, -0.0009,  ...,  0.0032,  0.0144,  0.0031],\n",
            "        ...,\n",
            "        [-0.0040,  0.0091, -0.0149,  ..., -0.0104,  0.0058,  0.0046],\n",
            "        [ 0.0024, -0.0095, -0.0023,  ..., -0.0081, -0.0041, -0.0189],\n",
            "        [ 0.0093, -0.0009,  0.0115,  ...,  0.0060,  0.0056, -0.0081]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.24.self_attn.v_proj.weight': tensor([[-0.0068,  0.0099, -0.0161,  ...,  0.0094, -0.0236,  0.0089],\n",
            "        [ 0.0084,  0.0156, -0.0139,  ..., -0.0138, -0.0177,  0.0117],\n",
            "        [-0.0079, -0.0012,  0.0165,  ...,  0.0080,  0.0032,  0.0184],\n",
            "        ...,\n",
            "        [-0.0308, -0.0093,  0.0038,  ..., -0.0027,  0.0298, -0.0085],\n",
            "        [ 0.0121, -0.0046, -0.0212,  ..., -0.0069, -0.0039, -0.0016],\n",
            "        [ 0.0189,  0.0167, -0.0102,  ...,  0.0036, -0.0038,  0.0140]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.25.block_sparse_moe.gate.weight': tensor([[-0.0036,  0.0055, -0.0038,  ...,  0.0069,  0.0036, -0.0053],\n",
            "        [-0.0008,  0.0045, -0.0008,  ...,  0.0137, -0.0016,  0.0052],\n",
            "        [ 0.0049,  0.0044,  0.0054,  ..., -0.0159, -0.0010,  0.0071],\n",
            "        ...,\n",
            "        [ 0.0023, -0.0066, -0.0062,  ...,  0.0184,  0.0049, -0.0045],\n",
            "        [-0.0004,  0.0020,  0.0068,  ..., -0.0121, -0.0099, -0.0010],\n",
            "        [-0.0044, -0.0051,  0.0058,  ...,  0.0044, -0.0030,  0.0072]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.25.input_layernorm.weight': tensor([2.6719, 2.7344, 2.7812,  ..., 2.4531, 2.5781, 2.5625], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.25.post_attention_layernorm.weight': tensor([3.5938, 4.0312, 3.7500,  ..., 3.4219, 3.5938, 3.5469], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.25.self_attn.k_proj.weight': tensor([[-1.0132e-02, -4.2725e-04,  2.0313e-04,  ...,  3.9673e-04,\n",
            "          1.4038e-03,  8.0490e-04],\n",
            "        [-1.2634e-02, -1.0529e-03, -8.9645e-04,  ..., -4.4556e-03,\n",
            "         -2.2278e-03, -3.4904e-04],\n",
            "        [ 4.9591e-04, -6.9809e-04, -1.1921e-07,  ...,  2.8992e-03,\n",
            "          1.1015e-04, -1.5945e-03],\n",
            "        ...,\n",
            "        [ 7.4463e-03, -3.6621e-03,  1.7944e-02,  ..., -1.8677e-02,\n",
            "         -9.6436e-03,  1.1536e-02],\n",
            "        [-2.0599e-03,  2.0447e-03,  2.3956e-03,  ..., -8.7357e-04,\n",
            "          5.1270e-03, -3.4790e-03],\n",
            "        [-3.7994e-03, -7.8735e-03,  1.1978e-03,  ..., -5.7602e-04,\n",
            "         -4.0817e-04, -5.1270e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.25.self_attn.o_proj.weight': tensor([[ 0.0014, -0.0160, -0.0031,  ...,  0.0017,  0.0302, -0.0177],\n",
            "        [ 0.0175,  0.0354, -0.0018,  ...,  0.0065,  0.0082, -0.0030],\n",
            "        [-0.0117, -0.0347, -0.0074,  ...,  0.0199,  0.0017,  0.0128],\n",
            "        ...,\n",
            "        [ 0.0003,  0.0010, -0.0121,  ...,  0.0276, -0.0077, -0.0101],\n",
            "        [ 0.0074, -0.0003,  0.0007,  ..., -0.0034,  0.0026,  0.0247],\n",
            "        [-0.0118, -0.0126, -0.0192,  ..., -0.0066,  0.0123,  0.0186]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.25.self_attn.q_proj.weight': tensor([[-6.7749e-03, -2.8534e-03,  1.0742e-02,  ..., -1.1963e-02,\n",
            "          1.0803e-02, -1.2390e-02],\n",
            "        [-4.4441e-04,  4.7493e-04, -2.0905e-03,  ...,  1.2878e-02,\n",
            "          4.7607e-03,  1.2512e-03],\n",
            "        [ 6.3171e-03, -5.5313e-04,  2.8839e-03,  ...,  9.2773e-03,\n",
            "         -1.1292e-03,  1.6785e-04],\n",
            "        ...,\n",
            "        [-3.5477e-04,  1.6479e-02, -2.0447e-03,  ...,  1.9775e-02,\n",
            "         -9.5215e-03,  3.6621e-04],\n",
            "        [ 2.3804e-02, -6.2466e-05,  3.0396e-02,  ..., -3.6316e-03,\n",
            "          6.9275e-03, -8.7891e-03],\n",
            "        [ 2.0996e-02, -1.9165e-02,  1.0452e-03,  ...,  7.6599e-03,\n",
            "         -1.7456e-02,  1.5747e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.25.self_attn.v_proj.weight': tensor([[ 0.0104, -0.0104, -0.0085,  ...,  0.0154,  0.0017, -0.0058],\n",
            "        [-0.0085, -0.0206, -0.0033,  ...,  0.0153, -0.0049, -0.0273],\n",
            "        [ 0.0144, -0.0038, -0.0055,  ...,  0.0166, -0.0299,  0.0171],\n",
            "        ...,\n",
            "        [-0.0042,  0.0150,  0.0084,  ...,  0.0110,  0.0079, -0.0095],\n",
            "        [-0.0007,  0.0109, -0.0205,  ..., -0.0177,  0.0128,  0.0042],\n",
            "        [-0.0068, -0.0017, -0.0012,  ...,  0.0110,  0.0031,  0.0081]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.26.block_sparse_moe.gate.weight': tensor([[ 0.0059,  0.0026, -0.0037,  ..., -0.0167, -0.0053,  0.0003],\n",
            "        [-0.0018,  0.0050,  0.0009,  ...,  0.0059,  0.0004,  0.0044],\n",
            "        [ 0.0067, -0.0042,  0.0043,  ..., -0.0054,  0.0041,  0.0043],\n",
            "        ...,\n",
            "        [-0.0011, -0.0016, -0.0068,  ..., -0.0010, -0.0003, -0.0055],\n",
            "        [-0.0022, -0.0061,  0.0040,  ...,  0.0067, -0.0028,  0.0060],\n",
            "        [-0.0041,  0.0034,  0.0085,  ...,  0.0015, -0.0019, -0.0052]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.26.input_layernorm.weight': tensor([2.4219, 2.4844, 2.4219,  ..., 2.4219, 2.2812, 2.4062], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.26.post_attention_layernorm.weight': tensor([3.7031, 4.0000, 3.7969,  ..., 3.4844, 3.6250, 3.6094], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.26.self_attn.k_proj.weight': tensor([[-0.0031, -0.0076,  0.0019,  ..., -0.0082, -0.0044,  0.0026],\n",
            "        [ 0.0053, -0.0016, -0.0090,  ...,  0.0018, -0.0036,  0.0078],\n",
            "        [-0.0069, -0.0043,  0.0094,  ...,  0.0038,  0.0047, -0.0081],\n",
            "        ...,\n",
            "        [ 0.0043, -0.0021,  0.0070,  ...,  0.0002, -0.0039,  0.0023],\n",
            "        [ 0.0049,  0.0010, -0.0046,  ...,  0.0029, -0.0024, -0.0058],\n",
            "        [-0.0077, -0.0063,  0.0118,  ..., -0.0103,  0.0032, -0.0020]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.26.self_attn.o_proj.weight': tensor([[ 0.0022,  0.0090, -0.0003,  ..., -0.0099, -0.0089, -0.0068],\n",
            "        [-0.0037, -0.0001, -0.0098,  ..., -0.0125,  0.0005,  0.0042],\n",
            "        [ 0.0020,  0.0016, -0.0040,  ...,  0.0014, -0.0233,  0.0178],\n",
            "        ...,\n",
            "        [-0.0106, -0.0294,  0.0145,  ..., -0.0088, -0.0234, -0.0075],\n",
            "        [-0.0129, -0.0053, -0.0104,  ...,  0.0015,  0.0019,  0.0078],\n",
            "        [ 0.0047, -0.0177, -0.0135,  ..., -0.0044,  0.0142,  0.0018]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.26.self_attn.q_proj.weight': tensor([[ 0.0002, -0.0072,  0.0019,  ..., -0.0035,  0.0022, -0.0019],\n",
            "        [ 0.0044,  0.0010,  0.0007,  ..., -0.0004,  0.0095,  0.0077],\n",
            "        [ 0.0119,  0.0017, -0.0055,  ..., -0.0120, -0.0069,  0.0022],\n",
            "        ...,\n",
            "        [ 0.0092,  0.0125, -0.0029,  ..., -0.0138, -0.0038, -0.0161],\n",
            "        [-0.0060, -0.0276,  0.0071,  ...,  0.0102,  0.0067, -0.0089],\n",
            "        [ 0.0118,  0.0084,  0.0001,  ...,  0.0090,  0.0117, -0.0114]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.26.self_attn.v_proj.weight': tensor([[ 0.0175,  0.0067,  0.0135,  ...,  0.0164,  0.0044,  0.0109],\n",
            "        [-0.0082,  0.0148, -0.0308,  ...,  0.0097,  0.0121, -0.0148],\n",
            "        [-0.0366,  0.0038, -0.0178,  ...,  0.0104,  0.0142,  0.0020],\n",
            "        ...,\n",
            "        [-0.0096,  0.0037,  0.0142,  ...,  0.0101, -0.0242,  0.0215],\n",
            "        [ 0.0003,  0.0143,  0.0266,  ..., -0.0034,  0.0178,  0.0151],\n",
            "        [-0.0042, -0.0007, -0.0052,  ...,  0.0026,  0.0261, -0.0073]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.27.block_sparse_moe.gate.weight': tensor([[-0.0022, -0.0049, -0.0030,  ..., -0.0067, -0.0045,  0.0070],\n",
            "        [-0.0015, -0.0034,  0.0064,  ..., -0.0083,  0.0096, -0.0023],\n",
            "        [-0.0025, -0.0028,  0.0042,  ...,  0.0048,  0.0023,  0.0027],\n",
            "        ...,\n",
            "        [ 0.0002,  0.0014,  0.0010,  ...,  0.0100,  0.0074,  0.0049],\n",
            "        [ 0.0060,  0.0044, -0.0026,  ..., -0.0096, -0.0022, -0.0061],\n",
            "        [ 0.0015,  0.0011, -0.0033,  ...,  0.0042, -0.0010,  0.0012]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.27.input_layernorm.weight': tensor([2.3906, 2.6562, 2.3906,  ..., 2.3594, 2.2812, 2.3906], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.27.post_attention_layernorm.weight': tensor([3.7344, 4.0625, 3.8594,  ..., 3.5781, 3.6719, 3.6875], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.27.self_attn.k_proj.weight': tensor([[ 0.0053, -0.0015, -0.0051,  ..., -0.0052,  0.0033, -0.0046],\n",
            "        [ 0.0069,  0.0003,  0.0007,  ..., -0.0194, -0.0090, -0.0038],\n",
            "        [ 0.0018, -0.0002, -0.0060,  ...,  0.0041,  0.0061, -0.0060],\n",
            "        ...,\n",
            "        [-0.0018,  0.0024,  0.0023,  ...,  0.0018, -0.0059, -0.0031],\n",
            "        [ 0.0006,  0.0056, -0.0036,  ..., -0.0031, -0.0022,  0.0022],\n",
            "        [ 0.0038, -0.0060,  0.0096,  ...,  0.0019, -0.0103,  0.0002]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.27.self_attn.o_proj.weight': tensor([[-0.0024,  0.0039,  0.0011,  ..., -0.0041, -0.0016,  0.0071],\n",
            "        [-0.0134, -0.0165,  0.0002,  ...,  0.0110, -0.0154,  0.0198],\n",
            "        [-0.0104, -0.0054,  0.0060,  ...,  0.0161, -0.0056, -0.0253],\n",
            "        ...,\n",
            "        [-0.0065, -0.0093,  0.0192,  ...,  0.0058,  0.0098,  0.0182],\n",
            "        [ 0.0083,  0.0017,  0.0074,  ..., -0.0065, -0.0052,  0.0067],\n",
            "        [ 0.0006,  0.0160, -0.0047,  ..., -0.0239,  0.0200,  0.0018]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.27.self_attn.q_proj.weight': tensor([[ 0.0033,  0.0012, -0.0052,  ...,  0.0113,  0.0150, -0.0052],\n",
            "        [-0.0065, -0.0021, -0.0073,  ...,  0.0089, -0.0039,  0.0013],\n",
            "        [-0.0047, -0.0059, -0.0068,  ...,  0.0007, -0.0039, -0.0014],\n",
            "        ...,\n",
            "        [-0.0002, -0.0021,  0.0061,  ..., -0.0068,  0.0132,  0.0024],\n",
            "        [ 0.0052,  0.0034, -0.0059,  ..., -0.0035, -0.0013, -0.0071],\n",
            "        [-0.0049,  0.0045,  0.0071,  ...,  0.0083,  0.0099, -0.0039]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.27.self_attn.v_proj.weight': tensor([[ 0.0070,  0.0165, -0.0229,  ...,  0.0142,  0.0023,  0.0037],\n",
            "        [ 0.0236,  0.0025, -0.0201,  ..., -0.0007,  0.0095, -0.0044],\n",
            "        [-0.0225,  0.0038,  0.0206,  ...,  0.0211,  0.0013, -0.0104],\n",
            "        ...,\n",
            "        [ 0.0249,  0.0248, -0.0337,  ..., -0.0009, -0.0197,  0.0270],\n",
            "        [-0.0160, -0.0063,  0.0126,  ...,  0.0171, -0.0008, -0.0237],\n",
            "        [-0.0011, -0.0168,  0.0272,  ..., -0.0154,  0.0027, -0.0006]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.28.block_sparse_moe.gate.weight': tensor([[ 0.0008, -0.0006,  0.0012,  ...,  0.0085,  0.0004,  0.0009],\n",
            "        [ 0.0010, -0.0006,  0.0002,  ..., -0.0003, -0.0072, -0.0047],\n",
            "        [ 0.0035,  0.0022, -0.0015,  ...,  0.0010, -0.0025, -0.0021],\n",
            "        ...,\n",
            "        [ 0.0026, -0.0015,  0.0009,  ..., -0.0028,  0.0045,  0.0037],\n",
            "        [-0.0059,  0.0027,  0.0024,  ..., -0.0082,  0.0029,  0.0034],\n",
            "        [-0.0048,  0.0038,  0.0007,  ...,  0.0098, -0.0010,  0.0105]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.28.input_layernorm.weight': tensor([2.4531, 2.6406, 2.5000,  ..., 2.2812, 2.3750, 2.3281], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.28.post_attention_layernorm.weight': tensor([3.7656, 4.0938, 3.9375,  ..., 3.7031, 3.7500, 3.7188], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.28.self_attn.k_proj.weight': tensor([[-7.9346e-03, -8.8501e-03,  8.5449e-03,  ...,  3.6774e-03,\n",
            "          2.1820e-03,  2.2278e-03],\n",
            "        [ 1.6403e-03,  4.1199e-03, -4.5776e-03,  ..., -1.4877e-03,\n",
            "          6.3477e-03, -1.8997e-03],\n",
            "        [ 9.6560e-06, -5.6458e-03, -5.3711e-03,  ...,  1.4877e-03,\n",
            "          3.0670e-03,  3.8757e-03],\n",
            "        ...,\n",
            "        [-6.4087e-03, -4.8828e-03,  5.0354e-03,  ...,  2.9755e-03,\n",
            "         -9.7046e-03,  9.0790e-04],\n",
            "        [-1.1597e-03,  4.1771e-04, -7.3547e-03,  ..., -6.8054e-03,\n",
            "         -4.3030e-03, -2.4414e-03],\n",
            "        [ 5.5542e-03,  4.2419e-03, -5.3711e-03,  ..., -8.7738e-04,\n",
            "         -6.1646e-03, -5.9128e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.28.self_attn.o_proj.weight': tensor([[ 0.0014,  0.0079,  0.0128,  ..., -0.0061, -0.0104, -0.0085],\n",
            "        [-0.0024,  0.0070, -0.0229,  ...,  0.0101,  0.0151,  0.0045],\n",
            "        [ 0.0142,  0.0095,  0.0172,  ...,  0.0012,  0.0117, -0.0082],\n",
            "        ...,\n",
            "        [-0.0120, -0.0032,  0.0146,  ...,  0.0013, -0.0193,  0.0228],\n",
            "        [-0.0109,  0.0181,  0.0060,  ..., -0.0103,  0.0050,  0.0020],\n",
            "        [-0.0070, -0.0052, -0.0038,  ..., -0.0106,  0.0059, -0.0181]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.28.self_attn.q_proj.weight': tensor([[-0.0023,  0.0010, -0.0069,  ...,  0.0080,  0.0009,  0.0007],\n",
            "        [ 0.0037, -0.0032,  0.0035,  ..., -0.0038, -0.0044,  0.0035],\n",
            "        [-0.0015,  0.0050,  0.0040,  ...,  0.0020, -0.0064, -0.0002],\n",
            "        ...,\n",
            "        [-0.0049, -0.0146,  0.0054,  ..., -0.0099, -0.0042, -0.0092],\n",
            "        [-0.0008,  0.0089,  0.0018,  ..., -0.0003,  0.0042, -0.0009],\n",
            "        [-0.0030, -0.0046,  0.0030,  ...,  0.0179,  0.0208, -0.0017]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.28.self_attn.v_proj.weight': tensor([[ 0.0242,  0.0035, -0.0138,  ..., -0.0082,  0.0049, -0.0171],\n",
            "        [ 0.0156,  0.0045,  0.0067,  ...,  0.0270, -0.0057,  0.0025],\n",
            "        [-0.0182, -0.0127, -0.0097,  ..., -0.0005, -0.0126,  0.0280],\n",
            "        ...,\n",
            "        [-0.0006,  0.0024, -0.0005,  ...,  0.0026, -0.0291,  0.0045],\n",
            "        [-0.0065,  0.0095, -0.0118,  ..., -0.0052, -0.0143,  0.0208],\n",
            "        [ 0.0052,  0.0106,  0.0053,  ...,  0.0101, -0.0044, -0.0255]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.29.block_sparse_moe.gate.weight': tensor([[-0.0008, -0.0025, -0.0001,  ...,  0.0062,  0.0028,  0.0027],\n",
            "        [ 0.0020, -0.0002, -0.0027,  ..., -0.0044, -0.0003,  0.0003],\n",
            "        [-0.0114, -0.0045, -0.0056,  ...,  0.0024,  0.0038, -0.0089],\n",
            "        ...,\n",
            "        [ 0.0008,  0.0020,  0.0028,  ..., -0.0080, -0.0032,  0.0027],\n",
            "        [-0.0022,  0.0069, -0.0009,  ...,  0.0024,  0.0079,  0.0002],\n",
            "        [ 0.0073,  0.0014,  0.0014,  ..., -0.0042, -0.0038, -0.0014]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.29.input_layernorm.weight': tensor([2.8438, 3.1719, 2.8906,  ..., 2.5000, 2.8906, 2.7969], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.29.post_attention_layernorm.weight': tensor([3.7500, 3.9062, 3.8125,  ..., 3.7031, 3.6719, 3.6875], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.29.self_attn.k_proj.weight': tensor([[ 2.9907e-03,  6.1417e-04,  1.0605e-03,  ...,  4.3106e-04,\n",
            "          7.5912e-04,  3.6621e-04],\n",
            "        [-2.5482e-03,  8.0490e-04,  2.3499e-03,  ...,  4.8256e-04,\n",
            "          6.5994e-04,  1.3885e-03],\n",
            "        [-6.1989e-05,  1.9836e-03,  2.4109e-03,  ...,  6.1798e-04,\n",
            "         -1.0529e-03, -5.7983e-04],\n",
            "        ...,\n",
            "        [ 7.9956e-03,  8.0566e-03,  1.5869e-03,  ..., -6.1989e-05,\n",
            "         -1.0986e-02,  1.2817e-03],\n",
            "        [-9.8267e-03, -8.6670e-03,  2.3041e-03,  ..., -2.3651e-03,\n",
            "          1.0529e-03, -3.6469e-03],\n",
            "        [ 1.9531e-03, -1.8005e-03, -4.8828e-03,  ..., -1.4404e-02,\n",
            "          8.4305e-04,  8.2016e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.29.self_attn.o_proj.weight': tensor([[ 1.0986e-02,  2.9175e-02,  1.5198e-02,  ..., -6.9275e-03,\n",
            "         -1.8921e-02,  3.7842e-03],\n",
            "        [-4.8523e-03,  7.7209e-03, -1.6724e-02,  ..., -7.4768e-03,\n",
            "          1.3885e-03, -3.8910e-03],\n",
            "        [ 2.1210e-03,  1.6113e-02,  7.6294e-03,  ...,  4.4861e-03,\n",
            "         -2.5940e-03,  1.2512e-03],\n",
            "        ...,\n",
            "        [-3.7003e-04, -1.5869e-02,  6.9275e-03,  ..., -1.8555e-02,\n",
            "          1.2451e-02,  4.4250e-03],\n",
            "        [ 2.0142e-03, -5.9814e-03,  1.3184e-02,  ..., -4.5471e-03,\n",
            "         -2.7161e-03, -2.6894e-04],\n",
            "        [ 1.8066e-02,  1.7738e-04, -4.2915e-05,  ..., -9.7656e-03,\n",
            "         -1.5411e-03, -1.1353e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.29.self_attn.q_proj.weight': tensor([[ 3.9368e-03, -3.6163e-03,  5.3406e-03,  ..., -6.6528e-03,\n",
            "          1.3411e-05, -4.4250e-03],\n",
            "        [ 4.9438e-03,  4.4556e-03,  2.5330e-03,  ..., -5.0354e-03,\n",
            "          1.2207e-03,  4.8828e-04],\n",
            "        [-2.7161e-03, -5.5542e-03, -9.4604e-03,  ..., -6.4087e-03,\n",
            "         -1.1730e-04, -4.1199e-03],\n",
            "        ...,\n",
            "        [-5.9814e-03,  2.2430e-03, -4.6997e-03,  ...,  2.1362e-03,\n",
            "         -1.6861e-03, -1.2451e-02],\n",
            "        [-1.8539e-03,  1.1108e-02, -2.8076e-02,  ..., -5.8365e-04,\n",
            "          3.2806e-03, -2.7924e-03],\n",
            "        [-3.5858e-03, -3.3417e-03,  1.1169e-02,  ...,  1.5259e-02,\n",
            "         -3.7231e-03, -1.8311e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.29.self_attn.v_proj.weight': tensor([[-0.0303,  0.0258,  0.0040,  ..., -0.0096,  0.0083, -0.0276],\n",
            "        [-0.0635, -0.0137, -0.0435,  ...,  0.0476,  0.0199, -0.0145],\n",
            "        [-0.0237,  0.0454, -0.0172,  ..., -0.0215, -0.0140,  0.0086],\n",
            "        ...,\n",
            "        [-0.0063,  0.0052,  0.0074,  ..., -0.0208, -0.0214, -0.0083],\n",
            "        [ 0.0032, -0.0036, -0.0125,  ..., -0.0041,  0.0086, -0.0022],\n",
            "        [-0.0034, -0.0022,  0.0049,  ...,  0.0145,  0.0046, -0.0055]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.3.block_sparse_moe.gate.weight': tensor([[ 0.0049,  0.0098,  0.0082,  ...,  0.0080, -0.0125,  0.0123],\n",
            "        [-0.0255,  0.0023,  0.0019,  ...,  0.0052, -0.0106, -0.0327],\n",
            "        [ 0.0124,  0.0166,  0.0227,  ...,  0.0228, -0.0101,  0.0139],\n",
            "        ...,\n",
            "        [ 0.0216, -0.0097, -0.0233,  ..., -0.0003,  0.0332,  0.0233],\n",
            "        [-0.0266, -0.0006,  0.0109,  ..., -0.0112, -0.0219, -0.0072],\n",
            "        [ 0.0149, -0.0049, -0.0156,  ...,  0.0007, -0.0187,  0.0019]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.3.input_layernorm.weight': tensor([0.6562, 0.4629, 0.4473,  ..., 0.6914, 0.5391, 0.5195], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.3.post_attention_layernorm.weight': tensor([0.6875, 0.6953, 0.6953,  ..., 0.6875, 0.6836, 0.6875], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.3.self_attn.k_proj.weight': tensor([[-0.0016, -0.0024,  0.0005,  ..., -0.0067,  0.0155,  0.0066],\n",
            "        [ 0.0092, -0.0198,  0.0144,  ..., -0.0075,  0.0108, -0.0295],\n",
            "        [ 0.0167, -0.0111,  0.0140,  ..., -0.0022,  0.0106, -0.0060],\n",
            "        ...,\n",
            "        [-0.0723,  0.0082,  0.0091,  ...,  0.0217,  0.0153,  0.0114],\n",
            "        [ 0.0117,  0.0038,  0.0291,  ...,  0.0014,  0.0107, -0.0225],\n",
            "        [-0.0085,  0.0033, -0.0056,  ...,  0.0028, -0.0069,  0.0052]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.3.self_attn.o_proj.weight': tensor([[-0.0219,  0.0012,  0.0068,  ...,  0.0021, -0.0054, -0.0002],\n",
            "        [ 0.0192,  0.0255,  0.0023,  ..., -0.0028, -0.0106,  0.0123],\n",
            "        [ 0.0118,  0.0052,  0.0019,  ...,  0.0168,  0.0167, -0.0049],\n",
            "        ...,\n",
            "        [ 0.0069, -0.0164, -0.0022,  ...,  0.0157,  0.0070, -0.0070],\n",
            "        [ 0.0245,  0.0193, -0.0033,  ..., -0.0015,  0.0068, -0.0045],\n",
            "        [ 0.0327,  0.0244, -0.0040,  ...,  0.0060, -0.0117,  0.0103]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.3.self_attn.q_proj.weight': tensor([[ 4.2343e-04,  3.4904e-04, -3.4485e-03,  ...,  1.1536e-02,\n",
            "          1.0376e-03, -5.1880e-03],\n",
            "        [-9.9487e-03,  1.2894e-03, -7.6294e-05,  ...,  3.9673e-03,\n",
            "         -7.5684e-03,  5.1880e-03],\n",
            "        [ 1.8616e-03, -3.4180e-03,  5.9204e-03,  ...,  1.0254e-02,\n",
            "          3.0365e-03, -4.1504e-03],\n",
            "        ...,\n",
            "        [-3.6316e-03, -4.9133e-03,  9.2773e-03,  ...,  4.3640e-03,\n",
            "         -7.5378e-03,  8.8215e-05],\n",
            "        [ 2.0874e-02, -9.2163e-03,  1.0315e-02,  ..., -2.1057e-03,\n",
            "         -2.1729e-02,  1.4648e-03],\n",
            "        [ 1.9165e-02, -1.3306e-02,  2.0752e-02,  ..., -6.2561e-03,\n",
            "         -2.0294e-03, -1.3123e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.3.self_attn.v_proj.weight': tensor([[-0.0181,  0.0181, -0.0126,  ..., -0.0025,  0.0471,  0.0175],\n",
            "        [-0.0152,  0.0031, -0.0009,  ..., -0.0117, -0.0121, -0.0084],\n",
            "        [ 0.0059, -0.0090,  0.0029,  ...,  0.0021,  0.0195, -0.0079],\n",
            "        ...,\n",
            "        [ 0.0066, -0.0008,  0.0073,  ..., -0.0058, -0.0022,  0.0041],\n",
            "        [-0.0076, -0.0052,  0.0153,  ...,  0.0099, -0.0011,  0.0003],\n",
            "        [ 0.0005,  0.0069,  0.0149,  ..., -0.0068,  0.0151,  0.0013]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.30.block_sparse_moe.gate.weight': tensor([[ 2.0142e-03, -1.4191e-03,  2.5940e-04,  ...,  3.6926e-03,\n",
            "         -2.2888e-03, -3.6011e-03],\n",
            "        [-2.5482e-03,  8.9111e-03, -1.1902e-03,  ...,  3.0365e-03,\n",
            "          1.8692e-03, -2.3346e-03],\n",
            "        [-2.7618e-03, -1.7624e-03, -7.2479e-04,  ...,  3.2196e-03,\n",
            "         -5.0735e-04,  5.6763e-03],\n",
            "        ...,\n",
            "        [ 5.6152e-03, -8.9645e-04, -1.1444e-03,  ..., -5.4626e-03,\n",
            "          1.6880e-04,  4.7302e-03],\n",
            "        [ 1.0193e-02,  1.2131e-03,  2.5940e-03,  ..., -6.3782e-03,\n",
            "          1.9836e-03, -1.8616e-03],\n",
            "        [-4.3945e-03,  1.7319e-03, -1.8616e-03,  ..., -6.3782e-03,\n",
            "         -9.4175e-06, -4.4861e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.30.input_layernorm.weight': tensor([2.6719, 2.6250, 2.5781,  ..., 2.2812, 2.3438, 2.5156], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.30.post_attention_layernorm.weight': tensor([3.6875, 3.8750, 3.7656,  ..., 3.6875, 3.6562, 3.6875], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.30.self_attn.k_proj.weight': tensor([[ 1.0223e-03, -1.6708e-03, -5.8746e-04,  ...,  1.2016e-04,\n",
            "          6.1035e-03,  1.3885e-03],\n",
            "        [-2.3956e-03, -9.8419e-04,  5.8365e-04,  ...,  2.0447e-03,\n",
            "          1.9455e-04,  2.9755e-03],\n",
            "        [ 3.8719e-04,  4.0531e-05, -2.7771e-03,  ..., -4.4556e-03,\n",
            "         -1.8005e-03,  2.1362e-03],\n",
            "        ...,\n",
            "        [ 4.2534e-04,  5.7068e-03, -4.4556e-03,  ...,  4.5776e-03,\n",
            "          1.0681e-02,  4.6158e-04],\n",
            "        [ 7.4768e-03,  3.5248e-03,  1.7319e-03,  ..., -5.5542e-03,\n",
            "          2.7618e-03,  2.1515e-03],\n",
            "        [-5.0049e-03, -1.0559e-02,  5.3711e-03,  ...,  9.5749e-04,\n",
            "         -4.8637e-04, -3.5858e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.30.self_attn.o_proj.weight': tensor([[-3.5400e-03,  1.6937e-03, -4.4823e-04,  ...,  9.5215e-03,\n",
            "          6.5002e-03,  3.7766e-04],\n",
            "        [-8.3008e-03, -1.3000e-02, -4.5395e-04,  ...,  1.3306e-02,\n",
            "          6.0425e-03,  1.7166e-03],\n",
            "        [-9.5215e-03,  1.8433e-02,  1.2573e-02,  ..., -2.1484e-02,\n",
            "          5.7678e-03, -7.1411e-03],\n",
            "        ...,\n",
            "        [-1.0071e-02,  9.5215e-03,  2.2217e-02,  ..., -5.6744e-05,\n",
            "         -5.7068e-03, -1.0193e-02],\n",
            "        [-1.4648e-03, -1.7090e-02, -3.8605e-03,  ..., -9.6436e-03,\n",
            "          1.0437e-02, -7.5378e-03],\n",
            "        [-6.2561e-03,  4.1389e-04,  8.6670e-03,  ..., -3.4424e-02,\n",
            "         -2.6367e-02, -1.2741e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.30.self_attn.q_proj.weight': tensor([[-0.0045, -0.0042,  0.0016,  ..., -0.0047, -0.0134,  0.0086],\n",
            "        [ 0.0027,  0.0077,  0.0043,  ...,  0.0075, -0.0034,  0.0016],\n",
            "        [-0.0006,  0.0014, -0.0070,  ...,  0.0034,  0.0027, -0.0024],\n",
            "        ...,\n",
            "        [-0.0088,  0.0155,  0.0021,  ..., -0.0208, -0.0137, -0.0077],\n",
            "        [-0.0066, -0.0098,  0.0012,  ..., -0.0007, -0.0115, -0.0184],\n",
            "        [ 0.0007, -0.0125, -0.0068,  ...,  0.0065,  0.0064,  0.0089]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.30.self_attn.v_proj.weight': tensor([[-0.0062, -0.0026, -0.0013,  ...,  0.0074, -0.0198,  0.0042],\n",
            "        [-0.0173,  0.0052, -0.0236,  ...,  0.0123, -0.0079,  0.0215],\n",
            "        [-0.0417,  0.0101,  0.0010,  ..., -0.0186, -0.0038, -0.0153],\n",
            "        ...,\n",
            "        [ 0.0018,  0.0182, -0.0405,  ..., -0.0040, -0.0065, -0.0312],\n",
            "        [ 0.0282, -0.0047, -0.0181,  ...,  0.0038,  0.0366, -0.0410],\n",
            "        [-0.0047,  0.0045, -0.0221,  ..., -0.0315, -0.0100, -0.0195]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.31.block_sparse_moe.gate.weight': tensor([[-5.1880e-03,  1.2875e-04, -5.5542e-03,  ...,  6.0730e-03,\n",
            "          4.6082e-03, -2.2030e-04],\n",
            "        [-3.0136e-04, -2.0266e-06, -2.4719e-03,  ...,  1.8845e-03,\n",
            "          3.9978e-03,  1.0452e-03],\n",
            "        [ 2.6245e-03,  3.7994e-03,  6.4697e-03,  ..., -1.5259e-03,\n",
            "          4.6082e-03, -4.2419e-03],\n",
            "        ...,\n",
            "        [ 4.6692e-03, -1.1520e-03, -3.8757e-03,  ...,  3.9368e-03,\n",
            "         -4.1504e-03,  3.4637e-03],\n",
            "        [-2.8687e-03,  2.7008e-03,  8.1177e-03,  ..., -5.2795e-03,\n",
            "         -3.6163e-03, -2.3804e-03],\n",
            "        [-2.3956e-03,  3.4485e-03,  4.6997e-03,  ..., -4.0894e-03,\n",
            "         -4.4556e-03,  7.1716e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.31.input_layernorm.weight': tensor([2.5000, 2.7188, 2.5312,  ..., 2.6406, 2.5156, 2.6094], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.31.post_attention_layernorm.weight': tensor([3.7656, 3.9688, 3.8750,  ..., 3.6719, 3.6875, 3.7344], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.31.self_attn.k_proj.weight': tensor([[ 8.4839e-03, -6.1646e-03,  1.3504e-03,  ...,  1.6556e-03,\n",
            "         -4.6997e-03,  2.5787e-03],\n",
            "        [-1.8406e-04, -5.0964e-03,  2.3651e-03,  ...,  9.6321e-05,\n",
            "          4.7607e-03,  1.8082e-03],\n",
            "        [-3.9062e-03, -4.3030e-03,  2.1667e-03,  ...,  3.3875e-03,\n",
            "          3.7842e-03, -2.4719e-03],\n",
            "        ...,\n",
            "        [ 5.9605e-05,  3.3722e-03,  4.6692e-03,  ..., -6.0730e-03,\n",
            "          6.5002e-03,  1.9684e-03],\n",
            "        [ 4.6387e-03, -4.8523e-03, -3.6163e-03,  ..., -7.2021e-03,\n",
            "         -2.9907e-03,  3.1738e-03],\n",
            "        [-6.9580e-03,  4.7302e-03, -3.7842e-03,  ...,  3.1891e-03,\n",
            "          4.4861e-03,  6.1798e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.31.self_attn.o_proj.weight': tensor([[ 0.0027, -0.0012,  0.0022,  ...,  0.0063,  0.0334, -0.0076],\n",
            "        [ 0.0034, -0.0004,  0.0118,  ..., -0.0205, -0.0293,  0.0003],\n",
            "        [ 0.0006, -0.0172,  0.0031,  ..., -0.0087, -0.0184, -0.0099],\n",
            "        ...,\n",
            "        [ 0.0110,  0.0067,  0.0054,  ...,  0.0080, -0.0074,  0.0036],\n",
            "        [ 0.0175,  0.0023, -0.0032,  ...,  0.0090, -0.0106, -0.0029],\n",
            "        [-0.0129, -0.0065,  0.0046,  ...,  0.0171,  0.0027, -0.0220]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.31.self_attn.q_proj.weight': tensor([[ 8.0466e-06,  5.6076e-04, -2.6093e-03,  ..., -5.0964e-03,\n",
            "          2.0752e-03, -7.5378e-03],\n",
            "        [ 6.9809e-04, -1.9989e-03, -2.2583e-03,  ...,  8.2397e-04,\n",
            "          1.9264e-04, -1.1902e-02],\n",
            "        [ 4.7913e-03, -1.1902e-02, -8.8501e-04,  ...,  4.3640e-03,\n",
            "         -3.8147e-03,  1.5831e-04],\n",
            "        ...,\n",
            "        [-5.4016e-03, -6.8054e-03,  7.5989e-03,  ..., -6.4697e-03,\n",
            "         -1.8120e-04, -1.0147e-03],\n",
            "        [ 8.4839e-03, -1.9379e-03, -1.4114e-03,  ..., -5.1575e-03,\n",
            "         -5.8899e-03, -7.2632e-03],\n",
            "        [-8.4229e-03,  4.5471e-03,  4.8218e-03,  ...,  8.0566e-03,\n",
            "         -1.9989e-03,  4.5471e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.31.self_attn.v_proj.weight': tensor([[ 0.0237, -0.0125, -0.0226,  ..., -0.0135, -0.0043, -0.0034],\n",
            "        [ 0.0091,  0.0022, -0.0155,  ..., -0.0133, -0.0033, -0.0125],\n",
            "        [ 0.0381, -0.0139,  0.0124,  ...,  0.0177, -0.0144,  0.0050],\n",
            "        ...,\n",
            "        [ 0.0239, -0.0096, -0.0221,  ..., -0.0123, -0.0004, -0.0026],\n",
            "        [ 0.0251,  0.0248,  0.0052,  ..., -0.0148, -0.0037,  0.0231],\n",
            "        [-0.0371, -0.0056, -0.0124,  ...,  0.0237,  0.0308, -0.0315]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.4.block_sparse_moe.gate.weight': tensor([[-5.4626e-03,  1.4038e-02, -1.5335e-03,  ..., -1.2329e-02,\n",
            "          1.0834e-03, -5.1880e-03],\n",
            "        [-1.2939e-02, -6.0272e-04,  6.1417e-04,  ..., -5.6076e-04,\n",
            "          1.4465e-02, -8.3542e-04],\n",
            "        [ 1.3428e-02, -1.1108e-02,  1.0223e-03,  ...,  4.1504e-03,\n",
            "         -5.0354e-04, -1.0925e-02],\n",
            "        ...,\n",
            "        [-2.2949e-02, -2.0905e-03,  9.7046e-03,  ...,  1.0986e-02,\n",
            "          3.8862e-05, -2.6093e-03],\n",
            "        [ 2.3438e-02,  5.8899e-03,  1.3550e-02,  ..., -7.0190e-03,\n",
            "         -0.0000e+00, -6.8359e-03],\n",
            "        [-5.9509e-03, -1.3123e-03, -5.7983e-03,  ..., -1.2695e-02,\n",
            "         -1.8311e-02,  4.8523e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.4.input_layernorm.weight': tensor([0.9336, 0.7539, 0.7031,  ..., 0.9766, 0.8164, 0.6992], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.4.post_attention_layernorm.weight': tensor([0.8594, 0.8789, 0.8789,  ..., 0.8711, 0.8672, 0.8750], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.4.self_attn.k_proj.weight': tensor([[-2.1667e-03,  1.4587e-02, -3.5858e-03,  ..., -6.5613e-03,\n",
            "         -3.0212e-03, -2.2278e-03],\n",
            "        [ 9.2773e-03, -1.2817e-02,  5.5237e-03,  ..., -3.3722e-03,\n",
            "          5.4321e-03, -1.2268e-02],\n",
            "        [-1.7319e-03,  1.3428e-03,  4.1504e-03,  ...,  3.2997e-04,\n",
            "          6.1646e-03,  1.3916e-02],\n",
            "        ...,\n",
            "        [-6.6528e-03,  7.3624e-04,  2.3041e-03,  ...,  6.7749e-03,\n",
            "         -1.6928e-05,  9.0027e-04],\n",
            "        [ 1.0986e-02, -1.7822e-02,  3.5156e-02,  ...,  7.6599e-03,\n",
            "         -1.8677e-02, -3.0151e-02],\n",
            "        [-1.6357e-02, -1.3916e-02, -1.7212e-02,  ..., -2.0630e-02,\n",
            "         -1.9165e-02,  2.7466e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.4.self_attn.o_proj.weight': tensor([[ 0.0068, -0.0007,  0.0145,  ...,  0.0175, -0.0014, -0.0016],\n",
            "        [-0.0068, -0.0139, -0.0160,  ...,  0.0051, -0.0087, -0.0024],\n",
            "        [-0.0126, -0.0060,  0.0114,  ...,  0.0009, -0.0014,  0.0072],\n",
            "        ...,\n",
            "        [ 0.0008, -0.0073, -0.0081,  ..., -0.0100,  0.0028, -0.0054],\n",
            "        [-0.0009,  0.0079,  0.0021,  ..., -0.0021,  0.0023, -0.0143],\n",
            "        [ 0.0197,  0.0026, -0.0018,  ..., -0.0090,  0.0067, -0.0065]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.4.self_attn.q_proj.weight': tensor([[ 0.0004, -0.0046,  0.0046,  ...,  0.0054, -0.0216, -0.0034],\n",
            "        [-0.0134,  0.0056,  0.0121,  ..., -0.0153,  0.0121, -0.0258],\n",
            "        [-0.0015, -0.0034,  0.0177,  ...,  0.0081,  0.0016, -0.0113],\n",
            "        ...,\n",
            "        [ 0.0098, -0.0018,  0.0024,  ..., -0.0025,  0.0056, -0.0024],\n",
            "        [ 0.0054, -0.0136,  0.0085,  ..., -0.0134, -0.0095,  0.0051],\n",
            "        [ 0.0099, -0.0113, -0.0074,  ...,  0.0201,  0.0021, -0.0146]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.4.self_attn.v_proj.weight': tensor([[-5.9509e-03, -4.9744e-03, -7.3853e-03,  ...,  7.0496e-03,\n",
            "          4.3945e-03, -4.3335e-03],\n",
            "        [ 5.7983e-03,  1.5015e-02, -7.2632e-03,  ...,  1.6861e-03,\n",
            "         -4.2725e-03, -1.1444e-03],\n",
            "        [ 1.0986e-02, -6.4373e-05, -8.6670e-03,  ..., -3.8757e-03,\n",
            "         -1.7090e-02,  2.4109e-03],\n",
            "        ...,\n",
            "        [-4.2725e-03, -1.7334e-02,  9.5825e-03,  ...,  7.9956e-03,\n",
            "          9.8877e-03, -3.4027e-03],\n",
            "        [ 3.9673e-03,  1.3924e-04, -9.9659e-05,  ..., -6.9580e-03,\n",
            "          2.1076e-04, -9.8877e-03],\n",
            "        [ 1.2436e-03,  1.1475e-02, -1.6479e-02,  ..., -1.0986e-03,\n",
            "          1.0071e-02, -2.2736e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.5.block_sparse_moe.gate.weight': tensor([[ 1.7319e-03,  1.4832e-02,  4.3030e-03,  ...,  1.9165e-02,\n",
            "          5.8594e-03, -2.4414e-03],\n",
            "        [ 2.2217e-02, -4.1809e-03,  5.5237e-03,  ..., -5.9814e-03,\n",
            "          1.6235e-02, -9.2163e-03],\n",
            "        [-1.0559e-02, -1.1536e-02,  3.7537e-03,  ...,  8.8501e-03,\n",
            "         -4.5471e-03,  5.3101e-03],\n",
            "        ...,\n",
            "        [ 6.3171e-03, -1.3733e-02,  7.5340e-05,  ..., -1.0803e-02,\n",
            "          4.3030e-03, -1.0223e-03],\n",
            "        [-1.4526e-02,  6.5613e-03, -1.0681e-02,  ...,  3.2959e-03,\n",
            "         -1.6479e-02, -5.1270e-03],\n",
            "        [-3.4790e-03,  7.1411e-03, -8.4229e-03,  ...,  7.2479e-04,\n",
            "          4.7913e-03, -2.1667e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.5.input_layernorm.weight': tensor([1.2656, 0.9297, 0.9141,  ..., 1.1641, 1.0469, 0.9727], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.5.post_attention_layernorm.weight': tensor([1.0234, 1.0703, 1.0625,  ..., 1.0391, 1.0391, 1.0391], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.5.self_attn.k_proj.weight': tensor([[-0.0095, -0.0092,  0.0110,  ...,  0.0003, -0.0011, -0.0010],\n",
            "        [-0.0061,  0.0126,  0.0014,  ..., -0.0058, -0.0094,  0.0017],\n",
            "        [-0.0064,  0.0130,  0.0275,  ...,  0.0104, -0.0023, -0.0098],\n",
            "        ...,\n",
            "        [ 0.0154, -0.0094,  0.0270,  ..., -0.0084, -0.0049, -0.0125],\n",
            "        [ 0.0055,  0.0065, -0.0205,  ...,  0.0052,  0.0112,  0.0018],\n",
            "        [ 0.0027,  0.0209, -0.0176,  ...,  0.0090, -0.0168,  0.0078]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.5.self_attn.o_proj.weight': tensor([[ 0.0120, -0.0005,  0.0038,  ...,  0.0027,  0.0028,  0.0210],\n",
            "        [-0.0005, -0.0008,  0.0034,  ...,  0.0095, -0.0098,  0.0075],\n",
            "        [-0.0113, -0.0107,  0.0126,  ..., -0.0098,  0.0044,  0.0079],\n",
            "        ...,\n",
            "        [-0.0071, -0.0031, -0.0110,  ..., -0.0072, -0.0029, -0.0231],\n",
            "        [ 0.0034,  0.0063, -0.0086,  ...,  0.0010,  0.0113,  0.0192],\n",
            "        [ 0.0121,  0.0139,  0.0208,  ..., -0.0039,  0.0013,  0.0156]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.5.self_attn.q_proj.weight': tensor([[ 1.6968e-02, -1.3489e-02, -2.9297e-03,  ..., -5.8899e-03,\n",
            "         -1.0071e-03, -2.4719e-03],\n",
            "        [ 1.7047e-05, -3.5706e-03, -2.4567e-03,  ..., -7.0190e-03,\n",
            "         -2.8534e-03, -1.4343e-03],\n",
            "        [ 4.7607e-03, -1.7166e-03,  1.0071e-02,  ..., -3.3569e-03,\n",
            "          2.1240e-02, -6.6757e-05],\n",
            "        ...,\n",
            "        [ 1.8433e-02,  1.0437e-02, -1.1414e-02,  ..., -2.3926e-02,\n",
            "          4.5166e-03, -4.2114e-03],\n",
            "        [ 5.9891e-04,  2.5024e-02,  1.1047e-02,  ...,  1.2207e-02,\n",
            "         -5.9204e-03,  8.8501e-03],\n",
            "        [-1.5625e-02,  1.1902e-02, -9.1553e-03,  ...,  1.7700e-02,\n",
            "         -9.7656e-04, -1.8066e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.5.self_attn.v_proj.weight': tensor([[-0.0017,  0.0093, -0.0065,  ...,  0.0082, -0.0188,  0.0006],\n",
            "        [ 0.0092, -0.0104, -0.0053,  ..., -0.0025,  0.0029,  0.0006],\n",
            "        [ 0.0128,  0.0142, -0.0004,  ...,  0.0117,  0.0088, -0.0192],\n",
            "        ...,\n",
            "        [ 0.0031, -0.0177,  0.0065,  ...,  0.0037, -0.0115,  0.0052],\n",
            "        [-0.0081,  0.0143,  0.0129,  ..., -0.0006,  0.0098,  0.0149],\n",
            "        [ 0.0114, -0.0069,  0.0070,  ..., -0.0075,  0.0079, -0.0018]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.6.block_sparse_moe.gate.weight': tensor([[-0.0027, -0.0064,  0.0021,  ...,  0.0147, -0.0075, -0.0038],\n",
            "        [ 0.0155, -0.0025,  0.0102,  ..., -0.0147,  0.0081, -0.0082],\n",
            "        [-0.0160,  0.0022, -0.0081,  ..., -0.0193,  0.0104, -0.0046],\n",
            "        ...,\n",
            "        [-0.0049,  0.0005, -0.0097,  ..., -0.0322,  0.0001, -0.0038],\n",
            "        [ 0.0002,  0.0081, -0.0041,  ...,  0.0171,  0.0005,  0.0019],\n",
            "        [ 0.0219, -0.0045,  0.0062,  ...,  0.0309, -0.0087,  0.0063]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.6.input_layernorm.weight': tensor([1.0781, 0.9492, 0.8906,  ..., 1.0000, 1.0234, 0.9219], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.6.post_attention_layernorm.weight': tensor([1.1172, 1.1875, 1.1953,  ..., 1.0938, 1.1406, 1.1406], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.6.self_attn.k_proj.weight': tensor([[-0.0022, -0.0155, -0.0165,  ..., -0.0053, -0.0149,  0.0086],\n",
            "        [ 0.0142,  0.0118, -0.0017,  ...,  0.0061,  0.0010,  0.0004],\n",
            "        [-0.0052,  0.0101,  0.0021,  ...,  0.0103, -0.0087,  0.0074],\n",
            "        ...,\n",
            "        [-0.0037,  0.0116, -0.0210,  ...,  0.0171,  0.0194, -0.0079],\n",
            "        [ 0.0043, -0.0266, -0.0153,  ..., -0.0012, -0.0026,  0.0103],\n",
            "        [ 0.0040, -0.0153, -0.0103,  ...,  0.0386,  0.0016, -0.0265]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.6.self_attn.o_proj.weight': tensor([[ 0.0009,  0.0147, -0.0056,  ...,  0.0029, -0.0186,  0.0063],\n",
            "        [ 0.0034, -0.0078, -0.0078,  ..., -0.0042,  0.0018, -0.0054],\n",
            "        [-0.0041, -0.0013, -0.0087,  ...,  0.0061,  0.0027, -0.0071],\n",
            "        ...,\n",
            "        [-0.0061,  0.0079, -0.0253,  ...,  0.0028,  0.0140,  0.0041],\n",
            "        [-0.0081, -0.0034, -0.0127,  ..., -0.0354, -0.0103, -0.0098],\n",
            "        [ 0.0032,  0.0161,  0.0090,  ..., -0.0013, -0.0020, -0.0076]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.6.self_attn.q_proj.weight': tensor([[-0.0041,  0.0151,  0.0250,  ..., -0.0215,  0.0046, -0.0311],\n",
            "        [-0.0305,  0.0073,  0.0153,  ...,  0.0063, -0.0009, -0.0050],\n",
            "        [ 0.0181, -0.0317, -0.0054,  ..., -0.0049, -0.0052,  0.0067],\n",
            "        ...,\n",
            "        [ 0.0164, -0.0117,  0.0128,  ...,  0.0179, -0.0058, -0.0042],\n",
            "        [ 0.0011,  0.0053,  0.0033,  ...,  0.0297, -0.0150, -0.0042],\n",
            "        [-0.0047,  0.0149, -0.0147,  ..., -0.0277, -0.0005,  0.0203]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.6.self_attn.v_proj.weight': tensor([[ 0.0045, -0.0130,  0.0033,  ...,  0.0064,  0.0137,  0.0039],\n",
            "        [ 0.0141,  0.0004, -0.0012,  ...,  0.0031,  0.0021, -0.0029],\n",
            "        [ 0.0010,  0.0091,  0.0043,  ...,  0.0049,  0.0088,  0.0054],\n",
            "        ...,\n",
            "        [ 0.0061, -0.0007,  0.0209,  ...,  0.0098, -0.0192, -0.0033],\n",
            "        [-0.0129,  0.0188, -0.0073,  ...,  0.0026, -0.0077, -0.0048],\n",
            "        [ 0.0103, -0.0161, -0.0093,  ...,  0.0014, -0.0132, -0.0106]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.7.block_sparse_moe.gate.weight': tensor([[ 0.0097,  0.0002, -0.0156,  ..., -0.0277, -0.0022, -0.0134],\n",
            "        [-0.0052,  0.0070,  0.0136,  ...,  0.0018,  0.0028,  0.0047],\n",
            "        [ 0.0154, -0.0112, -0.0089,  ..., -0.0015, -0.0074, -0.0160],\n",
            "        ...,\n",
            "        [-0.0001,  0.0023,  0.0035,  ...,  0.0112, -0.0070,  0.0187],\n",
            "        [-0.0032, -0.0035, -0.0078,  ...,  0.0219, -0.0087,  0.0002],\n",
            "        [-0.0172, -0.0045, -0.0027,  ...,  0.0173,  0.0278, -0.0103]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.7.input_layernorm.weight': tensor([1.2031, 1.1562, 1.1719,  ..., 1.1406, 1.2422, 1.1250], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.7.post_attention_layernorm.weight': tensor([1.2656, 1.4219, 1.4141,  ..., 1.2422, 1.3125, 1.3125], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.7.self_attn.k_proj.weight': tensor([[-0.0112, -0.0030,  0.0022,  ..., -0.0120,  0.0027, -0.0046],\n",
            "        [-0.0127, -0.0099, -0.0112,  ...,  0.0090, -0.0021, -0.0013],\n",
            "        [ 0.0016, -0.0065, -0.0045,  ...,  0.0034,  0.0004,  0.0090],\n",
            "        ...,\n",
            "        [ 0.0061, -0.0099,  0.0093,  ...,  0.0017,  0.0293, -0.0102],\n",
            "        [ 0.0371,  0.0206,  0.0064,  ..., -0.0161, -0.0264,  0.0149],\n",
            "        [-0.0237, -0.0048,  0.0078,  ...,  0.0356,  0.0210,  0.0220]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.7.self_attn.o_proj.weight': tensor([[-7.1411e-03,  4.9133e-03, -2.5177e-03,  ..., -1.0071e-03,\n",
            "          1.7700e-02, -3.0975e-03],\n",
            "        [ 1.4954e-02,  5.7068e-03,  4.7445e-05,  ..., -1.8555e-02,\n",
            "          1.6556e-03,  1.0498e-02],\n",
            "        [ 4.1580e-04,  4.5776e-03,  5.8899e-03,  ..., -8.3618e-03,\n",
            "          1.5442e-02,  6.7139e-03],\n",
            "        ...,\n",
            "        [-4.8828e-03,  3.9673e-03,  2.7313e-03,  ...,  1.3000e-02,\n",
            "         -1.0681e-02, -6.6833e-03],\n",
            "        [-5.2795e-03, -1.6602e-02,  1.8387e-03,  ...,  1.7700e-02,\n",
            "         -1.4038e-02, -6.9275e-03],\n",
            "        [-5.5542e-03,  9.1553e-03, -2.8839e-03,  ...,  3.2806e-03,\n",
            "         -1.3123e-02, -1.0071e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.7.self_attn.q_proj.weight': tensor([[ 0.0055,  0.0004, -0.0135,  ...,  0.0136, -0.0059, -0.0010],\n",
            "        [-0.0011, -0.0008,  0.0146,  ..., -0.0083, -0.0045, -0.0123],\n",
            "        [-0.0009,  0.0137,  0.0045,  ...,  0.0092,  0.0033, -0.0152],\n",
            "        ...,\n",
            "        [ 0.0244, -0.0068, -0.0099,  ..., -0.0035,  0.0078, -0.0201],\n",
            "        [-0.0530,  0.0048,  0.0137,  ...,  0.0040, -0.0159,  0.0056],\n",
            "        [-0.0042, -0.0481, -0.0262,  ..., -0.0033,  0.0283, -0.0289]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.7.self_attn.v_proj.weight': tensor([[-0.0062,  0.0125, -0.0030,  ..., -0.0007, -0.0043, -0.0049],\n",
            "        [ 0.0045,  0.0030,  0.0003,  ...,  0.0041, -0.0051,  0.0120],\n",
            "        [-0.0107, -0.0042,  0.0048,  ...,  0.0030, -0.0017, -0.0109],\n",
            "        ...,\n",
            "        [-0.0050, -0.0183,  0.0078,  ...,  0.0148,  0.0135,  0.0011],\n",
            "        [ 0.0002,  0.0234,  0.0092,  ..., -0.0030, -0.0046, -0.0131],\n",
            "        [ 0.0084, -0.0018,  0.0074,  ..., -0.0015, -0.0003,  0.0037]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.8.block_sparse_moe.gate.weight': tensor([[-0.0007,  0.0007, -0.0132,  ...,  0.0159, -0.0137, -0.0002],\n",
            "        [-0.0095,  0.0043, -0.0119,  ...,  0.0145,  0.0044, -0.0023],\n",
            "        [-0.0034, -0.0104,  0.0086,  ..., -0.0008, -0.0017,  0.0023],\n",
            "        ...,\n",
            "        [-0.0194, -0.0087, -0.0029,  ..., -0.0101,  0.0085,  0.0025],\n",
            "        [ 0.0244,  0.0063, -0.0050,  ..., -0.0109, -0.0139, -0.0111],\n",
            "        [-0.0031,  0.0134,  0.0118,  ...,  0.0063,  0.0112,  0.0075]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.8.input_layernorm.weight': tensor([1.2344, 1.2422, 1.3984,  ..., 1.1016, 1.3281, 1.1797], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.8.post_attention_layernorm.weight': tensor([1.3203, 1.5703, 1.5547,  ..., 1.2578, 1.3750, 1.4062], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.8.self_attn.k_proj.weight': tensor([[-0.0054,  0.0082,  0.0015,  ..., -0.0151, -0.0021,  0.0039],\n",
            "        [ 0.0049, -0.0018, -0.0050,  ...,  0.0031, -0.0015, -0.0093],\n",
            "        [-0.0014, -0.0067, -0.0007,  ..., -0.0072, -0.0003,  0.0073],\n",
            "        ...,\n",
            "        [ 0.0084,  0.0190, -0.0093,  ..., -0.0045,  0.0019,  0.0032],\n",
            "        [ 0.0045,  0.0217,  0.0022,  ..., -0.0002, -0.0076, -0.0111],\n",
            "        [-0.0042,  0.0092, -0.0058,  ..., -0.0087,  0.0111,  0.0106]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.8.self_attn.o_proj.weight': tensor([[ 7.9346e-04, -1.2054e-03, -8.3008e-03,  ...,  7.8735e-03,\n",
            "         -1.6937e-03,  6.5308e-03],\n",
            "        [-1.3123e-02, -2.5482e-03, -3.3569e-03,  ...,  2.9144e-03,\n",
            "         -1.4404e-02,  4.7684e-04],\n",
            "        [-2.7618e-03,  1.3962e-03, -7.2327e-03,  ..., -6.7139e-03,\n",
            "         -7.1411e-03,  4.0588e-03],\n",
            "        ...,\n",
            "        [-8.4229e-03, -3.4332e-03, -1.1841e-02,  ...,  6.6223e-03,\n",
            "         -1.5182e-03,  1.0193e-02],\n",
            "        [-1.0071e-02, -7.5073e-03,  6.3171e-03,  ..., -1.8768e-03,\n",
            "          1.8921e-03, -1.8555e-02],\n",
            "        [-1.3046e-03, -8.6670e-03,  2.4719e-03,  ...,  2.2650e-05,\n",
            "          2.5940e-03,  8.8882e-04]], device='cuda:0', dtype=torch.float16), 'model.layers.8.self_attn.q_proj.weight': tensor([[-4.8218e-03, -3.6316e-03, -4.0283e-03,  ...,  2.1667e-03,\n",
            "          2.2583e-03, -5.7983e-03],\n",
            "        [-4.8218e-03, -5.4169e-04,  6.2256e-03,  ...,  9.5825e-03,\n",
            "         -2.9755e-04, -1.8463e-03],\n",
            "        [-1.6689e-04, -1.2817e-03, -1.4343e-03,  ...,  4.7913e-03,\n",
            "         -7.3242e-03, -6.9885e-03],\n",
            "        ...,\n",
            "        [-1.1230e-02,  6.8970e-03, -1.2817e-02,  ...,  5.5237e-03,\n",
            "         -6.6757e-04,  1.5869e-02],\n",
            "        [-7.9632e-05, -2.9602e-03, -9.5215e-03,  ..., -3.7689e-03,\n",
            "          2.7771e-03, -2.0630e-02],\n",
            "        [ 6.5231e-04,  1.8188e-02,  1.1414e-02,  ...,  7.8125e-03,\n",
            "          1.1047e-02,  6.8665e-03]], device='cuda:0', dtype=torch.float16), 'model.layers.8.self_attn.v_proj.weight': tensor([[-1.0559e-02,  3.0060e-03,  6.3477e-03,  ..., -9.1553e-03,\n",
            "          6.5308e-03, -1.6968e-02],\n",
            "        [-3.2806e-03, -7.9632e-05,  3.0975e-03,  ..., -8.6670e-03,\n",
            "         -1.0681e-02,  7.2098e-04],\n",
            "        [-8.3008e-03, -2.4567e-03, -1.7357e-04,  ..., -6.9809e-04,\n",
            "         -9.8877e-03, -8.3618e-03],\n",
            "        ...,\n",
            "        [ 4.4250e-03, -1.9836e-04,  3.3569e-03,  ...,  3.0518e-03,\n",
            "          8.9722e-03, -1.1353e-02],\n",
            "        [ 2.0981e-04,  4.4861e-03, -3.0060e-03,  ..., -1.5411e-03,\n",
            "          2.1667e-03,  1.5945e-03],\n",
            "        [-1.5793e-03,  1.0986e-02,  5.7373e-03,  ..., -5.8594e-03,\n",
            "          5.9814e-03, -1.7578e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.9.block_sparse_moe.gate.weight': tensor([[ 0.0300, -0.0161,  0.0042,  ..., -0.0095,  0.0046, -0.0127],\n",
            "        [ 0.0030, -0.0047, -0.0071,  ...,  0.0161, -0.0006, -0.0052],\n",
            "        [-0.0042,  0.0077, -0.0093,  ...,  0.0029, -0.0042,  0.0077],\n",
            "        ...,\n",
            "        [-0.0161,  0.0045,  0.0104,  ..., -0.0098, -0.0214, -0.0047],\n",
            "        [-0.0007,  0.0048, -0.0028,  ...,  0.0048, -0.0052,  0.0075],\n",
            "        [ 0.0028,  0.0019,  0.0077,  ..., -0.0172,  0.0053,  0.0096]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.9.input_layernorm.weight': tensor([1.5078, 1.5938, 1.7734,  ..., 1.2812, 1.5703, 1.5391], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.9.post_attention_layernorm.weight': tensor([1.3984, 1.7109, 1.7109,  ..., 1.2812, 1.4141, 1.4844], device='cuda:0',\n",
            "       dtype=torch.float16), 'model.layers.9.self_attn.k_proj.weight': tensor([[ 0.0159,  0.0002,  0.0043,  ...,  0.0088,  0.0011,  0.0077],\n",
            "        [-0.0182,  0.0081, -0.0019,  ..., -0.0138,  0.0093,  0.0012],\n",
            "        [ 0.0060,  0.0114,  0.0031,  ...,  0.0102, -0.0046,  0.0033],\n",
            "        ...,\n",
            "        [-0.0020,  0.0160, -0.0074,  ...,  0.0160,  0.0017, -0.0034],\n",
            "        [-0.0033,  0.0197,  0.0089,  ...,  0.0104,  0.0063,  0.0042],\n",
            "        [-0.0035, -0.0262,  0.0055,  ...,  0.0133, -0.0403, -0.0078]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.9.self_attn.o_proj.weight': tensor([[-0.0068, -0.0017,  0.0059,  ..., -0.0010,  0.0046,  0.0005],\n",
            "        [ 0.0029, -0.0040,  0.0156,  ...,  0.0029, -0.0016,  0.0086],\n",
            "        [ 0.0085,  0.0043, -0.0014,  ...,  0.0145,  0.0071,  0.0103],\n",
            "        ...,\n",
            "        [-0.0172, -0.0033,  0.0046,  ...,  0.0170,  0.0008,  0.0025],\n",
            "        [-0.0137, -0.0005,  0.0002,  ...,  0.0054, -0.0041, -0.0166],\n",
            "        [ 0.0019, -0.0032, -0.0015,  ..., -0.0107, -0.0035, -0.0011]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.layers.9.self_attn.q_proj.weight': tensor([[ 1.8066e-02, -7.5378e-03,  1.9287e-02,  ...,  3.9864e-04,\n",
            "          1.2207e-03,  2.6131e-04],\n",
            "        [ 4.4250e-03, -2.8534e-03,  8.6670e-03,  ..., -7.5073e-03,\n",
            "          5.1880e-03,  1.0010e-02],\n",
            "        [ 3.2806e-03, -1.2390e-02, -9.2316e-04,  ..., -6.7234e-05,\n",
            "         -5.7678e-03, -1.2268e-02],\n",
            "        ...,\n",
            "        [-4.3030e-03, -2.2583e-02, -1.7944e-02,  ..., -3.9673e-04,\n",
            "          3.0884e-02, -1.7578e-02],\n",
            "        [ 2.6855e-02,  6.2988e-02, -8.4229e-03,  ...,  1.5381e-02,\n",
            "          1.0437e-02, -1.7334e-02],\n",
            "        [ 1.6235e-02, -4.6082e-03, -8.8501e-04,  ...,  1.5381e-02,\n",
            "         -1.7456e-02, -1.3977e-02]], device='cuda:0', dtype=torch.float16), 'model.layers.9.self_attn.v_proj.weight': tensor([[-0.0171, -0.0034,  0.0147,  ..., -0.0041, -0.0036,  0.0049],\n",
            "        [ 0.0008,  0.0070, -0.0101,  ..., -0.0085,  0.0055, -0.0011],\n",
            "        [ 0.0070,  0.0055,  0.0026,  ...,  0.0092, -0.0075,  0.0023],\n",
            "        ...,\n",
            "        [-0.0047,  0.0009,  0.0080,  ...,  0.0054,  0.0011, -0.0113],\n",
            "        [ 0.0046, -0.0131,  0.0020,  ..., -0.0002, -0.0108,  0.0070],\n",
            "        [-0.0099,  0.0117,  0.0069,  ...,  0.0020,  0.0003,  0.0027]],\n",
            "       device='cuda:0', dtype=torch.float16), 'model.norm.weight': tensor([5.0312, 5.1562, 5.0625,  ..., 4.9688, 5.2812, 4.9688], device='cuda:0',\n",
            "       dtype=torch.float16)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MixtralBLockSparseTop2MLP is deprecated by MixtralBlockSparseTop2MLP and will be removed in v4.40.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish loading trunk states!\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.297885417938232 GB\n",
            "Peak GPU Memory Usage: 7.428245544433594 GB\n",
            "Peak GPU Memory Usage: 7.756370544433594 GB\n",
            "Peak GPU Memory Usage: 8.084495544433594 GB\n",
            "Peak GPU Memory Usage: 8.412620544433594 GB\n",
            "Peak GPU Memory Usage: 8.740745544433594 GB\n",
            "Peak GPU Memory Usage: 9.068870544433594 GB\n",
            "Peak GPU Memory Usage: 9.396995544433594 GB\n",
            "Peak GPU Memory Usage: 9.725120544433594 GB\n",
            "Peak GPU Memory Usage: 10.053245544433594 GB\n",
            "Peak GPU Memory Usage: 10.381370544433594 GB\n",
            "Peak GPU Memory Usage: 10.709495544433594 GB\n",
            "Peak GPU Memory Usage: 11.037620544433594 GB\n",
            "Peak GPU Memory Usage: 11.365745544433594 GB\n",
            "Peak GPU Memory Usage: 11.693870544433594 GB\n",
            "Peak GPU Memory Usage: 12.021995544433594 GB\n",
            "Peak GPU Memory Usage: 12.350120544433594 GB\n",
            "Peak GPU Memory Usage: 12.678245544433594 GB\n",
            "Peak GPU Memory Usage: 13.006370544433594 GB\n",
            "Peak GPU Memory Usage: 13.334495544433594 GB\n",
            "Peak GPU Memory Usage: 13.662620544433594 GB\n",
            "Peak GPU Memory Usage: 13.990745544433594 GB\n",
            "Peak GPU Memory Usage: 14.318870544433594 GB\n",
            "Peak GPU Memory Usage: 14.646995544433594 GB\n",
            "Peak GPU Memory Usage: 14.975120544433594 GB\n",
            "Peak GPU Memory Usage: 15.303245544433594 GB\n",
            "Peak GPU Memory Usage: 15.631370544433594 GB\n",
            "Peak GPU Memory Usage: 15.959495544433594 GB\n",
            "Peak GPU Memory Usage: 16.287620544433594 GB\n",
            "Peak GPU Memory Usage: 16.615745544433594 GB\n",
            "Peak GPU Memory Usage: 16.943870544433594 GB\n",
            "Peak GPU Memory Usage: 17.271995544433594 GB\n",
            "Peak GPU Memory Usage: 17.600120544433594 GB\n",
            "Peak GPU Memory Usage: 17.928245544433594 GB\n",
            "Peak GPU Memory Usage: 18.256370544433594 GB\n",
            "Peak GPU Memory Usage: 18.584495544433594 GB\n",
            "Peak GPU Memory Usage: 18.912620544433594 GB\n",
            "Peak GPU Memory Usage: 19.240745544433594 GB\n",
            "Peak GPU Memory Usage: 19.568870544433594 GB\n",
            "Peak GPU Memory Usage: 19.896995544433594 GB\n",
            "Peak GPU Memory Usage: 20.225120544433594 GB\n",
            "Peak GPU Memory Usage: 20.553245544433594 GB\n",
            "Peak GPU Memory Usage: 20.881370544433594 GB\n",
            "Peak GPU Memory Usage: 21.209495544433594 GB\n",
            "Peak GPU Memory Usage: 21.537620544433594 GB\n",
            "Peak GPU Memory Usage: 21.865745544433594 GB\n",
            "Peak GPU Memory Usage: 22.193870544433594 GB\n",
            "Peak GPU Memory Usage: 22.521995544433594 GB\n",
            "Peak GPU Memory Usage: 22.850120544433594 GB\n",
            "Peak GPU Memory Usage: 23.178245544433594 GB\n",
            "Peak GPU Memory Usage: 23.506370544433594 GB\n",
            "Peak GPU Memory Usage: 23.834495544433594 GB\n",
            "Peak GPU Memory Usage: 24.162620544433594 GB\n",
            "Peak GPU Memory Usage: 24.490745544433594 GB\n",
            "Peak GPU Memory Usage: 24.818870544433594 GB\n",
            "Peak GPU Memory Usage: 25.146995544433594 GB\n",
            "Peak GPU Memory Usage: 25.475120544433594 GB\n",
            "Peak GPU Memory Usage: 25.803245544433594 GB\n",
            "Peak GPU Memory Usage: 26.131370544433594 GB\n",
            "Peak GPU Memory Usage: 26.240745544433594 GB\n",
            "Created expert cache! ... Cleaning cache\n",
            "Cleaned.\n",
            "Peak GPU Memory Usage: 26.240745544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 26.459495544433594 GB\n",
            "Peak GPU Memory Usage: 26.568870544433594 GB\n",
            "Peak GPU Memory Usage: 26.787620544433594 GB\n",
            "Peak GPU Memory Usage: 27.115745544433594 GB\n",
            "Peak GPU Memory Usage: 27.443870544433594 GB\n",
            "Peak GPU Memory Usage: 27.553245544433594 GB\n",
            "Peak GPU Memory Usage: 27.553245544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   3%|▎         | 1/32 [00:52<27:16, 52.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 27.553245544433594 GB\n",
            "Peak GPU Memory Usage: 27.553245544433594 GB\n",
            "Peak GPU Memory Usage: 27.771995544433594 GB\n",
            "Peak GPU Memory Usage: 27.881370544433594 GB\n",
            "Peak GPU Memory Usage: 27.881370544433594 GB\n",
            "Peak GPU Memory Usage: 27.881370544433594 GB\n",
            "Peak GPU Memory Usage: 27.881370544433594 GB\n",
            "Peak GPU Memory Usage: 27.881370544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   6%|▋         | 2/32 [01:08<15:29, 30.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 28.100120544433594 GB\n",
            "Peak GPU Memory Usage: 28.100120544433594 GB\n",
            "Peak GPU Memory Usage: 28.100120544433594 GB\n",
            "Peak GPU Memory Usage: 28.100120544433594 GB\n",
            "Peak GPU Memory Usage: 28.100120544433594 GB\n",
            "Peak GPU Memory Usage: 28.100120544433594 GB\n",
            "Peak GPU Memory Usage: 28.209495544433594 GB\n",
            "Peak GPU Memory Usage: 28.209495544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   9%|▉         | 3/32 [01:24<11:44, 24.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 28.209495544433594 GB\n",
            "Peak GPU Memory Usage: 28.209495544433594 GB\n",
            "Peak GPU Memory Usage: 28.209495544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  12%|█▎        | 4/32 [02:06<14:29, 31.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.428245544433594 GB\n",
            "Peak GPU Memory Usage: 28.756370544433594 GB\n",
            "Peak GPU Memory Usage: 28.756370544433594 GB\n",
            "Peak GPU Memory Usage: 28.756370544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  16%|█▌        | 5/32 [02:21<11:24, 25.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 28.756370544433594 GB\n",
            "Peak GPU Memory Usage: 28.756370544433594 GB\n",
            "Peak GPU Memory Usage: 29.084495544433594 GB\n",
            "Peak GPU Memory Usage: 29.084495544433594 GB\n",
            "Peak GPU Memory Usage: 29.084495544433594 GB\n",
            "Peak GPU Memory Usage: 29.084495544433594 GB\n",
            "Peak GPU Memory Usage: 29.084495544433594 GB\n",
            "Peak GPU Memory Usage: 29.412620544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  19%|█▉        | 6/32 [02:34<09:08, 21.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 29.412620544433594 GB\n",
            "Peak GPU Memory Usage: 29.412620544433594 GB\n",
            "Peak GPU Memory Usage: 29.412620544433594 GB\n",
            "Peak GPU Memory Usage: 29.412620544433594 GB\n",
            "Peak GPU Memory Usage: 29.740745544433594 GB\n",
            "Peak GPU Memory Usage: 29.740745544433594 GB\n",
            "Peak GPU Memory Usage: 29.740745544433594 GB\n",
            "Peak GPU Memory Usage: 29.740745544433594 GB\n",
            "Peak GPU Memory Usage: 29.740745544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  22%|██▏       | 7/32 [03:16<11:37, 27.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  25%|██▌       | 8/32 [03:32<09:41, 24.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  28%|██▊       | 9/32 [03:48<08:20, 21.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  31%|███▏      | 10/32 [04:18<08:51, 24.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  34%|███▍      | 11/32 [04:45<08:44, 24.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  38%|███▊      | 12/32 [05:24<09:45, 29.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  41%|████      | 13/32 [06:06<10:29, 33.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  44%|████▍     | 14/32 [06:57<11:35, 38.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  47%|████▋     | 15/32 [08:14<14:13, 50.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  50%|█████     | 16/32 [08:29<10:32, 39.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  53%|█████▎    | 17/32 [09:21<10:46, 43.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  56%|█████▋    | 18/32 [09:37<08:12, 35.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  59%|█████▉    | 19/32 [09:51<06:14, 28.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  62%|██████▎   | 20/32 [10:20<05:44, 28.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  66%|██████▌   | 21/32 [10:36<04:35, 25.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  69%|██████▉   | 22/32 [10:51<03:38, 21.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  72%|███████▏  | 23/32 [11:06<02:59, 19.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  75%|███████▌  | 24/32 [11:50<03:37, 27.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  78%|███████▊  | 25/32 [12:04<02:41, 23.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  81%|████████▏ | 26/32 [12:18<02:01, 20.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  84%|████████▍ | 27/32 [12:53<02:03, 24.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  88%|████████▊ | 28/32 [14:07<02:38, 39.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  91%|█████████ | 29/32 [14:23<01:37, 32.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  94%|█████████▍| 30/32 [14:39<00:55, 27.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  97%|█████████▋| 31/32 [15:32<00:35, 35.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n",
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts: 100%|██████████| 32/32 [15:53<00:00, 29.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 30.068870544433594 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except NameError:\n",
        "    pass\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "# state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/Mixtral-8x7B-Instruct-v0.1\"\n",
        "# state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/test_dir\"\n",
        "config = AutoConfig.from_pretrained(model_name, torch_dtype=torch.float16,)\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "print(f\"initial GPU Memory Usage: {peak_memory_usage} GB\")  \n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "offload_per_layer = 6\n",
        "\n",
        "num_experts = config.num_local_experts\n",
        "print(\"number experts:\", num_experts)\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "print(offload_config)\n",
        "\n",
        "model = build_model_without_quant(\n",
        "    device=device,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")\n",
        "peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "print(f\"Peak GPU Memory Usage: {peak_memory_usage} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227,
          "referenced_widgets": [
            "67dc7ff4d0fe4af796308b2c8355d150",
            "b41dad78195344a6a77b570b1783f7a0",
            "494bb8b09d1f4f2abbc111eb5e6da130",
            "243efd24b8994278a406c597039d0fc4",
            "c2ccdf34885d41a0ab3f9b40c775b25a",
            "39efadb9ccb048deab07969016e7bd38",
            "d36446b0a9534f41a9f21b865b0a08f4",
            "6a458bdef43543d0b7b7edc2883e2dc1",
            "7f85bc3a52a540558999a1ff25a813b3",
            "325f9b174633405483cf60cff12d652b",
            "e60bfa41b7454322a908e01bb9caed65",
            "7b7b50c298de414b85cc622ef5660dfb",
            "d1e82a41293d41d6aa8807d9d575e86b",
            "a19471c4ecce428bbebab2a90f535861",
            "8a2c815f41b8483692a79165ebeaf3f6",
            "9782a56e37e24892a78b282295ae5ac0",
            "9be1146cefb9416cb875741640dca611",
            "15bfcbb901574f0583b8af3733f58b7a",
            "c0dba9bd6ee94ca1a0084eabeac0e1ca",
            "83bcda80b79a4e6eaf5b0f27bcbc0ee0",
            "b792acc1fff049fca658f836557c8e6f",
            "dcf2b97ed0b7415fa02f38c8bb0e3009"
          ]
        },
        "id": "_mIpePTMFyRY",
        "outputId": "8a113aee-bcaa-4434-bdfc-2e65c0c58b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number experts: 8\n",
            "dict_keys(['lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.block_sparse_moe.gate.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.W_q', 'model.layers.0.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.k_proj.meta.scale_q', 'model.layers.0.self_attn.k_proj.meta.zero_q', 'model.layers.0.self_attn.o_proj.W_q', 'model.layers.0.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.o_proj.meta.scale_q', 'model.layers.0.self_attn.o_proj.meta.zero_q', 'model.layers.0.self_attn.q_proj.W_q', 'model.layers.0.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.q_proj.meta.scale_q', 'model.layers.0.self_attn.q_proj.meta.zero_q', 'model.layers.0.self_attn.v_proj.W_q', 'model.layers.0.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.0.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.0.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.0.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.0.self_attn.v_proj.meta.scale_q', 'model.layers.0.self_attn.v_proj.meta.zero_q', 'model.layers.1.block_sparse_moe.gate.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.W_q', 'model.layers.1.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.k_proj.meta.scale_q', 'model.layers.1.self_attn.k_proj.meta.zero_q', 'model.layers.1.self_attn.o_proj.W_q', 'model.layers.1.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.o_proj.meta.scale_q', 'model.layers.1.self_attn.o_proj.meta.zero_q', 'model.layers.1.self_attn.q_proj.W_q', 'model.layers.1.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.q_proj.meta.scale_q', 'model.layers.1.self_attn.q_proj.meta.zero_q', 'model.layers.1.self_attn.v_proj.W_q', 'model.layers.1.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.1.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.1.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.1.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.1.self_attn.v_proj.meta.scale_q', 'model.layers.1.self_attn.v_proj.meta.zero_q', 'model.layers.10.block_sparse_moe.gate.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.W_q', 'model.layers.10.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.k_proj.meta.scale_q', 'model.layers.10.self_attn.k_proj.meta.zero_q', 'model.layers.10.self_attn.o_proj.W_q', 'model.layers.10.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.o_proj.meta.scale_q', 'model.layers.10.self_attn.o_proj.meta.zero_q', 'model.layers.10.self_attn.q_proj.W_q', 'model.layers.10.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.q_proj.meta.scale_q', 'model.layers.10.self_attn.q_proj.meta.zero_q', 'model.layers.10.self_attn.v_proj.W_q', 'model.layers.10.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.10.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.10.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.10.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.10.self_attn.v_proj.meta.scale_q', 'model.layers.10.self_attn.v_proj.meta.zero_q', 'model.layers.11.block_sparse_moe.gate.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.W_q', 'model.layers.11.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.k_proj.meta.scale_q', 'model.layers.11.self_attn.k_proj.meta.zero_q', 'model.layers.11.self_attn.o_proj.W_q', 'model.layers.11.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.o_proj.meta.scale_q', 'model.layers.11.self_attn.o_proj.meta.zero_q', 'model.layers.11.self_attn.q_proj.W_q', 'model.layers.11.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.q_proj.meta.scale_q', 'model.layers.11.self_attn.q_proj.meta.zero_q', 'model.layers.11.self_attn.v_proj.W_q', 'model.layers.11.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.11.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.11.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.11.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.11.self_attn.v_proj.meta.scale_q', 'model.layers.11.self_attn.v_proj.meta.zero_q', 'model.layers.12.block_sparse_moe.gate.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.W_q', 'model.layers.12.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.k_proj.meta.scale_q', 'model.layers.12.self_attn.k_proj.meta.zero_q', 'model.layers.12.self_attn.o_proj.W_q', 'model.layers.12.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.o_proj.meta.scale_q', 'model.layers.12.self_attn.o_proj.meta.zero_q', 'model.layers.12.self_attn.q_proj.W_q', 'model.layers.12.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.q_proj.meta.scale_q', 'model.layers.12.self_attn.q_proj.meta.zero_q', 'model.layers.12.self_attn.v_proj.W_q', 'model.layers.12.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.12.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.12.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.12.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.12.self_attn.v_proj.meta.scale_q', 'model.layers.12.self_attn.v_proj.meta.zero_q', 'model.layers.13.block_sparse_moe.gate.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.W_q', 'model.layers.13.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.k_proj.meta.scale_q', 'model.layers.13.self_attn.k_proj.meta.zero_q', 'model.layers.13.self_attn.o_proj.W_q', 'model.layers.13.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.o_proj.meta.scale_q', 'model.layers.13.self_attn.o_proj.meta.zero_q', 'model.layers.13.self_attn.q_proj.W_q', 'model.layers.13.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.q_proj.meta.scale_q', 'model.layers.13.self_attn.q_proj.meta.zero_q', 'model.layers.13.self_attn.v_proj.W_q', 'model.layers.13.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.13.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.13.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.13.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.13.self_attn.v_proj.meta.scale_q', 'model.layers.13.self_attn.v_proj.meta.zero_q', 'model.layers.14.block_sparse_moe.gate.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.W_q', 'model.layers.14.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.k_proj.meta.scale_q', 'model.layers.14.self_attn.k_proj.meta.zero_q', 'model.layers.14.self_attn.o_proj.W_q', 'model.layers.14.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.o_proj.meta.scale_q', 'model.layers.14.self_attn.o_proj.meta.zero_q', 'model.layers.14.self_attn.q_proj.W_q', 'model.layers.14.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.q_proj.meta.scale_q', 'model.layers.14.self_attn.q_proj.meta.zero_q', 'model.layers.14.self_attn.v_proj.W_q', 'model.layers.14.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.14.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.14.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.14.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.14.self_attn.v_proj.meta.scale_q', 'model.layers.14.self_attn.v_proj.meta.zero_q', 'model.layers.15.block_sparse_moe.gate.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.W_q', 'model.layers.15.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.k_proj.meta.scale_q', 'model.layers.15.self_attn.k_proj.meta.zero_q', 'model.layers.15.self_attn.o_proj.W_q', 'model.layers.15.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.o_proj.meta.scale_q', 'model.layers.15.self_attn.o_proj.meta.zero_q', 'model.layers.15.self_attn.q_proj.W_q', 'model.layers.15.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.q_proj.meta.scale_q', 'model.layers.15.self_attn.q_proj.meta.zero_q', 'model.layers.15.self_attn.v_proj.W_q', 'model.layers.15.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.15.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.15.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.15.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.15.self_attn.v_proj.meta.scale_q', 'model.layers.15.self_attn.v_proj.meta.zero_q', 'model.layers.16.block_sparse_moe.gate.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.W_q', 'model.layers.16.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.k_proj.meta.scale_q', 'model.layers.16.self_attn.k_proj.meta.zero_q', 'model.layers.16.self_attn.o_proj.W_q', 'model.layers.16.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.o_proj.meta.scale_q', 'model.layers.16.self_attn.o_proj.meta.zero_q', 'model.layers.16.self_attn.q_proj.W_q', 'model.layers.16.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.q_proj.meta.scale_q', 'model.layers.16.self_attn.q_proj.meta.zero_q', 'model.layers.16.self_attn.v_proj.W_q', 'model.layers.16.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.16.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.16.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.16.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.16.self_attn.v_proj.meta.scale_q', 'model.layers.16.self_attn.v_proj.meta.zero_q', 'model.layers.17.block_sparse_moe.gate.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.W_q', 'model.layers.17.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.k_proj.meta.scale_q', 'model.layers.17.self_attn.k_proj.meta.zero_q', 'model.layers.17.self_attn.o_proj.W_q', 'model.layers.17.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.o_proj.meta.scale_q', 'model.layers.17.self_attn.o_proj.meta.zero_q', 'model.layers.17.self_attn.q_proj.W_q', 'model.layers.17.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.q_proj.meta.scale_q', 'model.layers.17.self_attn.q_proj.meta.zero_q', 'model.layers.17.self_attn.v_proj.W_q', 'model.layers.17.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.17.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.17.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.17.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.17.self_attn.v_proj.meta.scale_q', 'model.layers.17.self_attn.v_proj.meta.zero_q', 'model.layers.18.block_sparse_moe.gate.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.W_q', 'model.layers.18.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.k_proj.meta.scale_q', 'model.layers.18.self_attn.k_proj.meta.zero_q', 'model.layers.18.self_attn.o_proj.W_q', 'model.layers.18.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.o_proj.meta.scale_q', 'model.layers.18.self_attn.o_proj.meta.zero_q', 'model.layers.18.self_attn.q_proj.W_q', 'model.layers.18.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.q_proj.meta.scale_q', 'model.layers.18.self_attn.q_proj.meta.zero_q', 'model.layers.18.self_attn.v_proj.W_q', 'model.layers.18.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.18.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.18.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.18.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.18.self_attn.v_proj.meta.scale_q', 'model.layers.18.self_attn.v_proj.meta.zero_q', 'model.layers.19.block_sparse_moe.gate.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.W_q', 'model.layers.19.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.k_proj.meta.scale_q', 'model.layers.19.self_attn.k_proj.meta.zero_q', 'model.layers.19.self_attn.o_proj.W_q', 'model.layers.19.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.o_proj.meta.scale_q', 'model.layers.19.self_attn.o_proj.meta.zero_q', 'model.layers.19.self_attn.q_proj.W_q', 'model.layers.19.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.q_proj.meta.scale_q', 'model.layers.19.self_attn.q_proj.meta.zero_q', 'model.layers.19.self_attn.v_proj.W_q', 'model.layers.19.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.19.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.19.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.19.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.19.self_attn.v_proj.meta.scale_q', 'model.layers.19.self_attn.v_proj.meta.zero_q', 'model.layers.2.block_sparse_moe.gate.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.W_q', 'model.layers.2.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.k_proj.meta.scale_q', 'model.layers.2.self_attn.k_proj.meta.zero_q', 'model.layers.2.self_attn.o_proj.W_q', 'model.layers.2.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.o_proj.meta.scale_q', 'model.layers.2.self_attn.o_proj.meta.zero_q', 'model.layers.2.self_attn.q_proj.W_q', 'model.layers.2.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.q_proj.meta.scale_q', 'model.layers.2.self_attn.q_proj.meta.zero_q', 'model.layers.2.self_attn.v_proj.W_q', 'model.layers.2.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.2.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.2.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.2.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.2.self_attn.v_proj.meta.scale_q', 'model.layers.2.self_attn.v_proj.meta.zero_q', 'model.layers.20.block_sparse_moe.gate.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.W_q', 'model.layers.20.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.k_proj.meta.scale_q', 'model.layers.20.self_attn.k_proj.meta.zero_q', 'model.layers.20.self_attn.o_proj.W_q', 'model.layers.20.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.o_proj.meta.scale_q', 'model.layers.20.self_attn.o_proj.meta.zero_q', 'model.layers.20.self_attn.q_proj.W_q', 'model.layers.20.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.q_proj.meta.scale_q', 'model.layers.20.self_attn.q_proj.meta.zero_q', 'model.layers.20.self_attn.v_proj.W_q', 'model.layers.20.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.20.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.20.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.20.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.20.self_attn.v_proj.meta.scale_q', 'model.layers.20.self_attn.v_proj.meta.zero_q', 'model.layers.21.block_sparse_moe.gate.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.W_q', 'model.layers.21.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.k_proj.meta.scale_q', 'model.layers.21.self_attn.k_proj.meta.zero_q', 'model.layers.21.self_attn.o_proj.W_q', 'model.layers.21.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.o_proj.meta.scale_q', 'model.layers.21.self_attn.o_proj.meta.zero_q', 'model.layers.21.self_attn.q_proj.W_q', 'model.layers.21.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.q_proj.meta.scale_q', 'model.layers.21.self_attn.q_proj.meta.zero_q', 'model.layers.21.self_attn.v_proj.W_q', 'model.layers.21.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.21.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.21.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.21.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.21.self_attn.v_proj.meta.scale_q', 'model.layers.21.self_attn.v_proj.meta.zero_q', 'model.layers.22.block_sparse_moe.gate.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.W_q', 'model.layers.22.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.k_proj.meta.scale_q', 'model.layers.22.self_attn.k_proj.meta.zero_q', 'model.layers.22.self_attn.o_proj.W_q', 'model.layers.22.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.o_proj.meta.scale_q', 'model.layers.22.self_attn.o_proj.meta.zero_q', 'model.layers.22.self_attn.q_proj.W_q', 'model.layers.22.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.q_proj.meta.scale_q', 'model.layers.22.self_attn.q_proj.meta.zero_q', 'model.layers.22.self_attn.v_proj.W_q', 'model.layers.22.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.22.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.22.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.22.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.22.self_attn.v_proj.meta.scale_q', 'model.layers.22.self_attn.v_proj.meta.zero_q', 'model.layers.23.block_sparse_moe.gate.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.W_q', 'model.layers.23.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.k_proj.meta.scale_q', 'model.layers.23.self_attn.k_proj.meta.zero_q', 'model.layers.23.self_attn.o_proj.W_q', 'model.layers.23.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.o_proj.meta.scale_q', 'model.layers.23.self_attn.o_proj.meta.zero_q', 'model.layers.23.self_attn.q_proj.W_q', 'model.layers.23.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.q_proj.meta.scale_q', 'model.layers.23.self_attn.q_proj.meta.zero_q', 'model.layers.23.self_attn.v_proj.W_q', 'model.layers.23.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.23.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.23.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.23.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.23.self_attn.v_proj.meta.scale_q', 'model.layers.23.self_attn.v_proj.meta.zero_q', 'model.layers.24.block_sparse_moe.gate.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.W_q', 'model.layers.24.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.k_proj.meta.scale_q', 'model.layers.24.self_attn.k_proj.meta.zero_q', 'model.layers.24.self_attn.o_proj.W_q', 'model.layers.24.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.o_proj.meta.scale_q', 'model.layers.24.self_attn.o_proj.meta.zero_q', 'model.layers.24.self_attn.q_proj.W_q', 'model.layers.24.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.q_proj.meta.scale_q', 'model.layers.24.self_attn.q_proj.meta.zero_q', 'model.layers.24.self_attn.v_proj.W_q', 'model.layers.24.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.24.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.24.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.24.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.24.self_attn.v_proj.meta.scale_q', 'model.layers.24.self_attn.v_proj.meta.zero_q', 'model.layers.25.block_sparse_moe.gate.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.W_q', 'model.layers.25.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.k_proj.meta.scale_q', 'model.layers.25.self_attn.k_proj.meta.zero_q', 'model.layers.25.self_attn.o_proj.W_q', 'model.layers.25.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.o_proj.meta.scale_q', 'model.layers.25.self_attn.o_proj.meta.zero_q', 'model.layers.25.self_attn.q_proj.W_q', 'model.layers.25.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.q_proj.meta.scale_q', 'model.layers.25.self_attn.q_proj.meta.zero_q', 'model.layers.25.self_attn.v_proj.W_q', 'model.layers.25.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.25.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.25.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.25.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.25.self_attn.v_proj.meta.scale_q', 'model.layers.25.self_attn.v_proj.meta.zero_q', 'model.layers.26.block_sparse_moe.gate.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.W_q', 'model.layers.26.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.k_proj.meta.scale_q', 'model.layers.26.self_attn.k_proj.meta.zero_q', 'model.layers.26.self_attn.o_proj.W_q', 'model.layers.26.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.o_proj.meta.scale_q', 'model.layers.26.self_attn.o_proj.meta.zero_q', 'model.layers.26.self_attn.q_proj.W_q', 'model.layers.26.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.q_proj.meta.scale_q', 'model.layers.26.self_attn.q_proj.meta.zero_q', 'model.layers.26.self_attn.v_proj.W_q', 'model.layers.26.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.26.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.26.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.26.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.26.self_attn.v_proj.meta.scale_q', 'model.layers.26.self_attn.v_proj.meta.zero_q', 'model.layers.27.block_sparse_moe.gate.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.W_q', 'model.layers.27.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.k_proj.meta.scale_q', 'model.layers.27.self_attn.k_proj.meta.zero_q', 'model.layers.27.self_attn.o_proj.W_q', 'model.layers.27.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.o_proj.meta.scale_q', 'model.layers.27.self_attn.o_proj.meta.zero_q', 'model.layers.27.self_attn.q_proj.W_q', 'model.layers.27.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.q_proj.meta.scale_q', 'model.layers.27.self_attn.q_proj.meta.zero_q', 'model.layers.27.self_attn.v_proj.W_q', 'model.layers.27.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.27.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.27.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.27.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.27.self_attn.v_proj.meta.scale_q', 'model.layers.27.self_attn.v_proj.meta.zero_q', 'model.layers.28.block_sparse_moe.gate.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.W_q', 'model.layers.28.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.k_proj.meta.scale_q', 'model.layers.28.self_attn.k_proj.meta.zero_q', 'model.layers.28.self_attn.o_proj.W_q', 'model.layers.28.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.o_proj.meta.scale_q', 'model.layers.28.self_attn.o_proj.meta.zero_q', 'model.layers.28.self_attn.q_proj.W_q', 'model.layers.28.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.q_proj.meta.scale_q', 'model.layers.28.self_attn.q_proj.meta.zero_q', 'model.layers.28.self_attn.v_proj.W_q', 'model.layers.28.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.28.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.28.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.28.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.28.self_attn.v_proj.meta.scale_q', 'model.layers.28.self_attn.v_proj.meta.zero_q', 'model.layers.29.block_sparse_moe.gate.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.W_q', 'model.layers.29.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.k_proj.meta.scale_q', 'model.layers.29.self_attn.k_proj.meta.zero_q', 'model.layers.29.self_attn.o_proj.W_q', 'model.layers.29.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.o_proj.meta.scale_q', 'model.layers.29.self_attn.o_proj.meta.zero_q', 'model.layers.29.self_attn.q_proj.W_q', 'model.layers.29.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.q_proj.meta.scale_q', 'model.layers.29.self_attn.q_proj.meta.zero_q', 'model.layers.29.self_attn.v_proj.W_q', 'model.layers.29.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.29.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.29.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.29.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.29.self_attn.v_proj.meta.scale_q', 'model.layers.29.self_attn.v_proj.meta.zero_q', 'model.layers.3.block_sparse_moe.gate.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.W_q', 'model.layers.3.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.k_proj.meta.scale_q', 'model.layers.3.self_attn.k_proj.meta.zero_q', 'model.layers.3.self_attn.o_proj.W_q', 'model.layers.3.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.o_proj.meta.scale_q', 'model.layers.3.self_attn.o_proj.meta.zero_q', 'model.layers.3.self_attn.q_proj.W_q', 'model.layers.3.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.q_proj.meta.scale_q', 'model.layers.3.self_attn.q_proj.meta.zero_q', 'model.layers.3.self_attn.v_proj.W_q', 'model.layers.3.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.3.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.3.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.3.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.3.self_attn.v_proj.meta.scale_q', 'model.layers.3.self_attn.v_proj.meta.zero_q', 'model.layers.30.block_sparse_moe.gate.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.W_q', 'model.layers.30.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.k_proj.meta.scale_q', 'model.layers.30.self_attn.k_proj.meta.zero_q', 'model.layers.30.self_attn.o_proj.W_q', 'model.layers.30.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.o_proj.meta.scale_q', 'model.layers.30.self_attn.o_proj.meta.zero_q', 'model.layers.30.self_attn.q_proj.W_q', 'model.layers.30.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.q_proj.meta.scale_q', 'model.layers.30.self_attn.q_proj.meta.zero_q', 'model.layers.30.self_attn.v_proj.W_q', 'model.layers.30.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.30.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.30.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.30.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.30.self_attn.v_proj.meta.scale_q', 'model.layers.30.self_attn.v_proj.meta.zero_q', 'model.layers.31.block_sparse_moe.gate.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.W_q', 'model.layers.31.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.k_proj.meta.scale_q', 'model.layers.31.self_attn.k_proj.meta.zero_q', 'model.layers.31.self_attn.o_proj.W_q', 'model.layers.31.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.o_proj.meta.scale_q', 'model.layers.31.self_attn.o_proj.meta.zero_q', 'model.layers.31.self_attn.q_proj.W_q', 'model.layers.31.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.q_proj.meta.scale_q', 'model.layers.31.self_attn.q_proj.meta.zero_q', 'model.layers.31.self_attn.v_proj.W_q', 'model.layers.31.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.31.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.31.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.31.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.31.self_attn.v_proj.meta.scale_q', 'model.layers.31.self_attn.v_proj.meta.zero_q', 'model.layers.4.block_sparse_moe.gate.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.W_q', 'model.layers.4.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.k_proj.meta.scale_q', 'model.layers.4.self_attn.k_proj.meta.zero_q', 'model.layers.4.self_attn.o_proj.W_q', 'model.layers.4.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.o_proj.meta.scale_q', 'model.layers.4.self_attn.o_proj.meta.zero_q', 'model.layers.4.self_attn.q_proj.W_q', 'model.layers.4.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.q_proj.meta.scale_q', 'model.layers.4.self_attn.q_proj.meta.zero_q', 'model.layers.4.self_attn.v_proj.W_q', 'model.layers.4.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.4.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.4.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.4.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.4.self_attn.v_proj.meta.scale_q', 'model.layers.4.self_attn.v_proj.meta.zero_q', 'model.layers.5.block_sparse_moe.gate.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.W_q', 'model.layers.5.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.k_proj.meta.scale_q', 'model.layers.5.self_attn.k_proj.meta.zero_q', 'model.layers.5.self_attn.o_proj.W_q', 'model.layers.5.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.o_proj.meta.scale_q', 'model.layers.5.self_attn.o_proj.meta.zero_q', 'model.layers.5.self_attn.q_proj.W_q', 'model.layers.5.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.q_proj.meta.scale_q', 'model.layers.5.self_attn.q_proj.meta.zero_q', 'model.layers.5.self_attn.v_proj.W_q', 'model.layers.5.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.5.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.5.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.5.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.5.self_attn.v_proj.meta.scale_q', 'model.layers.5.self_attn.v_proj.meta.zero_q', 'model.layers.6.block_sparse_moe.gate.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.W_q', 'model.layers.6.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.k_proj.meta.scale_q', 'model.layers.6.self_attn.k_proj.meta.zero_q', 'model.layers.6.self_attn.o_proj.W_q', 'model.layers.6.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.o_proj.meta.scale_q', 'model.layers.6.self_attn.o_proj.meta.zero_q', 'model.layers.6.self_attn.q_proj.W_q', 'model.layers.6.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.q_proj.meta.scale_q', 'model.layers.6.self_attn.q_proj.meta.zero_q', 'model.layers.6.self_attn.v_proj.W_q', 'model.layers.6.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.6.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.6.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.6.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.6.self_attn.v_proj.meta.scale_q', 'model.layers.6.self_attn.v_proj.meta.zero_q', 'model.layers.7.block_sparse_moe.gate.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.W_q', 'model.layers.7.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.k_proj.meta.scale_q', 'model.layers.7.self_attn.k_proj.meta.zero_q', 'model.layers.7.self_attn.o_proj.W_q', 'model.layers.7.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.o_proj.meta.scale_q', 'model.layers.7.self_attn.o_proj.meta.zero_q', 'model.layers.7.self_attn.q_proj.W_q', 'model.layers.7.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.q_proj.meta.scale_q', 'model.layers.7.self_attn.q_proj.meta.zero_q', 'model.layers.7.self_attn.v_proj.W_q', 'model.layers.7.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.7.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.7.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.7.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.7.self_attn.v_proj.meta.scale_q', 'model.layers.7.self_attn.v_proj.meta.zero_q', 'model.layers.8.block_sparse_moe.gate.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.W_q', 'model.layers.8.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.k_proj.meta.scale_q', 'model.layers.8.self_attn.k_proj.meta.zero_q', 'model.layers.8.self_attn.o_proj.W_q', 'model.layers.8.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.o_proj.meta.scale_q', 'model.layers.8.self_attn.o_proj.meta.zero_q', 'model.layers.8.self_attn.q_proj.W_q', 'model.layers.8.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.q_proj.meta.scale_q', 'model.layers.8.self_attn.q_proj.meta.zero_q', 'model.layers.8.self_attn.v_proj.W_q', 'model.layers.8.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.8.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.8.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.8.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.8.self_attn.v_proj.meta.scale_q', 'model.layers.8.self_attn.v_proj.meta.zero_q', 'model.layers.9.block_sparse_moe.gate.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.W_q', 'model.layers.9.self_attn.k_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.k_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.k_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.k_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.k_proj.meta.scale_q', 'model.layers.9.self_attn.k_proj.meta.zero_q', 'model.layers.9.self_attn.o_proj.W_q', 'model.layers.9.self_attn.o_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.o_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.o_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.o_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.o_proj.meta.scale_q', 'model.layers.9.self_attn.o_proj.meta.zero_q', 'model.layers.9.self_attn.q_proj.W_q', 'model.layers.9.self_attn.q_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.q_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.q_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.q_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.q_proj.meta.scale_q', 'model.layers.9.self_attn.q_proj.meta.zero_q', 'model.layers.9.self_attn.v_proj.W_q', 'model.layers.9.self_attn.v_proj.meta.meta_scale.scale', 'model.layers.9.self_attn.v_proj.meta.meta_scale.zero', 'model.layers.9.self_attn.v_proj.meta.meta_zero.scale', 'model.layers.9.self_attn.v_proj.meta.meta_zero.zero', 'model.layers.9.self_attn.v_proj.meta.scale_q', 'model.layers.9.self_attn.v_proj.meta.zero_q', 'model.norm.weight'])\n",
            "start\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.877171516418457 GB\n",
            "1\n",
            "2\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.884068489074707 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 3.946568489074707 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.009068489074707 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.071568489074707 GB\n",
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.386344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   3%|▎         | 1/32 [00:04<02:29,  4.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.448844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.448844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.448844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   6%|▋         | 2/32 [00:11<03:00,  6.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.511344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.573844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:   9%|▉         | 3/32 [00:18<03:01,  6.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  12%|█▎        | 4/32 [00:24<02:51,  6.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.636344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  16%|█▌        | 5/32 [00:29<02:42,  6.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.698844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.761344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.761344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.761344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.823844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.823844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.823844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.886344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  19%|█▉        | 6/32 [00:36<02:36,  6.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.886344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.886344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.886344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.886344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.886344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 4.948844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.011344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.011344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  22%|██▏       | 7/32 [01:04<05:32, 13.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.011344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  25%|██▌       | 8/32 [01:17<05:14, 13.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  28%|██▊       | 9/32 [01:22<04:04, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  31%|███▏      | 10/32 [01:26<03:07,  8.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  34%|███▍      | 11/32 [01:31<02:39,  7.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  38%|███▊      | 12/32 [01:38<02:27,  7.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.073844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  41%|████      | 13/32 [01:43<02:09,  6.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  44%|████▍     | 14/32 [01:50<02:01,  6.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  47%|████▋     | 15/32 [01:57<01:55,  6.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  50%|█████     | 16/32 [02:03<01:45,  6.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  53%|█████▎    | 17/32 [02:10<01:39,  6.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.136344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  56%|█████▋    | 18/32 [02:18<01:37,  6.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  59%|█████▉    | 19/32 [02:24<01:30,  6.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  62%|██████▎   | 20/32 [02:30<01:18,  6.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  66%|██████▌   | 21/32 [02:58<02:22, 12.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  69%|██████▉   | 22/32 [03:06<01:55, 11.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.198844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  72%|███████▏  | 23/32 [03:10<01:23,  9.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  75%|███████▌  | 24/32 [03:16<01:05,  8.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  78%|███████▊  | 25/32 [03:21<00:51,  7.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  81%|████████▏ | 26/32 [03:27<00:40,  6.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  84%|████████▍ | 27/32 [03:33<00:32,  6.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.261344909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  88%|████████▊ | 28/32 [03:39<00:25,  6.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  91%|█████████ | 29/32 [03:46<00:20,  6.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  94%|█████████▍| 30/32 [03:54<00:14,  7.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts:  97%|█████████▋| 31/32 [04:01<00:06,  6.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n",
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading experts: 100%|██████████| 32/32 [04:07<00:00,  7.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Peak GPU Memory Usage: 5.323844909667969 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except NameError:\n",
        "    pass\n",
        "from src.build_model import OffloadConfig, QuantConfig, build_model\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "# state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(quantized_model_name)\n",
        "config.num_experts_per_tok = 1\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "print(f\"initial GPU Memory Usage: {peak_memory_usage} GB\") \n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
        "offload_per_layer = 7\n",
        "# offload_per_layer = 5\n",
        "###############################################################\n",
        "\n",
        "num_experts = config.num_local_experts\n",
        "print(\"number experts:\", num_experts)\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "\n",
        "attn_config = BaseQuantizeConfig(\n",
        "    nbits=4,\n",
        "    group_size=64,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
        "\n",
        "\n",
        "ffn_config = BaseQuantizeConfig(\n",
        "    nbits=2,\n",
        "    group_size=16,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "    device=device,\n",
        "    quant_config=quant_config,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4hBFYtPTUzT"
      },
      "source": [
        "## Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zf4GkspecSm8",
        "outputId": "a4ce58f9-30cc-414d-fa66-e5a28262d666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91315 \n",
            "output length: 32, batch_size: 4, iter 0\n",
            "total inference time: 106.84455347061157 s; latency per token: 3.3388922959566116 tokens/s\n",
            "Peak GPU Memory Usage: 26.923448085784912 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 4, iter 1\n",
            "total inference time: 107.92586183547974 s; latency per token: 3.3726831823587418 tokens/s\n",
            "Peak GPU Memory Usage: 27.224334239959717 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 4, iter 2\n",
            "total inference time: 109.82498121261597 s; latency per token: 3.432030662894249 tokens/s\n",
            "Peak GPU Memory Usage: 27.221885204315186 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 4, iter 3\n",
            "total inference time: 105.26572871208191 s; latency per token: 3.2895540222525597 tokens/s\n",
            "Peak GPU Memory Usage: 27.22917127609253 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 4, iter 4\n",
            "total inference time: 110.35679316520691 s; latency per token: 3.448649786412716 tokens/s\n",
            "Peak GPU Memory Usage: 27.228034496307373 GB\n",
            "###AVERAGE\n",
            "output length: 32, batch_size: 4\n",
            "total inference time: 108.04358367919922 s; latency per token: 3.3763619899749755 tokens/s\n",
            "91315 \n",
            "output length: 32, batch_size: 16, iter 0\n",
            "total inference time: 162.48488759994507 s; latency per token: 5.077652737498283 tokens/s\n",
            "Peak GPU Memory Usage: 30.207082748413086 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 16, iter 1\n",
            "total inference time: 162.78239059448242 s; latency per token: 5.086949706077576 tokens/s\n",
            "Peak GPU Memory Usage: 31.40365505218506 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 16, iter 2\n",
            "total inference time: 157.3818461894989 s; latency per token: 4.918182693421841 tokens/s\n",
            "Peak GPU Memory Usage: 31.402701377868652 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 16, iter 3\n",
            "total inference time: 157.27526783943176 s; latency per token: 4.914852119982243 tokens/s\n",
            "Peak GPU Memory Usage: 31.405348777770996 GB\n",
            "91315 \n",
            "output length: 32, batch_size: 16, iter 4\n",
            "total inference time: 156.7767264842987 s; latency per token: 4.899272702634335 tokens/s\n",
            "Peak GPU Memory Usage: 31.406241416931152 GB\n",
            "###AVERAGE\n",
            "output length: 32, batch_size: 16\n",
            "total inference time: 159.34022374153136 s; latency per token: 4.979381991922855 tokens/s\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "import time\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "past_key_values = None\n",
        "sequence = None\n",
        "\n",
        "seq_len = 0\n",
        "for output_length in [128, 512]:\n",
        "    # text = \"\"\"Write an email with the subject line \"Resumes\".\"\"\"\n",
        "    for batch_size in [1, 4, 16]:\n",
        "      text =  \"\"\"In the twilight haze of a city that never truly slept, where the neon lights flickered like the last gasps of a dying star, there lived a girl named Lyra. She resided in the heart of the metropolis, in an apartment that was too small for dreams as big as hers. The world outside her window was a tapestry of shadows and light, a place where fortunes were made and lost with the flip of a coin, and where secrets whispered on the wind were more valuable than gold.\n",
        "\n",
        "            Lyra had a peculiar talent, one that set her apart from the millions of souls that hustled through the city’s arteries each day. She could see the threads of fate that bound people together, glowing lines that stretched out into the distance, intertwining and parting in a dance as old as time. It was a gift she had hidden away, fearful of the consequences should the wrong eyes find her. But as the city around her grew darker, consumed by a greed that ate at its heart, Lyra knew she could no longer remain hidden in the shadows.\"\"\"\n",
        "      #user_entry = dict(role=\"user\", content=user_input)\n",
        "\n",
        "      # input_ids = tokenizer(text, return_tensors=\"pt\").to(0).to(device)\n",
        "\n",
        "# Tokenize all entries (batch operation)\n",
        "      texts = [text] * batch_size\n",
        "      input_ids = tokenizer(texts, padding=False, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "      # input_ids = tokenizer.apply_chat_template([user_entry] * batch_size, return_tensors=\"pt\").to(device)\n",
        "      # input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      if past_key_values is None:\n",
        "        attention_mask = torch.ones_like(input_ids[0])\n",
        "      else:\n",
        "        seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
        "        attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n",
        "      \"\"\"\n",
        "      \n",
        "      for i in range(2):\n",
        "        _ = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=output_length,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        return_dict_in_generate=True,\n",
        "        output_hidden_states=True,\n",
        "      )\n",
        "\n",
        "      total_time = 0\n",
        "      for iter in range(4):\n",
        "          torch.cuda.reset_peak_memory_stats()\n",
        "          st = time.time()\n",
        "          result = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            top_p=0.9,\n",
        "            max_new_tokens=output_length,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            return_dict_in_generate=True,\n",
        "            output_hidden_states=True,\n",
        "          )\n",
        "          et = time.time()\n",
        "          torch.cuda.synchronize()\n",
        "          # print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "          peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "          for i, response in enumerate(result):\n",
        "              print(len(response), end=\"\")\n",
        "          print(\" \")\n",
        "          print(f\"output length: {output_length}, batch_size: {batch_size}, iter {iter}\")\n",
        "          print(f\"total inference time: {et - st} s; latency per token: {(et - st)/output_length} tokens/s\")\n",
        "          print(f\"Peak GPU Memory Usage: {peak_memory_usage} GB\")\n",
        "          total_time += et - st\n",
        "\n",
        "      print(\"###AVERAGE\")\n",
        "      print(f\"output length: {output_length}, batch_size: {batch_size}\")\n",
        "      print(f\"total inference time: {total_time / 5} s; latency per token: {total_time / 5 / output_length} tokens/s\")\n",
        "\n",
        "      \"\"\"\n",
        "      result = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        past_key_values=past_key_values,\n",
        "        streamer=streamer,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        return_dict_in_generate=True,\n",
        "        output_hidden_states=True,\n",
        "      )\n",
        "      print(\"\\n\")\n",
        "\n",
        "      sequence = result[\"sequences\"]\n",
        "      past_key_values = result[\"past_key_values\"]\n",
        "      peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "      print(f\"Peak GPU Memory Usage: {peak_memory_usage} GB\")\n",
        "      \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "Peak GPU Memory Usage: 27.330001831054688 GB\n",
            "dict_keys(['model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.block_sparse_moe.gate.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.block_sparse_moe.gate.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.block_sparse_moe.gate.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.block_sparse_moe.gate.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.block_sparse_moe.gate.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.block_sparse_moe.gate.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.block_sparse_moe.gate.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.block_sparse_moe.gate.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.20.block_sparse_moe.gate.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.block_sparse_moe.gate.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.block_sparse_moe.gate.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.block_sparse_moe.gate.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'lm_head.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.block_sparse_moe.gate.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.norm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.block_sparse_moe.gate.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.block_sparse_moe.gate.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.block_sparse_moe.gate.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.block_sparse_moe.gate.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.block_sparse_moe.gate.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.block_sparse_moe.gate.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.10.block_sparse_moe.gate.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.block_sparse_moe.gate.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.block_sparse_moe.gate.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.block_sparse_moe.gate.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.block_sparse_moe.gate.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.block_sparse_moe.gate.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.block_sparse_moe.gate.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.embed_tokens.weight', 'model.layers.0.block_sparse_moe.gate.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.block_sparse_moe.gate.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.block_sparse_moe.gate.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.block_sparse_moe.gate.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.block_sparse_moe.gate.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.block_sparse_moe.gate.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight'])\n",
            "{'model.layers.0.block_sparse_moe.experts.0.w1.weight': 'model-00002-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.0.w2.weight': 'model-00002-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.0.w3.weight': 'model-00002-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.1.w1.weight': 'model-00003-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.1.w2.weight': 'model-00003-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.1.w3.weight': 'model-00003-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.2.w1.weight': 'model-00004-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.2.w2.weight': 'model-00004-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.2.w3.weight': 'model-00004-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.3.w1.weight': 'model-00005-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.3.w2.weight': 'model-00005-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.3.w3.weight': 'model-00005-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.4.w1.weight': 'model-00006-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.4.w2.weight': 'model-00006-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.4.w3.weight': 'model-00006-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.5.w1.weight': 'model-00007-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.5.w2.weight': 'model-00007-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.5.w3.weight': 'model-00007-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.6.w1.weight': 'model-00008-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.6.w2.weight': 'model-00008-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.6.w3.weight': 'model-00008-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.7.w1.weight': 'model-00009-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.7.w2.weight': 'model-00009-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.7.w3.weight': 'model-00009-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.0.w1.weight': 'model-00010-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.0.w2.weight': 'model-00010-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.0.w3.weight': 'model-00010-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.1.w1.weight': 'model-00011-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.1.w2.weight': 'model-00011-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.1.w3.weight': 'model-00011-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.2.w1.weight': 'model-00012-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.2.w2.weight': 'model-00012-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.2.w3.weight': 'model-00012-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.3.w1.weight': 'model-00013-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.3.w2.weight': 'model-00013-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.3.w3.weight': 'model-00013-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.4.w1.weight': 'model-00014-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.4.w2.weight': 'model-00014-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.4.w3.weight': 'model-00014-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.5.w1.weight': 'model-00015-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.5.w2.weight': 'model-00015-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.5.w3.weight': 'model-00015-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.6.w1.weight': 'model-00016-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.6.w2.weight': 'model-00016-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.6.w3.weight': 'model-00016-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.7.w1.weight': 'model-00017-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.7.w2.weight': 'model-00017-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.7.w3.weight': 'model-00017-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.0.w1.weight': 'model-00018-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.0.w2.weight': 'model-00018-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.0.w3.weight': 'model-00018-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.1.w1.weight': 'model-00019-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.1.w2.weight': 'model-00019-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.1.w3.weight': 'model-00019-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.2.w1.weight': 'model-00020-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.2.w2.weight': 'model-00020-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.2.w3.weight': 'model-00020-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.3.w1.weight': 'model-00021-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.3.w2.weight': 'model-00021-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.3.w3.weight': 'model-00021-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.4.w1.weight': 'model-00022-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.4.w2.weight': 'model-00022-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.4.w3.weight': 'model-00022-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.5.w1.weight': 'model-00023-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.5.w2.weight': 'model-00023-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.5.w3.weight': 'model-00023-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.6.w1.weight': 'model-00024-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.6.w2.weight': 'model-00024-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.6.w3.weight': 'model-00024-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.7.w1.weight': 'model-00025-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.7.w2.weight': 'model-00025-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.7.w3.weight': 'model-00025-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.0.w1.weight': 'model-00026-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.0.w2.weight': 'model-00026-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.0.w3.weight': 'model-00026-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.1.w1.weight': 'model-00027-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.1.w2.weight': 'model-00027-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.1.w3.weight': 'model-00027-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.2.w1.weight': 'model-00028-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.2.w2.weight': 'model-00028-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.2.w3.weight': 'model-00028-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.3.w1.weight': 'model-00029-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.3.w2.weight': 'model-00029-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.3.w3.weight': 'model-00029-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.4.w1.weight': 'model-00030-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.4.w2.weight': 'model-00030-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.4.w3.weight': 'model-00030-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.5.w1.weight': 'model-00031-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.5.w2.weight': 'model-00031-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.5.w3.weight': 'model-00031-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.6.w1.weight': 'model-00032-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.6.w2.weight': 'model-00032-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.6.w3.weight': 'model-00032-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.7.w1.weight': 'model-00033-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.7.w2.weight': 'model-00033-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.7.w3.weight': 'model-00033-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.0.w1.weight': 'model-00034-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.0.w2.weight': 'model-00034-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.0.w3.weight': 'model-00034-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.1.w1.weight': 'model-00035-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.1.w2.weight': 'model-00035-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.1.w3.weight': 'model-00035-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.2.w1.weight': 'model-00036-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.2.w2.weight': 'model-00036-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.2.w3.weight': 'model-00036-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.3.w1.weight': 'model-00037-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.3.w2.weight': 'model-00037-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.3.w3.weight': 'model-00037-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.4.w1.weight': 'model-00038-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.4.w2.weight': 'model-00038-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.4.w3.weight': 'model-00038-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.5.w1.weight': 'model-00039-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.5.w2.weight': 'model-00039-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.5.w3.weight': 'model-00039-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.6.w1.weight': 'model-00040-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.6.w2.weight': 'model-00040-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.6.w3.weight': 'model-00040-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.7.w1.weight': 'model-00041-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.7.w2.weight': 'model-00041-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.7.w3.weight': 'model-00041-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.0.w1.weight': 'model-00042-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.0.w2.weight': 'model-00042-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.0.w3.weight': 'model-00042-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.1.w1.weight': 'model-00043-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.1.w2.weight': 'model-00043-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.1.w3.weight': 'model-00043-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.2.w1.weight': 'model-00044-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.2.w2.weight': 'model-00044-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.2.w3.weight': 'model-00044-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.3.w1.weight': 'model-00045-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.3.w2.weight': 'model-00045-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.3.w3.weight': 'model-00045-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.4.w1.weight': 'model-00046-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.4.w2.weight': 'model-00046-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.4.w3.weight': 'model-00046-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.5.w1.weight': 'model-00047-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.5.w2.weight': 'model-00047-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.5.w3.weight': 'model-00047-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.6.w1.weight': 'model-00048-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.6.w2.weight': 'model-00048-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.6.w3.weight': 'model-00048-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.7.w1.weight': 'model-00049-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.7.w2.weight': 'model-00049-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.7.w3.weight': 'model-00049-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.0.w1.weight': 'model-00050-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.0.w2.weight': 'model-00050-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.0.w3.weight': 'model-00050-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.1.w1.weight': 'model-00051-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.1.w2.weight': 'model-00051-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.1.w3.weight': 'model-00051-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.2.w1.weight': 'model-00052-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.2.w2.weight': 'model-00052-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.2.w3.weight': 'model-00052-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.3.w1.weight': 'model-00053-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.3.w2.weight': 'model-00053-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.3.w3.weight': 'model-00053-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.4.w1.weight': 'model-00054-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.4.w2.weight': 'model-00054-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.4.w3.weight': 'model-00054-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.5.w1.weight': 'model-00055-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.5.w2.weight': 'model-00055-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.5.w3.weight': 'model-00055-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.6.w1.weight': 'model-00056-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.6.w2.weight': 'model-00056-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.6.w3.weight': 'model-00056-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.7.w1.weight': 'model-00057-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.7.w2.weight': 'model-00057-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.7.w3.weight': 'model-00057-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.0.w1.weight': 'model-00058-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.0.w2.weight': 'model-00058-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.0.w3.weight': 'model-00058-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.1.w1.weight': 'model-00059-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.1.w2.weight': 'model-00059-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.1.w3.weight': 'model-00059-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.2.w1.weight': 'model-00060-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.2.w2.weight': 'model-00060-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.2.w3.weight': 'model-00060-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.3.w1.weight': 'model-00061-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.3.w2.weight': 'model-00061-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.3.w3.weight': 'model-00061-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.4.w1.weight': 'model-00062-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.4.w2.weight': 'model-00062-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.4.w3.weight': 'model-00062-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.5.w1.weight': 'model-00063-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.5.w2.weight': 'model-00063-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.5.w3.weight': 'model-00063-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.6.w1.weight': 'model-00064-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.6.w2.weight': 'model-00064-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.6.w3.weight': 'model-00064-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.7.w1.weight': 'model-00065-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.7.w2.weight': 'model-00065-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.7.w3.weight': 'model-00065-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.0.w1.weight': 'model-00066-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.0.w2.weight': 'model-00066-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.0.w3.weight': 'model-00066-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.1.w1.weight': 'model-00067-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.1.w2.weight': 'model-00067-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.1.w3.weight': 'model-00067-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.2.w1.weight': 'model-00068-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.2.w2.weight': 'model-00068-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.2.w3.weight': 'model-00068-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.3.w1.weight': 'model-00069-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.3.w2.weight': 'model-00069-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.3.w3.weight': 'model-00069-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.4.w1.weight': 'model-00070-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.4.w2.weight': 'model-00070-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.4.w3.weight': 'model-00070-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.5.w1.weight': 'model-00071-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.5.w2.weight': 'model-00071-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.5.w3.weight': 'model-00071-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.6.w1.weight': 'model-00072-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.6.w2.weight': 'model-00072-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.6.w3.weight': 'model-00072-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.7.w1.weight': 'model-00073-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.7.w2.weight': 'model-00073-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.7.w3.weight': 'model-00073-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.0.w1.weight': 'model-00074-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.0.w2.weight': 'model-00074-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.0.w3.weight': 'model-00074-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.1.w1.weight': 'model-00075-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.1.w2.weight': 'model-00075-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.1.w3.weight': 'model-00075-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.2.w1.weight': 'model-00076-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.2.w2.weight': 'model-00076-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.2.w3.weight': 'model-00076-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.3.w1.weight': 'model-00077-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.3.w2.weight': 'model-00077-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.3.w3.weight': 'model-00077-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.4.w1.weight': 'model-00078-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.4.w2.weight': 'model-00078-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.4.w3.weight': 'model-00078-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.5.w1.weight': 'model-00079-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.5.w2.weight': 'model-00079-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.5.w3.weight': 'model-00079-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.6.w1.weight': 'model-00080-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.6.w2.weight': 'model-00080-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.6.w3.weight': 'model-00080-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.7.w1.weight': 'model-00081-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.7.w2.weight': 'model-00081-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.7.w3.weight': 'model-00081-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.0.w1.weight': 'model-00082-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.0.w2.weight': 'model-00082-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.0.w3.weight': 'model-00082-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.1.w1.weight': 'model-00083-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.1.w2.weight': 'model-00083-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.1.w3.weight': 'model-00083-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.2.w1.weight': 'model-00084-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.2.w2.weight': 'model-00084-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.2.w3.weight': 'model-00084-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.3.w1.weight': 'model-00085-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.3.w2.weight': 'model-00085-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.3.w3.weight': 'model-00085-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.4.w1.weight': 'model-00086-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.4.w2.weight': 'model-00086-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.4.w3.weight': 'model-00086-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.5.w1.weight': 'model-00087-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.5.w2.weight': 'model-00087-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.5.w3.weight': 'model-00087-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.6.w1.weight': 'model-00088-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.6.w2.weight': 'model-00088-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.6.w3.weight': 'model-00088-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.7.w1.weight': 'model-00089-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.7.w2.weight': 'model-00089-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.7.w3.weight': 'model-00089-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.0.w1.weight': 'model-00090-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.0.w2.weight': 'model-00090-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.0.w3.weight': 'model-00090-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.1.w1.weight': 'model-00091-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.1.w2.weight': 'model-00091-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.1.w3.weight': 'model-00091-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.2.w1.weight': 'model-00092-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.2.w2.weight': 'model-00092-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.2.w3.weight': 'model-00092-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.3.w1.weight': 'model-00093-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.3.w2.weight': 'model-00093-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.3.w3.weight': 'model-00093-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.4.w1.weight': 'model-00094-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.4.w2.weight': 'model-00094-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.4.w3.weight': 'model-00094-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.5.w1.weight': 'model-00095-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.5.w2.weight': 'model-00095-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.5.w3.weight': 'model-00095-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.6.w1.weight': 'model-00096-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.6.w2.weight': 'model-00096-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.6.w3.weight': 'model-00096-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.7.w1.weight': 'model-00097-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.7.w2.weight': 'model-00097-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.7.w3.weight': 'model-00097-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.0.w1.weight': 'model-00098-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.0.w2.weight': 'model-00098-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.0.w3.weight': 'model-00098-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.1.w1.weight': 'model-00099-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.1.w2.weight': 'model-00099-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.1.w3.weight': 'model-00099-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.2.w1.weight': 'model-00100-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.2.w2.weight': 'model-00100-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.2.w3.weight': 'model-00100-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.3.w1.weight': 'model-00101-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.3.w2.weight': 'model-00101-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.3.w3.weight': 'model-00101-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.4.w1.weight': 'model-00102-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.4.w2.weight': 'model-00102-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.4.w3.weight': 'model-00102-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.5.w1.weight': 'model-00103-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.5.w2.weight': 'model-00103-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.5.w3.weight': 'model-00103-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.6.w1.weight': 'model-00104-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.6.w2.weight': 'model-00104-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.6.w3.weight': 'model-00104-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.7.w1.weight': 'model-00105-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.7.w2.weight': 'model-00105-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.7.w3.weight': 'model-00105-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.0.w1.weight': 'model-00106-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.0.w2.weight': 'model-00106-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.0.w3.weight': 'model-00106-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.1.w1.weight': 'model-00107-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.1.w2.weight': 'model-00107-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.1.w3.weight': 'model-00107-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.2.w1.weight': 'model-00108-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.2.w2.weight': 'model-00108-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.2.w3.weight': 'model-00108-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.3.w1.weight': 'model-00109-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.3.w2.weight': 'model-00109-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.3.w3.weight': 'model-00109-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.4.w1.weight': 'model-00110-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.4.w2.weight': 'model-00110-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.4.w3.weight': 'model-00110-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.5.w1.weight': 'model-00111-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.5.w2.weight': 'model-00111-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.5.w3.weight': 'model-00111-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.6.w1.weight': 'model-00112-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.6.w2.weight': 'model-00112-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.6.w3.weight': 'model-00112-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.7.w1.weight': 'model-00113-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.7.w2.weight': 'model-00113-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.7.w3.weight': 'model-00113-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.0.w1.weight': 'model-00114-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.0.w2.weight': 'model-00114-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.0.w3.weight': 'model-00114-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.1.w1.weight': 'model-00115-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.1.w2.weight': 'model-00115-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.1.w3.weight': 'model-00115-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.2.w1.weight': 'model-00116-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.2.w2.weight': 'model-00116-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.2.w3.weight': 'model-00116-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.3.w1.weight': 'model-00117-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.3.w2.weight': 'model-00117-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.3.w3.weight': 'model-00117-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.4.w1.weight': 'model-00118-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.4.w2.weight': 'model-00118-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.4.w3.weight': 'model-00118-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.5.w1.weight': 'model-00119-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.5.w2.weight': 'model-00119-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.5.w3.weight': 'model-00119-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.6.w1.weight': 'model-00120-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.6.w2.weight': 'model-00120-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.6.w3.weight': 'model-00120-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.7.w1.weight': 'model-00121-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.7.w2.weight': 'model-00121-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.7.w3.weight': 'model-00121-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.0.w1.weight': 'model-00122-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.0.w2.weight': 'model-00122-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.0.w3.weight': 'model-00122-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.1.w1.weight': 'model-00123-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.1.w2.weight': 'model-00123-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.1.w3.weight': 'model-00123-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.2.w1.weight': 'model-00124-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.2.w2.weight': 'model-00124-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.2.w3.weight': 'model-00124-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.3.w1.weight': 'model-00125-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.3.w2.weight': 'model-00125-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.3.w3.weight': 'model-00125-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.4.w1.weight': 'model-00126-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.4.w2.weight': 'model-00126-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.4.w3.weight': 'model-00126-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.5.w1.weight': 'model-00127-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.5.w2.weight': 'model-00127-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.5.w3.weight': 'model-00127-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.6.w1.weight': 'model-00128-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.6.w2.weight': 'model-00128-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.6.w3.weight': 'model-00128-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.7.w1.weight': 'model-00129-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.7.w2.weight': 'model-00129-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.7.w3.weight': 'model-00129-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.0.w1.weight': 'model-00130-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.0.w2.weight': 'model-00130-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.0.w3.weight': 'model-00130-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.1.w1.weight': 'model-00131-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.1.w2.weight': 'model-00131-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.1.w3.weight': 'model-00131-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.2.w1.weight': 'model-00132-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.2.w2.weight': 'model-00132-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.2.w3.weight': 'model-00132-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.3.w1.weight': 'model-00133-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.3.w2.weight': 'model-00133-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.3.w3.weight': 'model-00133-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.4.w1.weight': 'model-00134-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.4.w2.weight': 'model-00134-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.4.w3.weight': 'model-00134-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.5.w1.weight': 'model-00135-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.5.w2.weight': 'model-00135-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.5.w3.weight': 'model-00135-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.6.w1.weight': 'model-00136-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.6.w2.weight': 'model-00136-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.6.w3.weight': 'model-00136-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.7.w1.weight': 'model-00137-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.7.w2.weight': 'model-00137-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.7.w3.weight': 'model-00137-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.0.w1.weight': 'model-00138-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.0.w2.weight': 'model-00138-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.0.w3.weight': 'model-00138-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.1.w1.weight': 'model-00139-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.1.w2.weight': 'model-00139-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.1.w3.weight': 'model-00139-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.2.w1.weight': 'model-00140-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.2.w2.weight': 'model-00140-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.2.w3.weight': 'model-00140-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.3.w1.weight': 'model-00141-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.3.w2.weight': 'model-00141-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.3.w3.weight': 'model-00141-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.4.w1.weight': 'model-00142-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.4.w2.weight': 'model-00142-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.4.w3.weight': 'model-00142-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.5.w1.weight': 'model-00143-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.5.w2.weight': 'model-00143-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.5.w3.weight': 'model-00143-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.6.w1.weight': 'model-00144-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.6.w2.weight': 'model-00144-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.6.w3.weight': 'model-00144-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.7.w1.weight': 'model-00145-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.7.w2.weight': 'model-00145-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.7.w3.weight': 'model-00145-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.0.w1.weight': 'model-00146-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.0.w2.weight': 'model-00146-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.0.w3.weight': 'model-00146-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.1.w1.weight': 'model-00147-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.1.w2.weight': 'model-00147-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.1.w3.weight': 'model-00147-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.2.w1.weight': 'model-00148-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.2.w2.weight': 'model-00148-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.2.w3.weight': 'model-00148-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.3.w1.weight': 'model-00149-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.3.w2.weight': 'model-00149-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.3.w3.weight': 'model-00149-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.4.w1.weight': 'model-00150-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.4.w2.weight': 'model-00150-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.4.w3.weight': 'model-00150-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.5.w1.weight': 'model-00151-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.5.w2.weight': 'model-00151-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.5.w3.weight': 'model-00151-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.6.w1.weight': 'model-00152-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.6.w2.weight': 'model-00152-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.6.w3.weight': 'model-00152-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.7.w1.weight': 'model-00153-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.7.w2.weight': 'model-00153-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.7.w3.weight': 'model-00153-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.0.w1.weight': 'model-00154-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.0.w2.weight': 'model-00154-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.0.w3.weight': 'model-00154-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.1.w1.weight': 'model-00155-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.1.w2.weight': 'model-00155-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.1.w3.weight': 'model-00155-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.2.w1.weight': 'model-00156-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.2.w2.weight': 'model-00156-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.2.w3.weight': 'model-00156-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.3.w1.weight': 'model-00157-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.3.w2.weight': 'model-00157-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.3.w3.weight': 'model-00157-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.4.w1.weight': 'model-00158-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.4.w2.weight': 'model-00158-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.4.w3.weight': 'model-00158-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.5.w1.weight': 'model-00159-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.5.w2.weight': 'model-00159-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.5.w3.weight': 'model-00159-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.6.w1.weight': 'model-00160-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.6.w2.weight': 'model-00160-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.6.w3.weight': 'model-00160-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.7.w1.weight': 'model-00161-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.7.w2.weight': 'model-00161-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.7.w3.weight': 'model-00161-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.0.w1.weight': 'model-00162-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.0.w2.weight': 'model-00162-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.0.w3.weight': 'model-00162-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.1.w1.weight': 'model-00163-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.1.w2.weight': 'model-00163-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.1.w3.weight': 'model-00163-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.2.w1.weight': 'model-00164-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.2.w2.weight': 'model-00164-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.2.w3.weight': 'model-00164-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.3.w1.weight': 'model-00165-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.3.w2.weight': 'model-00165-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.3.w3.weight': 'model-00165-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.4.w1.weight': 'model-00166-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.4.w2.weight': 'model-00166-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.4.w3.weight': 'model-00166-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.5.w1.weight': 'model-00167-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.5.w2.weight': 'model-00167-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.5.w3.weight': 'model-00167-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.6.w1.weight': 'model-00168-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.6.w2.weight': 'model-00168-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.6.w3.weight': 'model-00168-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.7.w1.weight': 'model-00169-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.7.w2.weight': 'model-00169-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.7.w3.weight': 'model-00169-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.0.w1.weight': 'model-00170-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.0.w2.weight': 'model-00170-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.0.w3.weight': 'model-00170-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.1.w1.weight': 'model-00171-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.1.w2.weight': 'model-00171-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.1.w3.weight': 'model-00171-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.2.w1.weight': 'model-00172-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.2.w2.weight': 'model-00172-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.2.w3.weight': 'model-00172-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.3.w1.weight': 'model-00173-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.3.w2.weight': 'model-00173-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.3.w3.weight': 'model-00173-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.4.w1.weight': 'model-00174-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.4.w2.weight': 'model-00174-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.4.w3.weight': 'model-00174-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.5.w1.weight': 'model-00175-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.5.w2.weight': 'model-00175-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.5.w3.weight': 'model-00175-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.6.w1.weight': 'model-00176-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.6.w2.weight': 'model-00176-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.6.w3.weight': 'model-00176-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.7.w1.weight': 'model-00177-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.7.w2.weight': 'model-00177-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.7.w3.weight': 'model-00177-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.0.w1.weight': 'model-00178-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.0.w2.weight': 'model-00178-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.0.w3.weight': 'model-00178-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.1.w1.weight': 'model-00179-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.1.w2.weight': 'model-00179-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.1.w3.weight': 'model-00179-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.2.w1.weight': 'model-00180-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.2.w2.weight': 'model-00180-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.2.w3.weight': 'model-00180-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.3.w1.weight': 'model-00181-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.3.w2.weight': 'model-00181-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.3.w3.weight': 'model-00181-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.4.w1.weight': 'model-00182-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.4.w2.weight': 'model-00182-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.4.w3.weight': 'model-00182-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.5.w1.weight': 'model-00183-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.5.w2.weight': 'model-00183-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.5.w3.weight': 'model-00183-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.6.w1.weight': 'model-00184-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.6.w2.weight': 'model-00184-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.6.w3.weight': 'model-00184-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.7.w1.weight': 'model-00185-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.7.w2.weight': 'model-00185-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.7.w3.weight': 'model-00185-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.0.w1.weight': 'model-00186-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.0.w2.weight': 'model-00186-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.0.w3.weight': 'model-00186-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.1.w1.weight': 'model-00187-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.1.w2.weight': 'model-00187-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.1.w3.weight': 'model-00187-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.2.w1.weight': 'model-00188-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.2.w2.weight': 'model-00188-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.2.w3.weight': 'model-00188-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.3.w1.weight': 'model-00189-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.3.w2.weight': 'model-00189-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.3.w3.weight': 'model-00189-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.4.w1.weight': 'model-00190-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.4.w2.weight': 'model-00190-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.4.w3.weight': 'model-00190-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.5.w1.weight': 'model-00191-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.5.w2.weight': 'model-00191-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.5.w3.weight': 'model-00191-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.6.w1.weight': 'model-00192-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.6.w2.weight': 'model-00192-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.6.w3.weight': 'model-00192-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.7.w1.weight': 'model-00193-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.7.w2.weight': 'model-00193-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.7.w3.weight': 'model-00193-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.0.w1.weight': 'model-00194-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.0.w2.weight': 'model-00194-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.0.w3.weight': 'model-00194-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.1.w1.weight': 'model-00195-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.1.w2.weight': 'model-00195-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.1.w3.weight': 'model-00195-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.2.w1.weight': 'model-00196-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.2.w2.weight': 'model-00196-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.2.w3.weight': 'model-00196-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.3.w1.weight': 'model-00197-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.3.w2.weight': 'model-00197-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.3.w3.weight': 'model-00197-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.4.w1.weight': 'model-00198-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.4.w2.weight': 'model-00198-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.4.w3.weight': 'model-00198-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.5.w1.weight': 'model-00199-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.5.w2.weight': 'model-00199-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.5.w3.weight': 'model-00199-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.6.w1.weight': 'model-00200-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.6.w2.weight': 'model-00200-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.6.w3.weight': 'model-00200-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.7.w1.weight': 'model-00201-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.7.w2.weight': 'model-00201-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.7.w3.weight': 'model-00201-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.0.w1.weight': 'model-00202-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.0.w2.weight': 'model-00202-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.0.w3.weight': 'model-00202-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.1.w1.weight': 'model-00203-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.1.w2.weight': 'model-00203-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.1.w3.weight': 'model-00203-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.2.w1.weight': 'model-00204-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.2.w2.weight': 'model-00204-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.2.w3.weight': 'model-00204-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.3.w1.weight': 'model-00205-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.3.w2.weight': 'model-00205-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.3.w3.weight': 'model-00205-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.4.w1.weight': 'model-00206-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.4.w2.weight': 'model-00206-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.4.w3.weight': 'model-00206-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.5.w1.weight': 'model-00207-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.5.w2.weight': 'model-00207-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.5.w3.weight': 'model-00207-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.6.w1.weight': 'model-00208-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.6.w2.weight': 'model-00208-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.6.w3.weight': 'model-00208-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.7.w1.weight': 'model-00209-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.7.w2.weight': 'model-00209-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.7.w3.weight': 'model-00209-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.0.w1.weight': 'model-00210-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.0.w2.weight': 'model-00210-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.0.w3.weight': 'model-00210-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.1.w1.weight': 'model-00211-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.1.w2.weight': 'model-00211-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.1.w3.weight': 'model-00211-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.2.w1.weight': 'model-00212-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.2.w2.weight': 'model-00212-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.2.w3.weight': 'model-00212-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.3.w1.weight': 'model-00213-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.3.w2.weight': 'model-00213-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.3.w3.weight': 'model-00213-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.4.w1.weight': 'model-00214-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.4.w2.weight': 'model-00214-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.4.w3.weight': 'model-00214-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.5.w1.weight': 'model-00215-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.5.w2.weight': 'model-00215-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.5.w3.weight': 'model-00215-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.6.w1.weight': 'model-00216-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.6.w2.weight': 'model-00216-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.6.w3.weight': 'model-00216-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.7.w1.weight': 'model-00217-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.7.w2.weight': 'model-00217-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.7.w3.weight': 'model-00217-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.0.w1.weight': 'model-00218-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.0.w2.weight': 'model-00218-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.0.w3.weight': 'model-00218-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.1.w1.weight': 'model-00219-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.1.w2.weight': 'model-00219-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.1.w3.weight': 'model-00219-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.2.w1.weight': 'model-00220-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.2.w2.weight': 'model-00220-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.2.w3.weight': 'model-00220-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.3.w1.weight': 'model-00221-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.3.w2.weight': 'model-00221-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.3.w3.weight': 'model-00221-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.4.w1.weight': 'model-00222-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.4.w2.weight': 'model-00222-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.4.w3.weight': 'model-00222-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.5.w1.weight': 'model-00223-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.5.w2.weight': 'model-00223-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.5.w3.weight': 'model-00223-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.6.w1.weight': 'model-00224-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.6.w2.weight': 'model-00224-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.6.w3.weight': 'model-00224-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.7.w1.weight': 'model-00225-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.7.w2.weight': 'model-00225-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.7.w3.weight': 'model-00225-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.0.w1.weight': 'model-00226-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.0.w2.weight': 'model-00226-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.0.w3.weight': 'model-00226-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.1.w1.weight': 'model-00227-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.1.w2.weight': 'model-00227-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.1.w3.weight': 'model-00227-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.2.w1.weight': 'model-00228-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.2.w2.weight': 'model-00228-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.2.w3.weight': 'model-00228-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.3.w1.weight': 'model-00229-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.3.w2.weight': 'model-00229-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.3.w3.weight': 'model-00229-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.4.w1.weight': 'model-00230-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.4.w2.weight': 'model-00230-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.4.w3.weight': 'model-00230-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.5.w1.weight': 'model-00231-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.5.w2.weight': 'model-00231-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.5.w3.weight': 'model-00231-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.6.w1.weight': 'model-00232-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.6.w2.weight': 'model-00232-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.6.w3.weight': 'model-00232-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.7.w1.weight': 'model-00233-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.7.w2.weight': 'model-00233-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.7.w3.weight': 'model-00233-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.0.w1.weight': 'model-00234-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.0.w2.weight': 'model-00234-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.0.w3.weight': 'model-00234-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.1.w1.weight': 'model-00235-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.1.w2.weight': 'model-00235-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.1.w3.weight': 'model-00235-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.2.w1.weight': 'model-00236-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.2.w2.weight': 'model-00236-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.2.w3.weight': 'model-00236-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.3.w1.weight': 'model-00237-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.3.w2.weight': 'model-00237-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.3.w3.weight': 'model-00237-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.4.w1.weight': 'model-00238-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.4.w2.weight': 'model-00238-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.4.w3.weight': 'model-00238-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.5.w1.weight': 'model-00239-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.5.w2.weight': 'model-00239-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.5.w3.weight': 'model-00239-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.6.w1.weight': 'model-00240-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.6.w2.weight': 'model-00240-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.6.w3.weight': 'model-00240-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.7.w1.weight': 'model-00241-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.7.w2.weight': 'model-00241-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.7.w3.weight': 'model-00241-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.0.w1.weight': 'model-00242-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.0.w2.weight': 'model-00242-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.0.w3.weight': 'model-00242-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.1.w1.weight': 'model-00243-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.1.w2.weight': 'model-00243-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.1.w3.weight': 'model-00243-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.2.w1.weight': 'model-00244-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.2.w2.weight': 'model-00244-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.2.w3.weight': 'model-00244-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.3.w1.weight': 'model-00245-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.3.w2.weight': 'model-00245-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.3.w3.weight': 'model-00245-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.4.w1.weight': 'model-00246-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.4.w2.weight': 'model-00246-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.4.w3.weight': 'model-00246-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.5.w1.weight': 'model-00247-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.5.w2.weight': 'model-00247-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.5.w3.weight': 'model-00247-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.6.w1.weight': 'model-00248-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.6.w2.weight': 'model-00248-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.6.w3.weight': 'model-00248-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.7.w1.weight': 'model-00249-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.7.w2.weight': 'model-00249-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.7.w3.weight': 'model-00249-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.0.w1.weight': 'model-00250-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.0.w2.weight': 'model-00250-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.0.w3.weight': 'model-00250-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.1.w1.weight': 'model-00251-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.1.w2.weight': 'model-00251-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.1.w3.weight': 'model-00251-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.2.w1.weight': 'model-00252-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.2.w2.weight': 'model-00252-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.2.w3.weight': 'model-00252-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.3.w1.weight': 'model-00253-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.3.w2.weight': 'model-00253-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.3.w3.weight': 'model-00253-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.4.w1.weight': 'model-00254-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.4.w2.weight': 'model-00254-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.4.w3.weight': 'model-00254-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.5.w1.weight': 'model-00255-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.5.w2.weight': 'model-00255-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.5.w3.weight': 'model-00255-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.6.w1.weight': 'model-00256-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.6.w2.weight': 'model-00256-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.6.w3.weight': 'model-00256-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.7.w1.weight': 'model-00257-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.7.w2.weight': 'model-00257-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.7.w3.weight': 'model-00257-of-00257.safetensors', 'model.layers.1.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.1.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.2.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.2.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.2.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.2.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.2.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.2.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.2.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.3.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.3.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.3.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.3.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.3.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.28.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.28.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.29.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.29.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.29.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.29.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.29.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.29.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.29.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.30.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.30.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.30.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.30.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.30.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.3.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.3.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.4.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.4.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.4.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.4.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.4.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.4.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.4.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.5.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.5.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.5.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.5.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.5.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.5.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.5.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.6.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.6.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.6.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.6.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.6.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.18.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.18.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.19.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.19.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.19.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.19.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.19.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.19.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.19.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.20.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.20.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.20.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.20.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.20.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.20.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.20.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.21.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.21.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.21.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.21.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.21.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.21.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.21.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.22.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.22.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.22.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.22.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.22.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.10.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.10.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.11.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.11.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.11.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.11.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.11.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'lm_head.weight': 'model-00001-of-00257.safetensors', 'model.layers.30.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.30.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.31.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.31.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.31.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.31.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.31.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.31.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.31.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.norm.weight': 'model-00001-of-00257.safetensors', 'model.layers.27.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.27.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.28.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.28.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.28.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.28.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.28.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.13.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.13.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.14.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.14.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.14.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.14.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.14.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.14.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.14.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.15.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.15.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.15.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.15.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.15.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.22.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.22.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.23.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.23.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.23.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.23.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.23.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.23.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.23.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.24.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.24.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.24.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.24.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.24.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.24.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.24.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.25.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.25.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.25.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.25.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.25.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.10.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.10.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.10.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.10.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.10.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.8.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.8.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.9.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.9.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.9.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.9.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.9.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.9.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.9.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.15.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.15.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.16.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.16.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.16.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.16.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.16.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.6.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.6.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.7.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.7.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.7.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.7.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.7.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.7.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.7.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.8.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.8.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.8.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.8.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.8.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.11.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.11.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.12.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.12.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.12.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.12.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.12.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.12.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.12.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.13.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.13.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.13.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.13.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.13.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.embed_tokens.weight': 'model-00001-of-00257.safetensors', 'model.layers.0.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.0.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.0.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.0.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.0.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.0.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.0.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.1.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.1.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.1.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.1.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.1.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.25.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.25.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.26.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.26.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.26.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.26.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.26.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.26.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.26.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.27.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.27.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.27.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.27.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.27.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.16.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.16.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.17.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.17.input_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.17.post_attention_layernorm.weight': 'model-00001-of-00257.safetensors', 'model.layers.17.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.17.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.17.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.17.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.18.block_sparse_moe.gate.weight': 'model-00001-of-00257.safetensors', 'model.layers.18.self_attn.k_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.18.self_attn.o_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.18.self_attn.q_proj.weight': 'model-00001-of-00257.safetensors', 'model.layers.18.self_attn.v_proj.weight': 'model-00001-of-00257.safetensors'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import gc\n",
        "import re\n",
        "import os\n",
        "from safetensors.torch import load_file, save_file\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "save_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/test_dir/\"\n",
        "state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/Mixtral-8x7B-Instruct-v0.1\"\n",
        "state_index_path = os.path.join(state_path, \"model.safetensors.index.json\")\n",
        "save_index_path = os.path.join(save_path, \"model.safetensors.index.json\")\n",
        "with open(save_index_path) as f:\n",
        "    save_weight_map = json.load(f)[\"weight_map\"]\n",
        "with open(state_index_path) as f:\n",
        "    state_weight_map = json.load(f)[\"weight_map\"]\n",
        "\n",
        "state_dict = {}\n",
        "exclusion_pattern = re.compile(r\"model\\.layers\\.\\d+\\.block_sparse_moe\\.experts\\.\\d+\\.(w1|w2|w3)\\.weight\")\n",
        "unique_filenames = set(state_weight_map.values())\n",
        "for filename in unique_filenames:\n",
        "    file_path = os.path.join(state_path, filename)\n",
        "    loaded_file = load_file(file_path, device=str(device))\n",
        "    for key, file_in_map in state_weight_map.items():\n",
        "        if filename == file_in_map and not exclusion_pattern.search(key):\n",
        "            if key in loaded_file:\n",
        "                # print(f\"Adding tensor to state_dict: {key}\")\n",
        "                state_dict[key] = loaded_file[key]\n",
        "            else:\n",
        "                print(f\"Expected tensor not found in safetensor file: {key}\")\n",
        "    peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3) \n",
        "    print(f\"Peak GPU Memory Usage: {peak_memory_usage} GB\")  \n",
        "\n",
        "print(state_dict.keys())\n",
        "half_state_dict = {key: tensor.half() for key, tensor in state_dict.items()}\n",
        "file_to_save = save_path + \"model-00001-of-00257.safetensors\" \n",
        "save_file(half_state_dict, file_to_save)\n",
        "\n",
        "if not os.path.exists(save_index_path):\n",
        "    state_index_data = {\"weight_map\": {}}\n",
        "    print(\"???\")\n",
        "else:\n",
        "    with open(save_index_path, 'r') as index_file:\n",
        "        index_data = json.load(index_file)\n",
        "\n",
        "for key in state_dict.keys():\n",
        "    index_data['weight_map'][key] = \"model-00001-of-00257.safetensors\" \n",
        "\n",
        "print(index_data['weight_map'])\n",
        "with open(save_index_path, 'w') as index_file:\n",
        "    json.dump(index_data, index_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "dict_keys(['w1.weight', 'w2.weight', 'w3.weight'])\n",
            "{'model.layers.0.block_sparse_moe.experts.0.w1.weight': 'model-00002-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.0.w2.weight': 'model-00002-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.0.w3.weight': 'model-00002-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.1.w1.weight': 'model-00003-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.1.w2.weight': 'model-00003-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.1.w3.weight': 'model-00003-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.2.w1.weight': 'model-00004-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.2.w2.weight': 'model-00004-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.2.w3.weight': 'model-00004-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.3.w1.weight': 'model-00005-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.3.w2.weight': 'model-00005-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.3.w3.weight': 'model-00005-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.4.w1.weight': 'model-00006-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.4.w2.weight': 'model-00006-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.4.w3.weight': 'model-00006-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.5.w1.weight': 'model-00007-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.5.w2.weight': 'model-00007-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.5.w3.weight': 'model-00007-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.6.w1.weight': 'model-00008-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.6.w2.weight': 'model-00008-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.6.w3.weight': 'model-00008-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.7.w1.weight': 'model-00009-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.7.w2.weight': 'model-00009-of-00257.safetensors', 'model.layers.0.block_sparse_moe.experts.7.w3.weight': 'model-00009-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.0.w1.weight': 'model-00010-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.0.w2.weight': 'model-00010-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.0.w3.weight': 'model-00010-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.1.w1.weight': 'model-00011-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.1.w2.weight': 'model-00011-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.1.w3.weight': 'model-00011-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.2.w1.weight': 'model-00012-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.2.w2.weight': 'model-00012-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.2.w3.weight': 'model-00012-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.3.w1.weight': 'model-00013-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.3.w2.weight': 'model-00013-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.3.w3.weight': 'model-00013-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.4.w1.weight': 'model-00014-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.4.w2.weight': 'model-00014-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.4.w3.weight': 'model-00014-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.5.w1.weight': 'model-00015-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.5.w2.weight': 'model-00015-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.5.w3.weight': 'model-00015-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.6.w1.weight': 'model-00016-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.6.w2.weight': 'model-00016-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.6.w3.weight': 'model-00016-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.7.w1.weight': 'model-00017-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.7.w2.weight': 'model-00017-of-00257.safetensors', 'model.layers.1.block_sparse_moe.experts.7.w3.weight': 'model-00017-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.0.w1.weight': 'model-00018-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.0.w2.weight': 'model-00018-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.0.w3.weight': 'model-00018-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.1.w1.weight': 'model-00019-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.1.w2.weight': 'model-00019-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.1.w3.weight': 'model-00019-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.2.w1.weight': 'model-00020-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.2.w2.weight': 'model-00020-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.2.w3.weight': 'model-00020-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.3.w1.weight': 'model-00021-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.3.w2.weight': 'model-00021-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.3.w3.weight': 'model-00021-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.4.w1.weight': 'model-00022-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.4.w2.weight': 'model-00022-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.4.w3.weight': 'model-00022-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.5.w1.weight': 'model-00023-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.5.w2.weight': 'model-00023-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.5.w3.weight': 'model-00023-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.6.w1.weight': 'model-00024-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.6.w2.weight': 'model-00024-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.6.w3.weight': 'model-00024-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.7.w1.weight': 'model-00025-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.7.w2.weight': 'model-00025-of-00257.safetensors', 'model.layers.2.block_sparse_moe.experts.7.w3.weight': 'model-00025-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.0.w1.weight': 'model-00026-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.0.w2.weight': 'model-00026-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.0.w3.weight': 'model-00026-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.1.w1.weight': 'model-00027-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.1.w2.weight': 'model-00027-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.1.w3.weight': 'model-00027-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.2.w1.weight': 'model-00028-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.2.w2.weight': 'model-00028-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.2.w3.weight': 'model-00028-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.3.w1.weight': 'model-00029-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.3.w2.weight': 'model-00029-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.3.w3.weight': 'model-00029-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.4.w1.weight': 'model-00030-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.4.w2.weight': 'model-00030-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.4.w3.weight': 'model-00030-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.5.w1.weight': 'model-00031-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.5.w2.weight': 'model-00031-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.5.w3.weight': 'model-00031-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.6.w1.weight': 'model-00032-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.6.w2.weight': 'model-00032-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.6.w3.weight': 'model-00032-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.7.w1.weight': 'model-00033-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.7.w2.weight': 'model-00033-of-00257.safetensors', 'model.layers.3.block_sparse_moe.experts.7.w3.weight': 'model-00033-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.0.w1.weight': 'model-00034-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.0.w2.weight': 'model-00034-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.0.w3.weight': 'model-00034-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.1.w1.weight': 'model-00035-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.1.w2.weight': 'model-00035-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.1.w3.weight': 'model-00035-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.2.w1.weight': 'model-00036-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.2.w2.weight': 'model-00036-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.2.w3.weight': 'model-00036-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.3.w1.weight': 'model-00037-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.3.w2.weight': 'model-00037-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.3.w3.weight': 'model-00037-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.4.w1.weight': 'model-00038-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.4.w2.weight': 'model-00038-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.4.w3.weight': 'model-00038-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.5.w1.weight': 'model-00039-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.5.w2.weight': 'model-00039-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.5.w3.weight': 'model-00039-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.6.w1.weight': 'model-00040-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.6.w2.weight': 'model-00040-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.6.w3.weight': 'model-00040-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.7.w1.weight': 'model-00041-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.7.w2.weight': 'model-00041-of-00257.safetensors', 'model.layers.4.block_sparse_moe.experts.7.w3.weight': 'model-00041-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.0.w1.weight': 'model-00042-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.0.w2.weight': 'model-00042-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.0.w3.weight': 'model-00042-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.1.w1.weight': 'model-00043-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.1.w2.weight': 'model-00043-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.1.w3.weight': 'model-00043-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.2.w1.weight': 'model-00044-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.2.w2.weight': 'model-00044-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.2.w3.weight': 'model-00044-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.3.w1.weight': 'model-00045-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.3.w2.weight': 'model-00045-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.3.w3.weight': 'model-00045-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.4.w1.weight': 'model-00046-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.4.w2.weight': 'model-00046-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.4.w3.weight': 'model-00046-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.5.w1.weight': 'model-00047-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.5.w2.weight': 'model-00047-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.5.w3.weight': 'model-00047-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.6.w1.weight': 'model-00048-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.6.w2.weight': 'model-00048-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.6.w3.weight': 'model-00048-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.7.w1.weight': 'model-00049-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.7.w2.weight': 'model-00049-of-00257.safetensors', 'model.layers.5.block_sparse_moe.experts.7.w3.weight': 'model-00049-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.0.w1.weight': 'model-00050-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.0.w2.weight': 'model-00050-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.0.w3.weight': 'model-00050-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.1.w1.weight': 'model-00051-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.1.w2.weight': 'model-00051-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.1.w3.weight': 'model-00051-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.2.w1.weight': 'model-00052-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.2.w2.weight': 'model-00052-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.2.w3.weight': 'model-00052-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.3.w1.weight': 'model-00053-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.3.w2.weight': 'model-00053-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.3.w3.weight': 'model-00053-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.4.w1.weight': 'model-00054-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.4.w2.weight': 'model-00054-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.4.w3.weight': 'model-00054-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.5.w1.weight': 'model-00055-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.5.w2.weight': 'model-00055-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.5.w3.weight': 'model-00055-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.6.w1.weight': 'model-00056-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.6.w2.weight': 'model-00056-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.6.w3.weight': 'model-00056-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.7.w1.weight': 'model-00057-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.7.w2.weight': 'model-00057-of-00257.safetensors', 'model.layers.6.block_sparse_moe.experts.7.w3.weight': 'model-00057-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.0.w1.weight': 'model-00058-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.0.w2.weight': 'model-00058-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.0.w3.weight': 'model-00058-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.1.w1.weight': 'model-00059-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.1.w2.weight': 'model-00059-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.1.w3.weight': 'model-00059-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.2.w1.weight': 'model-00060-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.2.w2.weight': 'model-00060-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.2.w3.weight': 'model-00060-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.3.w1.weight': 'model-00061-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.3.w2.weight': 'model-00061-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.3.w3.weight': 'model-00061-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.4.w1.weight': 'model-00062-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.4.w2.weight': 'model-00062-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.4.w3.weight': 'model-00062-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.5.w1.weight': 'model-00063-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.5.w2.weight': 'model-00063-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.5.w3.weight': 'model-00063-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.6.w1.weight': 'model-00064-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.6.w2.weight': 'model-00064-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.6.w3.weight': 'model-00064-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.7.w1.weight': 'model-00065-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.7.w2.weight': 'model-00065-of-00257.safetensors', 'model.layers.7.block_sparse_moe.experts.7.w3.weight': 'model-00065-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.0.w1.weight': 'model-00066-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.0.w2.weight': 'model-00066-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.0.w3.weight': 'model-00066-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.1.w1.weight': 'model-00067-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.1.w2.weight': 'model-00067-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.1.w3.weight': 'model-00067-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.2.w1.weight': 'model-00068-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.2.w2.weight': 'model-00068-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.2.w3.weight': 'model-00068-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.3.w1.weight': 'model-00069-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.3.w2.weight': 'model-00069-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.3.w3.weight': 'model-00069-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.4.w1.weight': 'model-00070-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.4.w2.weight': 'model-00070-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.4.w3.weight': 'model-00070-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.5.w1.weight': 'model-00071-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.5.w2.weight': 'model-00071-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.5.w3.weight': 'model-00071-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.6.w1.weight': 'model-00072-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.6.w2.weight': 'model-00072-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.6.w3.weight': 'model-00072-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.7.w1.weight': 'model-00073-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.7.w2.weight': 'model-00073-of-00257.safetensors', 'model.layers.8.block_sparse_moe.experts.7.w3.weight': 'model-00073-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.0.w1.weight': 'model-00074-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.0.w2.weight': 'model-00074-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.0.w3.weight': 'model-00074-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.1.w1.weight': 'model-00075-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.1.w2.weight': 'model-00075-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.1.w3.weight': 'model-00075-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.2.w1.weight': 'model-00076-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.2.w2.weight': 'model-00076-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.2.w3.weight': 'model-00076-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.3.w1.weight': 'model-00077-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.3.w2.weight': 'model-00077-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.3.w3.weight': 'model-00077-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.4.w1.weight': 'model-00078-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.4.w2.weight': 'model-00078-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.4.w3.weight': 'model-00078-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.5.w1.weight': 'model-00079-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.5.w2.weight': 'model-00079-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.5.w3.weight': 'model-00079-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.6.w1.weight': 'model-00080-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.6.w2.weight': 'model-00080-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.6.w3.weight': 'model-00080-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.7.w1.weight': 'model-00081-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.7.w2.weight': 'model-00081-of-00257.safetensors', 'model.layers.9.block_sparse_moe.experts.7.w3.weight': 'model-00081-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.0.w1.weight': 'model-00082-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.0.w2.weight': 'model-00082-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.0.w3.weight': 'model-00082-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.1.w1.weight': 'model-00083-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.1.w2.weight': 'model-00083-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.1.w3.weight': 'model-00083-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.2.w1.weight': 'model-00084-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.2.w2.weight': 'model-00084-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.2.w3.weight': 'model-00084-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.3.w1.weight': 'model-00085-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.3.w2.weight': 'model-00085-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.3.w3.weight': 'model-00085-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.4.w1.weight': 'model-00086-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.4.w2.weight': 'model-00086-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.4.w3.weight': 'model-00086-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.5.w1.weight': 'model-00087-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.5.w2.weight': 'model-00087-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.5.w3.weight': 'model-00087-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.6.w1.weight': 'model-00088-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.6.w2.weight': 'model-00088-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.6.w3.weight': 'model-00088-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.7.w1.weight': 'model-00089-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.7.w2.weight': 'model-00089-of-00257.safetensors', 'model.layers.10.block_sparse_moe.experts.7.w3.weight': 'model-00089-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.0.w1.weight': 'model-00090-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.0.w2.weight': 'model-00090-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.0.w3.weight': 'model-00090-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.1.w1.weight': 'model-00091-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.1.w2.weight': 'model-00091-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.1.w3.weight': 'model-00091-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.2.w1.weight': 'model-00092-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.2.w2.weight': 'model-00092-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.2.w3.weight': 'model-00092-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.3.w1.weight': 'model-00093-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.3.w2.weight': 'model-00093-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.3.w3.weight': 'model-00093-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.4.w1.weight': 'model-00094-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.4.w2.weight': 'model-00094-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.4.w3.weight': 'model-00094-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.5.w1.weight': 'model-00095-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.5.w2.weight': 'model-00095-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.5.w3.weight': 'model-00095-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.6.w1.weight': 'model-00096-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.6.w2.weight': 'model-00096-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.6.w3.weight': 'model-00096-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.7.w1.weight': 'model-00097-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.7.w2.weight': 'model-00097-of-00257.safetensors', 'model.layers.11.block_sparse_moe.experts.7.w3.weight': 'model-00097-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.0.w1.weight': 'model-00098-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.0.w2.weight': 'model-00098-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.0.w3.weight': 'model-00098-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.1.w1.weight': 'model-00099-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.1.w2.weight': 'model-00099-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.1.w3.weight': 'model-00099-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.2.w1.weight': 'model-00100-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.2.w2.weight': 'model-00100-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.2.w3.weight': 'model-00100-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.3.w1.weight': 'model-00101-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.3.w2.weight': 'model-00101-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.3.w3.weight': 'model-00101-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.4.w1.weight': 'model-00102-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.4.w2.weight': 'model-00102-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.4.w3.weight': 'model-00102-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.5.w1.weight': 'model-00103-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.5.w2.weight': 'model-00103-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.5.w3.weight': 'model-00103-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.6.w1.weight': 'model-00104-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.6.w2.weight': 'model-00104-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.6.w3.weight': 'model-00104-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.7.w1.weight': 'model-00105-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.7.w2.weight': 'model-00105-of-00257.safetensors', 'model.layers.12.block_sparse_moe.experts.7.w3.weight': 'model-00105-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.0.w1.weight': 'model-00106-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.0.w2.weight': 'model-00106-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.0.w3.weight': 'model-00106-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.1.w1.weight': 'model-00107-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.1.w2.weight': 'model-00107-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.1.w3.weight': 'model-00107-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.2.w1.weight': 'model-00108-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.2.w2.weight': 'model-00108-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.2.w3.weight': 'model-00108-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.3.w1.weight': 'model-00109-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.3.w2.weight': 'model-00109-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.3.w3.weight': 'model-00109-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.4.w1.weight': 'model-00110-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.4.w2.weight': 'model-00110-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.4.w3.weight': 'model-00110-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.5.w1.weight': 'model-00111-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.5.w2.weight': 'model-00111-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.5.w3.weight': 'model-00111-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.6.w1.weight': 'model-00112-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.6.w2.weight': 'model-00112-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.6.w3.weight': 'model-00112-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.7.w1.weight': 'model-00113-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.7.w2.weight': 'model-00113-of-00257.safetensors', 'model.layers.13.block_sparse_moe.experts.7.w3.weight': 'model-00113-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.0.w1.weight': 'model-00114-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.0.w2.weight': 'model-00114-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.0.w3.weight': 'model-00114-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.1.w1.weight': 'model-00115-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.1.w2.weight': 'model-00115-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.1.w3.weight': 'model-00115-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.2.w1.weight': 'model-00116-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.2.w2.weight': 'model-00116-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.2.w3.weight': 'model-00116-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.3.w1.weight': 'model-00117-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.3.w2.weight': 'model-00117-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.3.w3.weight': 'model-00117-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.4.w1.weight': 'model-00118-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.4.w2.weight': 'model-00118-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.4.w3.weight': 'model-00118-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.5.w1.weight': 'model-00119-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.5.w2.weight': 'model-00119-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.5.w3.weight': 'model-00119-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.6.w1.weight': 'model-00120-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.6.w2.weight': 'model-00120-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.6.w3.weight': 'model-00120-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.7.w1.weight': 'model-00121-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.7.w2.weight': 'model-00121-of-00257.safetensors', 'model.layers.14.block_sparse_moe.experts.7.w3.weight': 'model-00121-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.0.w1.weight': 'model-00122-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.0.w2.weight': 'model-00122-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.0.w3.weight': 'model-00122-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.1.w1.weight': 'model-00123-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.1.w2.weight': 'model-00123-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.1.w3.weight': 'model-00123-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.2.w1.weight': 'model-00124-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.2.w2.weight': 'model-00124-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.2.w3.weight': 'model-00124-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.3.w1.weight': 'model-00125-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.3.w2.weight': 'model-00125-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.3.w3.weight': 'model-00125-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.4.w1.weight': 'model-00126-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.4.w2.weight': 'model-00126-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.4.w3.weight': 'model-00126-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.5.w1.weight': 'model-00127-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.5.w2.weight': 'model-00127-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.5.w3.weight': 'model-00127-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.6.w1.weight': 'model-00128-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.6.w2.weight': 'model-00128-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.6.w3.weight': 'model-00128-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.7.w1.weight': 'model-00129-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.7.w2.weight': 'model-00129-of-00257.safetensors', 'model.layers.15.block_sparse_moe.experts.7.w3.weight': 'model-00129-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.0.w1.weight': 'model-00130-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.0.w2.weight': 'model-00130-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.0.w3.weight': 'model-00130-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.1.w1.weight': 'model-00131-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.1.w2.weight': 'model-00131-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.1.w3.weight': 'model-00131-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.2.w1.weight': 'model-00132-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.2.w2.weight': 'model-00132-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.2.w3.weight': 'model-00132-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.3.w1.weight': 'model-00133-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.3.w2.weight': 'model-00133-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.3.w3.weight': 'model-00133-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.4.w1.weight': 'model-00134-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.4.w2.weight': 'model-00134-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.4.w3.weight': 'model-00134-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.5.w1.weight': 'model-00135-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.5.w2.weight': 'model-00135-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.5.w3.weight': 'model-00135-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.6.w1.weight': 'model-00136-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.6.w2.weight': 'model-00136-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.6.w3.weight': 'model-00136-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.7.w1.weight': 'model-00137-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.7.w2.weight': 'model-00137-of-00257.safetensors', 'model.layers.16.block_sparse_moe.experts.7.w3.weight': 'model-00137-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.0.w1.weight': 'model-00138-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.0.w2.weight': 'model-00138-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.0.w3.weight': 'model-00138-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.1.w1.weight': 'model-00139-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.1.w2.weight': 'model-00139-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.1.w3.weight': 'model-00139-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.2.w1.weight': 'model-00140-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.2.w2.weight': 'model-00140-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.2.w3.weight': 'model-00140-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.3.w1.weight': 'model-00141-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.3.w2.weight': 'model-00141-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.3.w3.weight': 'model-00141-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.4.w1.weight': 'model-00142-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.4.w2.weight': 'model-00142-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.4.w3.weight': 'model-00142-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.5.w1.weight': 'model-00143-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.5.w2.weight': 'model-00143-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.5.w3.weight': 'model-00143-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.6.w1.weight': 'model-00144-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.6.w2.weight': 'model-00144-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.6.w3.weight': 'model-00144-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.7.w1.weight': 'model-00145-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.7.w2.weight': 'model-00145-of-00257.safetensors', 'model.layers.17.block_sparse_moe.experts.7.w3.weight': 'model-00145-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.0.w1.weight': 'model-00146-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.0.w2.weight': 'model-00146-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.0.w3.weight': 'model-00146-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.1.w1.weight': 'model-00147-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.1.w2.weight': 'model-00147-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.1.w3.weight': 'model-00147-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.2.w1.weight': 'model-00148-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.2.w2.weight': 'model-00148-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.2.w3.weight': 'model-00148-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.3.w1.weight': 'model-00149-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.3.w2.weight': 'model-00149-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.3.w3.weight': 'model-00149-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.4.w1.weight': 'model-00150-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.4.w2.weight': 'model-00150-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.4.w3.weight': 'model-00150-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.5.w1.weight': 'model-00151-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.5.w2.weight': 'model-00151-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.5.w3.weight': 'model-00151-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.6.w1.weight': 'model-00152-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.6.w2.weight': 'model-00152-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.6.w3.weight': 'model-00152-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.7.w1.weight': 'model-00153-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.7.w2.weight': 'model-00153-of-00257.safetensors', 'model.layers.18.block_sparse_moe.experts.7.w3.weight': 'model-00153-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.0.w1.weight': 'model-00154-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.0.w2.weight': 'model-00154-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.0.w3.weight': 'model-00154-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.1.w1.weight': 'model-00155-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.1.w2.weight': 'model-00155-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.1.w3.weight': 'model-00155-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.2.w1.weight': 'model-00156-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.2.w2.weight': 'model-00156-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.2.w3.weight': 'model-00156-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.3.w1.weight': 'model-00157-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.3.w2.weight': 'model-00157-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.3.w3.weight': 'model-00157-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.4.w1.weight': 'model-00158-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.4.w2.weight': 'model-00158-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.4.w3.weight': 'model-00158-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.5.w1.weight': 'model-00159-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.5.w2.weight': 'model-00159-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.5.w3.weight': 'model-00159-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.6.w1.weight': 'model-00160-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.6.w2.weight': 'model-00160-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.6.w3.weight': 'model-00160-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.7.w1.weight': 'model-00161-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.7.w2.weight': 'model-00161-of-00257.safetensors', 'model.layers.19.block_sparse_moe.experts.7.w3.weight': 'model-00161-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.0.w1.weight': 'model-00162-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.0.w2.weight': 'model-00162-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.0.w3.weight': 'model-00162-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.1.w1.weight': 'model-00163-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.1.w2.weight': 'model-00163-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.1.w3.weight': 'model-00163-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.2.w1.weight': 'model-00164-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.2.w2.weight': 'model-00164-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.2.w3.weight': 'model-00164-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.3.w1.weight': 'model-00165-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.3.w2.weight': 'model-00165-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.3.w3.weight': 'model-00165-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.4.w1.weight': 'model-00166-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.4.w2.weight': 'model-00166-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.4.w3.weight': 'model-00166-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.5.w1.weight': 'model-00167-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.5.w2.weight': 'model-00167-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.5.w3.weight': 'model-00167-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.6.w1.weight': 'model-00168-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.6.w2.weight': 'model-00168-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.6.w3.weight': 'model-00168-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.7.w1.weight': 'model-00169-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.7.w2.weight': 'model-00169-of-00257.safetensors', 'model.layers.20.block_sparse_moe.experts.7.w3.weight': 'model-00169-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.0.w1.weight': 'model-00170-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.0.w2.weight': 'model-00170-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.0.w3.weight': 'model-00170-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.1.w1.weight': 'model-00171-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.1.w2.weight': 'model-00171-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.1.w3.weight': 'model-00171-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.2.w1.weight': 'model-00172-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.2.w2.weight': 'model-00172-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.2.w3.weight': 'model-00172-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.3.w1.weight': 'model-00173-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.3.w2.weight': 'model-00173-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.3.w3.weight': 'model-00173-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.4.w1.weight': 'model-00174-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.4.w2.weight': 'model-00174-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.4.w3.weight': 'model-00174-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.5.w1.weight': 'model-00175-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.5.w2.weight': 'model-00175-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.5.w3.weight': 'model-00175-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.6.w1.weight': 'model-00176-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.6.w2.weight': 'model-00176-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.6.w3.weight': 'model-00176-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.7.w1.weight': 'model-00177-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.7.w2.weight': 'model-00177-of-00257.safetensors', 'model.layers.21.block_sparse_moe.experts.7.w3.weight': 'model-00177-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.0.w1.weight': 'model-00178-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.0.w2.weight': 'model-00178-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.0.w3.weight': 'model-00178-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.1.w1.weight': 'model-00179-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.1.w2.weight': 'model-00179-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.1.w3.weight': 'model-00179-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.2.w1.weight': 'model-00180-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.2.w2.weight': 'model-00180-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.2.w3.weight': 'model-00180-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.3.w1.weight': 'model-00181-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.3.w2.weight': 'model-00181-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.3.w3.weight': 'model-00181-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.4.w1.weight': 'model-00182-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.4.w2.weight': 'model-00182-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.4.w3.weight': 'model-00182-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.5.w1.weight': 'model-00183-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.5.w2.weight': 'model-00183-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.5.w3.weight': 'model-00183-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.6.w1.weight': 'model-00184-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.6.w2.weight': 'model-00184-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.6.w3.weight': 'model-00184-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.7.w1.weight': 'model-00185-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.7.w2.weight': 'model-00185-of-00257.safetensors', 'model.layers.22.block_sparse_moe.experts.7.w3.weight': 'model-00185-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.0.w1.weight': 'model-00186-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.0.w2.weight': 'model-00186-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.0.w3.weight': 'model-00186-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.1.w1.weight': 'model-00187-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.1.w2.weight': 'model-00187-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.1.w3.weight': 'model-00187-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.2.w1.weight': 'model-00188-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.2.w2.weight': 'model-00188-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.2.w3.weight': 'model-00188-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.3.w1.weight': 'model-00189-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.3.w2.weight': 'model-00189-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.3.w3.weight': 'model-00189-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.4.w1.weight': 'model-00190-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.4.w2.weight': 'model-00190-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.4.w3.weight': 'model-00190-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.5.w1.weight': 'model-00191-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.5.w2.weight': 'model-00191-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.5.w3.weight': 'model-00191-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.6.w1.weight': 'model-00192-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.6.w2.weight': 'model-00192-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.6.w3.weight': 'model-00192-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.7.w1.weight': 'model-00193-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.7.w2.weight': 'model-00193-of-00257.safetensors', 'model.layers.23.block_sparse_moe.experts.7.w3.weight': 'model-00193-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.0.w1.weight': 'model-00194-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.0.w2.weight': 'model-00194-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.0.w3.weight': 'model-00194-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.1.w1.weight': 'model-00195-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.1.w2.weight': 'model-00195-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.1.w3.weight': 'model-00195-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.2.w1.weight': 'model-00196-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.2.w2.weight': 'model-00196-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.2.w3.weight': 'model-00196-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.3.w1.weight': 'model-00197-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.3.w2.weight': 'model-00197-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.3.w3.weight': 'model-00197-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.4.w1.weight': 'model-00198-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.4.w2.weight': 'model-00198-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.4.w3.weight': 'model-00198-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.5.w1.weight': 'model-00199-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.5.w2.weight': 'model-00199-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.5.w3.weight': 'model-00199-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.6.w1.weight': 'model-00200-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.6.w2.weight': 'model-00200-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.6.w3.weight': 'model-00200-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.7.w1.weight': 'model-00201-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.7.w2.weight': 'model-00201-of-00257.safetensors', 'model.layers.24.block_sparse_moe.experts.7.w3.weight': 'model-00201-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.0.w1.weight': 'model-00202-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.0.w2.weight': 'model-00202-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.0.w3.weight': 'model-00202-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.1.w1.weight': 'model-00203-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.1.w2.weight': 'model-00203-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.1.w3.weight': 'model-00203-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.2.w1.weight': 'model-00204-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.2.w2.weight': 'model-00204-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.2.w3.weight': 'model-00204-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.3.w1.weight': 'model-00205-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.3.w2.weight': 'model-00205-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.3.w3.weight': 'model-00205-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.4.w1.weight': 'model-00206-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.4.w2.weight': 'model-00206-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.4.w3.weight': 'model-00206-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.5.w1.weight': 'model-00207-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.5.w2.weight': 'model-00207-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.5.w3.weight': 'model-00207-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.6.w1.weight': 'model-00208-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.6.w2.weight': 'model-00208-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.6.w3.weight': 'model-00208-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.7.w1.weight': 'model-00209-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.7.w2.weight': 'model-00209-of-00257.safetensors', 'model.layers.25.block_sparse_moe.experts.7.w3.weight': 'model-00209-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.0.w1.weight': 'model-00210-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.0.w2.weight': 'model-00210-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.0.w3.weight': 'model-00210-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.1.w1.weight': 'model-00211-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.1.w2.weight': 'model-00211-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.1.w3.weight': 'model-00211-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.2.w1.weight': 'model-00212-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.2.w2.weight': 'model-00212-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.2.w3.weight': 'model-00212-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.3.w1.weight': 'model-00213-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.3.w2.weight': 'model-00213-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.3.w3.weight': 'model-00213-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.4.w1.weight': 'model-00214-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.4.w2.weight': 'model-00214-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.4.w3.weight': 'model-00214-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.5.w1.weight': 'model-00215-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.5.w2.weight': 'model-00215-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.5.w3.weight': 'model-00215-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.6.w1.weight': 'model-00216-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.6.w2.weight': 'model-00216-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.6.w3.weight': 'model-00216-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.7.w1.weight': 'model-00217-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.7.w2.weight': 'model-00217-of-00257.safetensors', 'model.layers.26.block_sparse_moe.experts.7.w3.weight': 'model-00217-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.0.w1.weight': 'model-00218-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.0.w2.weight': 'model-00218-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.0.w3.weight': 'model-00218-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.1.w1.weight': 'model-00219-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.1.w2.weight': 'model-00219-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.1.w3.weight': 'model-00219-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.2.w1.weight': 'model-00220-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.2.w2.weight': 'model-00220-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.2.w3.weight': 'model-00220-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.3.w1.weight': 'model-00221-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.3.w2.weight': 'model-00221-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.3.w3.weight': 'model-00221-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.4.w1.weight': 'model-00222-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.4.w2.weight': 'model-00222-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.4.w3.weight': 'model-00222-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.5.w1.weight': 'model-00223-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.5.w2.weight': 'model-00223-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.5.w3.weight': 'model-00223-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.6.w1.weight': 'model-00224-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.6.w2.weight': 'model-00224-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.6.w3.weight': 'model-00224-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.7.w1.weight': 'model-00225-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.7.w2.weight': 'model-00225-of-00257.safetensors', 'model.layers.27.block_sparse_moe.experts.7.w3.weight': 'model-00225-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.0.w1.weight': 'model-00226-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.0.w2.weight': 'model-00226-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.0.w3.weight': 'model-00226-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.1.w1.weight': 'model-00227-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.1.w2.weight': 'model-00227-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.1.w3.weight': 'model-00227-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.2.w1.weight': 'model-00228-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.2.w2.weight': 'model-00228-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.2.w3.weight': 'model-00228-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.3.w1.weight': 'model-00229-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.3.w2.weight': 'model-00229-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.3.w3.weight': 'model-00229-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.4.w1.weight': 'model-00230-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.4.w2.weight': 'model-00230-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.4.w3.weight': 'model-00230-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.5.w1.weight': 'model-00231-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.5.w2.weight': 'model-00231-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.5.w3.weight': 'model-00231-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.6.w1.weight': 'model-00232-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.6.w2.weight': 'model-00232-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.6.w3.weight': 'model-00232-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.7.w1.weight': 'model-00233-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.7.w2.weight': 'model-00233-of-00257.safetensors', 'model.layers.28.block_sparse_moe.experts.7.w3.weight': 'model-00233-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.0.w1.weight': 'model-00234-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.0.w2.weight': 'model-00234-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.0.w3.weight': 'model-00234-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.1.w1.weight': 'model-00235-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.1.w2.weight': 'model-00235-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.1.w3.weight': 'model-00235-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.2.w1.weight': 'model-00236-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.2.w2.weight': 'model-00236-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.2.w3.weight': 'model-00236-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.3.w1.weight': 'model-00237-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.3.w2.weight': 'model-00237-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.3.w3.weight': 'model-00237-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.4.w1.weight': 'model-00238-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.4.w2.weight': 'model-00238-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.4.w3.weight': 'model-00238-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.5.w1.weight': 'model-00239-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.5.w2.weight': 'model-00239-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.5.w3.weight': 'model-00239-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.6.w1.weight': 'model-00240-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.6.w2.weight': 'model-00240-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.6.w3.weight': 'model-00240-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.7.w1.weight': 'model-00241-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.7.w2.weight': 'model-00241-of-00257.safetensors', 'model.layers.29.block_sparse_moe.experts.7.w3.weight': 'model-00241-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.0.w1.weight': 'model-00242-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.0.w2.weight': 'model-00242-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.0.w3.weight': 'model-00242-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.1.w1.weight': 'model-00243-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.1.w2.weight': 'model-00243-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.1.w3.weight': 'model-00243-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.2.w1.weight': 'model-00244-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.2.w2.weight': 'model-00244-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.2.w3.weight': 'model-00244-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.3.w1.weight': 'model-00245-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.3.w2.weight': 'model-00245-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.3.w3.weight': 'model-00245-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.4.w1.weight': 'model-00246-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.4.w2.weight': 'model-00246-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.4.w3.weight': 'model-00246-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.5.w1.weight': 'model-00247-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.5.w2.weight': 'model-00247-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.5.w3.weight': 'model-00247-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.6.w1.weight': 'model-00248-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.6.w2.weight': 'model-00248-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.6.w3.weight': 'model-00248-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.7.w1.weight': 'model-00249-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.7.w2.weight': 'model-00249-of-00257.safetensors', 'model.layers.30.block_sparse_moe.experts.7.w3.weight': 'model-00249-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.0.w1.weight': 'model-00250-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.0.w2.weight': 'model-00250-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.0.w3.weight': 'model-00250-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.1.w1.weight': 'model-00251-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.1.w2.weight': 'model-00251-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.1.w3.weight': 'model-00251-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.2.w1.weight': 'model-00252-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.2.w2.weight': 'model-00252-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.2.w3.weight': 'model-00252-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.3.w1.weight': 'model-00253-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.3.w2.weight': 'model-00253-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.3.w3.weight': 'model-00253-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.4.w1.weight': 'model-00254-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.4.w2.weight': 'model-00254-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.4.w3.weight': 'model-00254-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.5.w1.weight': 'model-00255-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.5.w2.weight': 'model-00255-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.5.w3.weight': 'model-00255-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.6.w1.weight': 'model-00256-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.6.w2.weight': 'model-00256-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.6.w3.weight': 'model-00256-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.7.w1.weight': 'model-00257-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.7.w2.weight': 'model-00257-of-00257.safetensors', 'model.layers.31.block_sparse_moe.experts.7.w3.weight': 'model-00257-of-00257.safetensors'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import gc\n",
        "import re\n",
        "import os\n",
        "from safetensors.torch import load_file, save_file\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except NameError:\n",
        "    pass\n",
        "device = torch.device(\"cuda:0\")\n",
        "save_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/mixtral-offloading/test_dir/\"\n",
        "state_path = \"/scratch/bcjw/yyuan6/mistral-8x7b/Mixtral-8x7B-Instruct-v0.1\"\n",
        "index_path = os.path.join(save_path, \"model.safetensors.index.json\")\n",
        "i = 1\n",
        "if not os.path.exists(index_path):\n",
        "    index_data = {\"weight_map\": {}}\n",
        "else:\n",
        "    with open(index_path, 'r') as index_file:\n",
        "        index_data = json.load(index_file)\n",
        "\n",
        "for layer_idx in range(32):  \n",
        "    for expert_idx in range(8):\n",
        "        index_path = os.path.join(state_path, \"model.safetensors.index.json\")\n",
        "        with open(index_path) as f:\n",
        "            module_idx = f\"model.layers.{layer_idx}.block_sparse_moe.experts.{expert_idx}\"\n",
        "            weight_map = json.load(f)[\"weight_map\"]\n",
        "            state_fpath = weight_map[f\"{module_idx}.w1.weight\"]\n",
        "            state_fpath2 = weight_map[f\"{module_idx}.w3.weight\"]\n",
        "        \n",
        "        loaded_state_dict = load_file(os.path.join(state_path, state_fpath), device=str(device))\n",
        "        state_dict = {}\n",
        "        state_dict[\"w1.weight\"] = loaded_state_dict[f'model.layers.{layer_idx}.block_sparse_moe.experts.{expert_idx}.w1.weight'] \n",
        "\n",
        "        if f'model.layers.{layer_idx}.block_sparse_moe.experts.{expert_idx}.w2.weight' not in loaded_state_dict:\n",
        "            loaded_state_dict = load_file(os.path.join(state_path, state_fpath2), device=str(device))\n",
        "        state_dict[\"w2.weight\"] = loaded_state_dict[f'model.layers.{layer_idx}.block_sparse_moe.experts.{expert_idx}.w2.weight'] \n",
        "        \n",
        "        if f'model.layers.{layer_idx}.block_sparse_moe.experts.{expert_idx}.w3.weight' not in loaded_state_dict:\n",
        "            loaded_state_dict = load_file(os.path.join(state_path, state_fpath2), device=str(device))\n",
        "        state_dict[\"w3.weight\"] = loaded_state_dict[f'model.layers.{layer_idx}.block_sparse_moe.experts.{expert_idx}.w3.weight'] \n",
        "\n",
        "        print(state_dict.keys())\n",
        "        # half_state_dict = {key: tensor.half() for key, tensor in state_dict.items()}\n",
        "\n",
        "        i = i + 1\n",
        "        save_file_name = f\"model-{i:05d}-of-00257.safetensors\"\n",
        "        # save_file(half_state_dict, save_path + save_file_name)\n",
        "\n",
        "        for weight_key in ['w1.weight', 'w2.weight', 'w3.weight']:\n",
        "            index_data['weight_map'][f\"model.layers.{layer_idx}.block_sparse_moe.experts.{expert_idx}.{weight_key}\"] = save_file_name\n",
        "\n",
        "print(index_data['weight_map'])\n",
        "with open(index_path, 'w') as index_file:\n",
        "    json.dump(index_data, index_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03638a96888649dd9dc625a623eb6c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffa7a23d200843068267e4e21553e98d",
              "IPY_MODEL_ad60a3884afe47a9ab9b4a70a461d49b",
              "IPY_MODEL_2248396d844b4f159fd895fa100f5875"
            ],
            "layout": "IPY_MODEL_073cc65432b44fe29eae1c91471ff970"
          }
        },
        "073cc65432b44fe29eae1c91471ff970": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108098be0bdc49e791fb784d487fe175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "137e376507144fccbabb12bf1268f699": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bfcbb901574f0583b8af3733f58b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a6a03868cdc4fb286b4ec580da9a391": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2248396d844b4f159fd895fa100f5875": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddc0e14f4448489499a534c5cf2f5945",
            "placeholder": "​",
            "style": "IPY_MODEL_6e8a37d65bf64bbaac84230613164936",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "243efd24b8994278a406c597039d0fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_325f9b174633405483cf60cff12d652b",
            "placeholder": "​",
            "style": "IPY_MODEL_e60bfa41b7454322a908e01bb9caed65",
            "value": " 720/720 [00:00&lt;00:00, 20.2kB/s]"
          }
        },
        "325f9b174633405483cf60cff12d652b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39efadb9ccb048deab07969016e7bd38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494bb8b09d1f4f2abbc111eb5e6da130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a458bdef43543d0b7b7edc2883e2dc1",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f85bc3a52a540558999a1ff25a813b3",
            "value": 720
          }
        },
        "67dc7ff4d0fe4af796308b2c8355d150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b41dad78195344a6a77b570b1783f7a0",
              "IPY_MODEL_494bb8b09d1f4f2abbc111eb5e6da130",
              "IPY_MODEL_243efd24b8994278a406c597039d0fc4"
            ],
            "layout": "IPY_MODEL_c2ccdf34885d41a0ab3f9b40c775b25a"
          }
        },
        "6a458bdef43543d0b7b7edc2883e2dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8a37d65bf64bbaac84230613164936": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77eb47b1a453410b860b91a01357e98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b7b50c298de414b85cc622ef5660dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1e82a41293d41d6aa8807d9d575e86b",
              "IPY_MODEL_a19471c4ecce428bbebab2a90f535861",
              "IPY_MODEL_8a2c815f41b8483692a79165ebeaf3f6"
            ],
            "layout": "IPY_MODEL_9782a56e37e24892a78b282295ae5ac0"
          }
        },
        "7f85bc3a52a540558999a1ff25a813b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83bcda80b79a4e6eaf5b0f27bcbc0ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a2c815f41b8483692a79165ebeaf3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b792acc1fff049fca658f836557c8e6f",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf2b97ed0b7415fa02f38c8bb0e3009",
            "value": " 32/32 [01:36&lt;00:00,  3.03s/it]"
          }
        },
        "9782a56e37e24892a78b282295ae5ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be1146cefb9416cb875741640dca611": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a19471c4ecce428bbebab2a90f535861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0dba9bd6ee94ca1a0084eabeac0e1ca",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83bcda80b79a4e6eaf5b0f27bcbc0ee0",
            "value": 32
          }
        },
        "ad60a3884afe47a9ab9b4a70a461d49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_108098be0bdc49e791fb784d487fe175",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77eb47b1a453410b860b91a01357e98f",
            "value": 0
          }
        },
        "b41dad78195344a6a77b570b1783f7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39efadb9ccb048deab07969016e7bd38",
            "placeholder": "​",
            "style": "IPY_MODEL_d36446b0a9534f41a9f21b865b0a08f4",
            "value": "config.json: 100%"
          }
        },
        "b792acc1fff049fca658f836557c8e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0dba9bd6ee94ca1a0084eabeac0e1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ccdf34885d41a0ab3f9b40c775b25a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e82a41293d41d6aa8807d9d575e86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be1146cefb9416cb875741640dca611",
            "placeholder": "​",
            "style": "IPY_MODEL_15bfcbb901574f0583b8af3733f58b7a",
            "value": "Loading experts: 100%"
          }
        },
        "d36446b0a9534f41a9f21b865b0a08f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcf2b97ed0b7415fa02f38c8bb0e3009": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddc0e14f4448489499a534c5cf2f5945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e60bfa41b7454322a908e01bb9caed65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffa7a23d200843068267e4e21553e98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137e376507144fccbabb12bf1268f699",
            "placeholder": "​",
            "style": "IPY_MODEL_1a6a03868cdc4fb286b4ec580da9a391",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
